<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Articles on sambaiz-net</title>
    <link>https://www.sambaiz.net/en/article/</link>
    <description>Recent content in Articles on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Mon, 02 Jan 2023 14:53:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/en/article/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Register the EKS cluster launched with CDK to EMR on EKS and run Spark jobs</title>
      <link>https://www.sambaiz.net/en/article/434/</link>
      <pubDate>Mon, 02 Jan 2023 14:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/434/</guid>
      <description>EMR on EKS is a feature to run Spark on EKS. While normal EMR also manages Hadoop clusters, EMR on EKS is only responsible for starting containers. Launch an EMR cluster with AWS CLI and run Spark applications - sambaiz-net By running on Kubernetes, you can use tools and functions for Kubernetes to manage and monitor, and you can use resources left over if you have an existing cluster. It</description>
    </item>
    
    <item>
      <title>The Scheduler which allocates resources in Hadoop YARN, and Dominant Resource Fairness (DRF)</title>
      <link>https://www.sambaiz.net/en/article/433/</link>
      <pubDate>Sat, 24 Dec 2022 22:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/433/</guid>
      <description>YARN&amp;rsquo;s ResourceManager consists of ApplicationsManager, which receives applications from clients and launches ApplicationMaster, and Scheduler, which receives requests from ApplicationMaster and allocates resources.
How Hadoop YARN allocates resources to applications and check how much resources are allocated - sambaiz-net
Scheduler Scheduler has implementations such as CapacityScheduler, which aims to maximize the throughput in multi-tenant clusters, and FairScheduler, which allocates fair resources to all applications. You can choose to use which one with yarn.</description>
    </item>
    
    <item>
      <title>Retry processing consisting of multiple Tasks with Callbacks in Airflow</title>
      <link>https://www.sambaiz.net/en/article/432/</link>
      <pubDate>Sun, 18 Dec 2022 17:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/432/</guid>
      <description>When processing a task in an EMR cluster, add a Step to the EMR cluster with EmrAddStepsOperator, and then wait for its execution to end with EmrStepSensor. When the Step fails, only the Sensor fails, so there is a problem that the Step is not re-executed even if it is retried.
Create an environment of Amazon Managed Workflow for Apache Airflow (MWAA) with CDK and run a workflow - sambaiz-net</description>
    </item>
    
    <item>
      <title>Check records of operations for AWS resources with CloudTrail</title>
      <link>https://www.sambaiz.net/en/article/431/</link>
      <pubDate>Tue, 06 Dec 2022 21:12:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/431/</guid>
      <description>AWS CloudTrail records AWS API call history, which is used for security auditing and other services such as GuardDuty.
Check security issues detected by GuardDuty, Inspector, and Macie, etc. in AWS Security Hub collectively - sambaiz-net
Event history Event history is recorded per region by default, which is free. The retention period is 90 days, and data events, such as Lambda&amp;rsquo;s invoke and object-level operations in S3, are not included.</description>
    </item>
    
    <item>
      <title>Check security issues detected by GuardDuty, Inspector, and Macie, etc. in AWS Security Hub collectively</title>
      <link>https://www.sambaiz.net/en/article/430/</link>
      <pubDate>Sun, 04 Dec 2022 10:34:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/430/</guid>
      <description>AWS Security Hub is a service that allows you to collectively check security issues detected by various services
Security standards Security Hub supports following security standards, and they are enabled, rules are added to AWS Config.
AWS Foundational Security Best Practices: Best practices for each service [ACM.1] Imported and ACM-issued certificates should be renewed after a specified time period [CloudFront.1] CloudFront distributions should have a default root object configured Center for Internet Security (CIS) AWS Foundations Benchmark: Meet the requirements for certification by the security standardized organization CIS established by the NSA, etc.</description>
    </item>
    
    <item>
      <title>Express dependencies on past tasks in Airflow</title>
      <link>https://www.sambaiz.net/en/article/429/</link>
      <pubDate>Wed, 30 Nov 2022 09:34:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/429/</guid>
      <description>If past aggregated values are required for periodic aggregation and such a workflow is simply executed periodically, subsequent processing will fail in a chain reaction when the processing fails or hasn&amp;rsquo;t been completed in time. Airflow allows you to describe dependencies on past tasks in the following way. This makes it possible to wait for the past aggregation to finish or to re-execute only dependent tasks collectively in the event of failure.</description>
    </item>
    
    <item>
      <title>Create an environment of Amazon Managed Workflow for Apache Airflow (MWAA) with CDK and run a workflow</title>
      <link>https://www.sambaiz.net/en/article/428/</link>
      <pubDate>Mon, 28 Nov 2022 19:34:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/428/</guid>
      <description>Amazon Managed Workflow for Apache Airflow (MWAA) is a managed service of Apache Airflow. Unlike Step Functions that is serverless, it costs per an instance hour, but Airflow&amp;rsquo;s abundant features and third-party&amp;rsquo;s, including AWS, providers packages are available. Run Apache Airflow with Docker Compose and execute a workflow - sambaiz-net Step Functions doesn&amp;rsquo;t support execution from the middle of the workflow currently, so retrying a very long workflow can be</description>
    </item>
    
    <item>
      <title>How Hadoop YARN allocates resources to applications and check how much resources are allocated</title>
      <link>https://www.sambaiz.net/en/article/427/</link>
      <pubDate>Wed, 23 Nov 2022 18:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/427/</guid>
      <description>YARN is a module that manages resources of a Hadoop cluster and schedules.
How Hadoop YARN allocates resources to applications Once ResourceManager (RM) receives an application from a client, it launches ApplicationMaster (AM) and passes information for executing the application. ApplicationMaster asks ResourceManager for allocating resources. After allocated, next it communicates with NodeManagers (NMs) running on each node, and then starts containers and runs the application.
The Scheduler which allocates resources in Hadoop YARN, and Dominant Resource Fairness (DRF) - sambaiz-net</description>
    </item>
    
    <item>
      <title>Pass AWS credentials to services for Docker Compose</title>
      <link>https://www.sambaiz.net/en/article/426/</link>
      <pubDate>Wed, 23 Nov 2022 15:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/426/</guid>
      <description>When you run a docker container locally, you can mount ~/.aws to pass AWS credentials, and similarly, you can do that with volumes in Docker Compose. Besides, there is also a secrets field. According to the document, it looks to depend on Swarm, but actually, it can run standalone. However, this is for development, and it seems to just bind the file.
version: &amp;#39;3&amp;#39; secrets: aws_creds: file: ~/.aws services: aws_cli: image: amazon/aws-cli:2.</description>
    </item>
    
    <item>
      <title>Run Apache Airflow with Docker Compose and execute a workflow</title>
      <link>https://www.sambaiz.net/en/article/425/</link>
      <pubDate>Sat, 19 Nov 2022 16:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/425/</guid>
      <description>Apache Airflow is an OSS that schedules workflows and visualize pipelines. It is scalable and has abundant features. Also, it can be extended with your own Operators in addition to third-party, such as AWS and Slack, providers packages existing in the repository.
Run Airflow Download docker-compose.yaml.
$ curl -LfO &amp;#39;https://airflow.apache.org/docs/apache-airflow/2.4.3/docker-compose.yaml&amp;#39; $ cat docker-compose.yaml ... x-airflow-common: &amp;amp;airflow-common image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.4.3} ... volumes: - ./dags:/opt/airflow/dags - ./logs:/opt/airflow/logs - ./plugins:/opt/airflow/plugins ... services: postgres: image: postgres:13 .</description>
    </item>
    
    <item>
      <title>Monitor AWS costs with New Relic</title>
      <link>https://www.sambaiz.net/en/article/424/</link>
      <pubDate>Sun, 13 Nov 2022 15:34:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/424/</guid>
      <description>Visualize Billing metrics There are Billing metrics in CloudWatch, so if you are sending all metrics in us-east-1 with Cloud Metric Streams, you can refer them with a query as follows.
SELECT max(`aws.billing.EstimatedCharges`) - min(`aws.billing.EstimatedCharges`) as daily_usage FROM Metric WHERE aws.Namespace = &amp;#39;AWS/Billing&amp;#39; AND `metricName` = &amp;#39;aws.billing.EstimatedCharges&amp;#39; AND `aws.billing.ServiceName` IS NOT NULL FACET monthOf(`timestamp`), `aws.billing.ServiceName` TIMESERIES 2 day SLIDE BY 1 day SINCE 4 week ago The values are accumulated monthly, so daily costs can be shown with taking differences from previous day&amp;rsquo;s one with Sliding window.</description>
    </item>
    
    <item>
      <title>Enumerated types and extending existing types in Scala 2/3</title>
      <link>https://www.sambaiz.net/en/article/423/</link>
      <pubDate>Sat, 12 Nov 2022 23:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/423/</guid>
      <description>Scala 2 Scala 2 doesn&amp;rsquo;t have an enum like Java, so Enumeration or case object are used. Besides, you can add fields to existing types by importing implicit class,
sealed trait Animal object Animal { case object Dog extends Animal case object Cat extends Animal } object Converter { implicit class AnimalConverter(animal: Animal) { def hello() = animal match { case Animal.Dog =&amp;gt; &amp;#34;wan!&amp;#34; case Animal.Cat =&amp;gt; &amp;#34;nya!&amp;#34; } } } import Converter.</description>
    </item>
    
    <item>
      <title>Monitor and optimize costs with AWS Cost Management</title>
      <link>https://www.sambaiz.net/en/article/422/</link>
      <pubDate>Thu, 10 Nov 2022 21:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/422/</guid>
      <description>AWS Cost Management is a feature group for monitoring, forecasting, and optimizing costs. Billing has similar features. Billing is for managing present costs whereas Cost Management seems to target future costs.
Cost Explorer Cost Explorer can show costs grouped and filtered by services and regions etc.
With filtering with Usage type filter, you can see costs for data transfer from ELB and EC2.
Budgets Budgets can notice that set budgets have been exceeded and report the usage with email etc.</description>
    </item>
    
    <item>
      <title>Create a role that can assume with OIDC from GitHub Actions with CDK</title>
      <link>https://www.sambaiz.net/en/article/421/</link>
      <pubDate>Sun, 30 Oct 2022 02:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/421/</guid>
      <description>aws-actions/configure-aws-credentials is an action that assumes a role, and it also supports authentication with an access key, but JWT issued by GitHub OIDC Provider enables to access to API securely without setting credential. OpenID ConnectのIDトークンの内容と検証 - sambaiz-net To trust the external provider, it is necessary to register the certificate thumbprint of</description>
    </item>
    
    <item>
      <title>Develop Spark Applications in Scala, deploy with GitHub Actions, and perform remote debugging on EMR</title>
      <link>https://www.sambaiz.net/en/article/420/</link>
      <pubDate>Fri, 21 Oct 2022 23:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/420/</guid>
      <description>Spark provides Java and Python APIs in addition to Scala, which is used for developing Spark itself. You can choose among them depending on the technical stack and technologies used in other components, etc.
While Python has highly compatible with data analysis and machine learning skill sets and easy to edit and run on Glue Studio, the error is hard to understand, and the performance also has disadvantages because it needs to exchange the data between JVM and Python Workers.</description>
    </item>
    
    <item>
      <title>Build Spark and debug it remotely at IntelliJ</title>
      <link>https://www.sambaiz.net/en/article/419/</link>
      <pubDate>Sun, 09 Oct 2022 19:06:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/419/</guid>
      <description>Build at the command prompt $ git clone --branch v3.3.0 --depth 1 https://github.com/apache/spark.git Install Java 8 with asdf. $ brew install asdf $ echo -e &amp;#34;\n. $(brew --prefix asdf)/libexec/asdf.sh&amp;#34; &amp;gt;&amp;gt; ${ZDOTDIR:-~}/.zshrc $ asdf --version v0.10.2 $ asdf plugin-add java $ asdf list-all java $ asdf install java corretto-8.342.07.3 $ asdf global java corretto-8.342.07.3 $ echo &amp;#34;. ~/.asdf/plugins/java/set-java-home.zsh&amp;#34; &amp;gt;&amp;gt; ~/.zprofile $ java -version openjdk version &amp;#34;1.8.0_342&amp;#34; OpenJDK Runtime Environment Corretto-8.342.07.3 (build</description>
    </item>
    
    <item>
      <title>Implement scripts running in Alfred Workflows with deanishe/awgo</title>
      <link>https://www.sambaiz.net/en/article/418/</link>
      <pubDate>Mon, 26 Sep 2022 12:34:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/418/</guid>
      <description>deanishe/awgo is a library to implement scripts running in Alfred Workflows. It enables to output Script Filter JSON to pass values to the next object with NewItem(), access keychain, and check logs with MagicAction by calling wf.Args() and passing workflow:log as arguments. The code is executed every time a character is typed by default, so it would be better to cache results of API calls, etc. as follows. There are</description>
    </item>
    
    <item>
      <title>Aggregate logs of spark running on an EMR cluster with Fluent Bit</title>
      <link>https://www.sambaiz.net/en/article/416/</link>
      <pubDate>Sun, 04 Sep 2022 14:44:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/416/</guid>
      <description>If Spark jobs run on Cluster mode, the logs are not outputted to step/ directory, so it is hard to check it on the console, so try aggregating them to New Relic.
Launch an EMR cluster with AWS CLI and run Spark applications - sambaiz-net
Monitor infrastructure and applications with New Relic - sambaiz-net
Option 1. Sending logs with self installed fluent bit Install Fluent Bit that is memory saving fluentd, and send logs with it.</description>
    </item>
    
    <item>
      <title>Why can Athena v2 fail to query map columns in parquet source tables</title>
      <link>https://www.sambaiz.net/en/article/415/</link>
      <pubDate>Tue, 16 Aug 2022 21:26:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/415/</guid>
      <description>Output logs containing map fields as json etc., convert it to parquet with Glue Studio, and execute queries with Athena, then the queries can succeed or fail depending on the table.
Columnar format Parquet structure and read optimization - sambaiz-net
type Log struct { A map[string]int } =&amp;gt; {&amp;#34;A&amp;#34;:{&amp;#34;B&amp;#34;:10,&amp;#34;C&amp;#34;:20}} The parquet metadata is as follows and the information about map is lost.
$ parquet meta test.parquet File path: test.parquet Created by: parquet-glue version 1.</description>
    </item>
    
    <item>
      <title>Settings for running Spark on EMR</title>
      <link>https://www.sambaiz.net/en/article/414/</link>
      <pubDate>Sat, 13 Aug 2022 19:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/414/</guid>
      <description>EMR and Glue are managed services that run Spark applications on AWS. Glue is easy to run ETL jobs with serverless, while EMR allows fine-tuning of resources and parameters. In other words, if the settings are not appropriate, the resources cannot be fully used, and tasks can fail due to OOM even if there is excess memory.
In the CLI, settings can be passed as json string or a file with &amp;ndash;configurations.</description>
    </item>
    
    <item>
      <title>Exploring the cause of OOM that occurred in Java from GC logs and heap dumps</title>
      <link>https://www.sambaiz.net/en/article/413/</link>
      <pubDate>Thu, 11 Aug 2022 08:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/413/</guid>
      <description>Visualize GC logs Output GC logs. -Xloggc:/tmp/gc.log -XX:+PrintGCDetails -XX:+PrintGCDateStamps $ cat /tmp/gc.log 2022-08-08T16:35:30.738+0000: [GC (Allocation Failure) [PSYoungGen: 638269K-&amp;gt;3108K(665600K)] 2017703K-&amp;gt;1382542K(2063872K), 0.0084360 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] 2022-08-08T16:35:31.320+0000: [GC (Allocation Failure) [PSYoungGen: 640548K-&amp;gt;2565K(666624K)] 2019982K-&amp;gt;1382000K(2064896K), 0.0086070 secs] [Times: user=0.05 sys=0.00, real=0.01 secs] 2022-08-08T16:35:31.878+0000: [GC (Allocation Failure) [PSYoungGen: 640005K-&amp;gt;2565K(667136K)] 2019440K-&amp;gt;1382000K(2065408K), 0.0086495 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] 2022-08-08T16:35:32.451+0000: [GC (Allocation Failure) [PSYoungGen: 643589K-&amp;gt;3301K(668672K)] 2023024K-&amp;gt;1382736K(2066944K), 0.0087513 secs] [Times: user=0.04 sys=0.00, real=0.01 secs] Opening this</description>
    </item>
    
    <item>
      <title>Call Go functions from browser JavaScript with WebAssembly</title>
      <link>https://www.sambaiz.net/en/article/412/</link>
      <pubDate>Sat, 30 Jul 2022 00:34:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/412/</guid>
      <description>Pushing the button, the function implemented in Go is called, and it updates the value of text.
$ go version go version go1.18 darwin/amd64 Go can refer variables and call functions in js with syscall/js package. By breaking changes of Go 1.12, NewCallback() is renamed to FuncOf(). The function must return the value that has a type js.ValueOf() expects, so if returns a struct, an error occurs.
package main import ( &amp;#34;strconv&amp;#34; &amp;#34;syscall/js&amp;#34; ) func increment(this js.</description>
    </item>
    
    <item>
      <title>Debug a Java application running on a remote machine by enabling JDWP</title>
      <link>https://www.sambaiz.net/en/article/411/</link>
      <pubDate>Sun, 24 Jul 2022 21:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/411/</guid>
      <description>Enable JDWP If -agentlib:jdwp is passed on starting, JDWP(Java Debug Wire Protocol), which is used for communicating between debugger and JVM, is enabled.
transport=dt_socket,server=y,address=*:5005: Listen debugger on port 5005. Prior to Java 8, *: is not required. suspend=n: Don&amp;rsquo;t suspend the JVM immediately before the main class is loaded For applications that terminate when processing is finished, you can start the debugger in advance and then run the application with server=n to connect to the debugger.</description>
    </item>
    
    <item>
      <title>Deploy a container to ECS on Fargate, execute commands by ECS Exec, and perform port forwarding by Session Manager</title>
      <link>https://www.sambaiz.net/en/article/410/</link>
      <pubDate>Sat, 23 Jul 2022 13:45:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/410/</guid>
      <description>When investigating an application running in a remote non-container environment, sshd is often running, so commands can be executed with SSH connection. On the other hand, sshd isn&amp;rsquo;t usually running in a container environment, so it can&amp;rsquo;t be executed similarly. If absolutely necessary, there is a way to run sshd, but it would be better to avoid it in terms of opening ports and managing keys. In this article, command</description>
    </item>
    
    <item>
      <title>Launch an EMR cluster with AWS CLI and run Spark applications</title>
      <link>https://www.sambaiz.net/en/article/409/</link>
      <pubDate>Wed, 22 Jun 2022 00:05:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/409/</guid>
      <description>Amazon EMR is the service that launches a cluster installed Spark, Hive, and Presto on EC2 or EKS. While Glue, the managed Spark service, can easily run Spark ETL jobs with serverless, EMR has the advantage of excellent cost performance by spot instances etc., and also fine-tuning is available, but now that EMR Serverless has been released, the difference has narrowed a little. Glue also has handy features such as</description>
    </item>
    
    <item>
      <title>Characteristics of Metrics and Events in New Relic and queries in NRQL</title>
      <link>https://www.sambaiz.net/en/article/408/</link>
      <pubDate>Thu, 09 Jun 2022 20:05:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/408/</guid>
      <description>New Relic categorizes telemetry data into Metrics, Events, Logs and Traces, called MELT. Sent data can be queried in NRQL, standard SQL-like queries, shown on a dashboard, and targeted as alert conditions, but sometimes desired value can&amp;rsquo;t be obtained depending on the data type. Metrics Periodically aggregated and sent data. Having aggregated data, the transfer cost can be reduced and track the long-term trend, but there is a trade-off that</description>
    </item>
    
    <item>
      <title>Maximum flow and minimum cut problem, Ford–Fulkerson algorithm</title>
      <link>https://www.sambaiz.net/en/article/407/</link>
      <pubDate>Sun, 05 Jun 2022 18:08:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/407/</guid>
      <description>Maximum flow problem Maximum flow promlem is a problem that maximizes the flow from the source to the sink in the network consisting of paths with capacity. It is necessary not to exceed the capacity and to equal input and output amount for each node. Besides, all of the data emitted from the source must be sent to the sink. Minimum cut problem Minimum cut problem is a problem that</description>
    </item>
    
    <item>
      <title>Calculate partial sum with Segment Tree or Bineary Indexed Tree (BIT)</title>
      <link>https://www.sambaiz.net/en/article/406/</link>
      <pubDate>Sun, 29 May 2022 19:18:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/406/</guid>
      <description>Segment Tree Segment Tree is a complete binary tree which can calculate partial sum at O(log n) by having the calculation results in each partials as node. In the following example, the calculated value is sum, but if it is the minimum value, Range Minimum Query (RMQ) can be solved and if it is the sorted list, merge sort is processed. When updating the value, recalculate in order from the</description>
    </item>
    
    <item>
      <title>Settings for querying tables of other accounts with Athena</title>
      <link>https://www.sambaiz.net/en/article/405/</link>
      <pubDate>Tue, 17 May 2022 23:51:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/405/</guid>
      <description>Redshift Serverless and other serverless aggregation services, run query with Glue Data Catalog - sambaiz-net Borrower Account that executes the query needs to access the resources of Owner Account having Data Catalog and the data. There are some ways to access resources of other accounts, such as access tokens or AssumeRole, but in this case, the role used to execute queries also needs the permission of the Borrower Account&amp;rsquo;s Athena,</description>
    </item>
    
    <item>
      <title>How faster is sending/receiving values by UNIX domain socket than starting new processes when executing commands</title>
      <link>https://www.sambaiz.net/en/article/404/</link>
      <pubDate>Fri, 06 May 2022 20:51:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/404/</guid>
      <description>For example when some procedures are written in different languages and they are repeatedly executed as different commands, overhead such as memory allocation occurs when creating a process. One of solutions to avoid this is to keep the process running and pass input/output value with interprocess communication. In this article I make a command in Go and benchmark to see how much the throughput differs.
For speedy interprocess communication on UNIX, besides shared memory, pipes and UNIX domain sockets are used.</description>
    </item>
    
    <item>
      <title>Make asking about codes and debugging efficient with New Relic CodeStream</title>
      <link>https://www.sambaiz.net/en/article/403/</link>
      <pubDate>Wed, 04 May 2022 22:43:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/403/</guid>
      <description>New Relic CodeStream is a plugin which is useful to ask about codes, create issue or PR and debug on IDE for IntelliJ, VSCode and so on. Install Install the plugin to IDE and set the integretion. Comment You can comment to code blocks to ask about it, or create an issue. If Slack integration is set, following post is submited. By &amp;ldquo;Open in IDE&amp;rdquo; button, you can jump to</description>
    </item>
    
    <item>
      <title>Implement Athena&#39;s data source connectors and user defined functions (UDF)</title>
      <link>https://www.sambaiz.net/en/article/402/</link>
      <pubDate>Sat, 23 Apr 2022 18:09:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/402/</guid>
      <description>Athena has a feature called Federate Query that can access data sources other than S3 using Lambda as a connector, and the official repository provides connectors for various data sources such as BigQuery and Snowflake, but you can also implement your own. This article, implement the minimum connector while referring to Example Connector and run it. The full codes has been pushed to GitHub.
Generate data with TPC-DS Connector in Athena&amp;rsquo;s Federated Query - sambaiz-net</description>
    </item>
    
    <item>
      <title>About newrelic-lambda-extension and how it works telemetry without CloudWatch Logs</title>
      <link>https://www.sambaiz.net/en/article/401/</link>
      <pubDate>Fri, 08 Apr 2022 12:11:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/401/</guid>
      <description>Theare are two methods to send Lambda logs to New Relic. First is a conventional method that uses a Lambda function aws-log-ingestion to subscribe and transfer CloudWatch Logs, and second is a method that uses a Lambda layer newrelic-lambda-extension. The latter send trace logs etc. without outputting to CloudWatch Logs so it can minimize the cost. Install Doing newrelic-lambda integrations install, Secret containing API Key Layer refers is deployed. $</description>
    </item>
    
    <item>
      <title>Query resources with NerdGraph, New Relic&#39;s GraphQL API</title>
      <link>https://www.sambaiz.net/en/article/400/</link>
      <pubDate>Fri, 01 Apr 2022 22:02:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/400/</guid>
      <description>NerdGraph is New Relic&amp;rsquo;s GraphQL API and it can be used for querying resources or migrating it.
curl -X POST https://api.newrelic.com/graphql \ -H &amp;#39;Content-Type: application/json&amp;#39; \ -H &amp;#39;API-Key: *****&amp;#39; \ -d &amp;#39;{ &amp;#34;query&amp;#34;: &amp;#34;{ requestContext { userId apiKey } actor { user { name } } }&amp;#34; }&amp;#39; | jq { &amp;#34;data&amp;#34;: { &amp;#34;actor&amp;#34;: { &amp;#34;user&amp;#34;: { &amp;#34;name&amp;#34;: &amp;#34;Taiki Sakamoto&amp;#34; } }, &amp;#34;requestContext&amp;#34;: { &amp;#34;apiKey&amp;#34;: &amp;#34;*****&amp;#34;, &amp;#34;userId&amp;#34;: &amp;#34;*****&amp;#34; } } } If select resources in GraphiQL explorer, the query is generated.</description>
    </item>
    
    <item>
      <title>Monitor infrastructure and applications with New Relic</title>
      <link>https://www.sambaiz.net/en/article/399/</link>
      <pubDate>Wed, 30 Mar 2022 19:11:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/399/</guid>
      <description>New Relic is a SaaS that monitors infrastructure and applications, and there is Datadog as a similar service. It seems that the pricing plans were changed drastically in 2020, and it charges according to the transfer volume and the number of admin users. Therefore compared to Datadog, which charges for hosts and additional features, there is an advantage when managing a large number of instances with a small number of</description>
    </item>
    
    <item>
      <title>Compare Redshift Serverless and Athena performances by TPC-DS queries</title>
      <link>https://www.sambaiz.net/en/article/397/</link>
      <pubDate>Sun, 20 Feb 2022 01:49:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/397/</guid>
      <description>Redshift Serverless and other serverless aggregation services, run query with Glue Data Catalog - sambaiz-net
Compare the performance between Redshift Serverless (Preview) and Athena by queries of TPC-DS, which is a database benchmark.
(PS: 2022-07-13) Following values were calculated at the rate at the time of preview. When it became GA, the rate dropped by about 30%.
Generate data with TPC-DS Connector for Glue - sambaiz-net
First, executed the following query to the json and parquet data.</description>
    </item>
    
    <item>
      <title>Generate data with TPC-DS Connector for Glue</title>
      <link>https://www.sambaiz.net/en/article/393/</link>
      <pubDate>Tue, 18 Jan 2022 21:26:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/393/</guid>
      <description>Previously, I tried to generate 250GB of data with Athena&amp;rsquo;s TPC-DS Connector and output it to S3 but it timed out even if I increased the Lambda resource to the maximum, so I do it with Glue this time.
Generate data with TPC-DS Connector in Athena&amp;rsquo;s Federated Query - sambaiz-net
Subscribe and activate TPC-DS connector for Glue.
Write a script like following. The scale is in GB, the same as Athena&amp;rsquo;s one.</description>
    </item>
    
    <item>
      <title>Redshift Serverless and other serverless ETL services, run query with Glue Data Catalog</title>
      <link>https://www.sambaiz.net/en/article/392/</link>
      <pubDate>Sun, 26 Dec 2021 22:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/392/</guid>
      <description>Redshift Serverless Redshift Serverless is a new feature that can use Redshift, a petabyte-scale DWH without launching an instance, announced at this year&amp;rsquo;s re:Invent. It is available existing features such as Redshift Spectrum, refer to S3 directly, Federated Query to RDS, and Redshift ML. I&amp;rsquo;m happy with this update as it is costly to keep the instance running for occasional usage such as analytics. The cost is charged for at</description>
    </item>
    
    <item>
      <title>Generate data with TPC-DS Connector in Athena&#39;s Federated Query</title>
      <link>https://www.sambaiz.net/en/article/391/</link>
      <pubDate>Sat, 25 Dec 2021 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/391/</guid>
      <description>Athena&amp;rsquo;s Federated Query is a feature to execute queries on non-S3 data sources such as DynamoDB and RDS through Lambda function which is a data sources connector. Implement Athena&amp;rsquo;s data source connectors and user defined functions (UDF) - sambaiz-net This article uses TPC-DS Connector in the AWS official repository. It generates the data of TPC-DS, which is a database benchmark in Decision Support. Although it is in the official repository,</description>
    </item>
    
    <item>
      <title>Check if there is a cycle in the undirected graph by Union-Find Tree</title>
      <link>https://www.sambaiz.net/en/article/390/</link>
      <pubDate>Sun, 12 Dec 2021 16:39:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/390/</guid>
      <description>Union-Find Tree Union-Find Tree is a data structure having some disjoint sets and can do &amp;ldquo;Union&amp;rdquo; which merges two sets, and &amp;ldquo;Find&amp;rdquo; which checks if two elements are in the same set with amortized O(α(n)) (α(n) is an inverse Ackermann function and smaller than log(n)). &amp;ldquo;Union&amp;rdquo; connects the tree with the smaller rank to under side so that merged tree</description>
    </item>
    
    <item>
      <title>Flutter&#39;s Navigator and AuroRoute</title>
      <link>https://www.sambaiz.net/en/article/389/</link>
      <pubDate>Sat, 11 Dec 2021 16:39:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/389/</guid>
      <description>Navigator Flutter&amp;rsquo;s Navigator is a screen transition class by stacking Routes and has APIs such as push() and pop().
import &amp;#39;package:flutter/material.dart&amp;#39;; void main() { runApp(const MyApp()); } class MyApp extends StatelessWidget { const MyApp({Key? key}) : super(key: key); @override Widget build(BuildContext context) { return MaterialApp( title: &amp;#39;Navigation Test&amp;#39;, theme: ThemeData( primarySwatch: Colors.blue, ), home: const Page1(), ); } } class Page1 extends StatelessWidget { const Page1({Key? key}) : super(key: key); @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: const Text(&amp;#34;page1&amp;#34;), ), body: ListView.</description>
    </item>
    
    <item>
      <title>Build iOS/Android/Web App by Flutter</title>
      <link>https://www.sambaiz.net/en/article/388/</link>
      <pubDate>Sun, 05 Dec 2021 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/388/</guid>
      <description>Flutter is a cross-platform framework by Google. In addition to iOS/Android, Web became stable in version 2.0 which is released in March of this year, and Windows/Mac/Linux is beta. Unlike React Native which uses native UI, Flutter uses its own UI. In addition to Material, iOS-style Cupertino is also available, but unless it branches etc., it become the same looks regardless of the platform.
Build the environment Build the environment according to the official Get started.</description>
    </item>
    
    <item>
      <title>Implement Rabin–Karp algorithm in C&#43;&#43;</title>
      <link>https://www.sambaiz.net/en/article/387/</link>
      <pubDate>Sat, 04 Dec 2021 22:38:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/387/</guid>
      <description>Rabin–Karp algorithm is a string-searching algorithm using rolling hash. Rolling hash can be made with O(1) by removing a first element from the previous hash and adding a next element to it. There are various hash functions which can do it, but for example, when simply summing up character codes, the hashes will collide just because the same character is contained,</description>
    </item>
    
    <item>
      <title>Columnar format Parquet structure and read optimization</title>
      <link>https://www.sambaiz.net/en/article/386/</link>
      <pubDate>Fri, 03 Dec 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/386/</guid>
      <description>Parquet is a columnar format mainly used in the Hadoop ecosystem. Compared to row-based formats like CSV, unnecessary columns can be skipped. Besides, there is a mechanics to read only rows that are needed, so queries can be executed efficiently.
Format Rows are horizontally partitioned into some Row Groups, and the Column Chunks of each column are arranged in order. Column Chunks are divided into Pages, and compression and encoding are performed in that unit.</description>
    </item>
    
    <item>
      <title>struct and class in C&#43;&#43;</title>
      <link>https://www.sambaiz.net/en/article/385/</link>
      <pubDate>Tue, 30 Nov 2021 18:12:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/385/</guid>
      <description>class and struct in C++ are functionally equivalent but class is private by default as opposed to struct being public. class is used for encapsulation and if it has public fields and few methods, it seems that struct is used generally. #include &amp;lt;iostream&amp;gt; using namespace std; class C { int value; public: C(int value) { this-&amp;gt;value = value; } int func() { return value; }; }; struct S { S(int</description>
    </item>
    
    <item>
      <title>Treat Spark struct as map to expand to multiple rows with explode</title>
      <link>https://www.sambaiz.net/en/article/384/</link>
      <pubDate>Wed, 13 Oct 2021 02:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/384/</guid>
      <description>When you read data without specifying schema in Spark, the schema is automatically determined from the input as follows. Why can Athena v2 fail to query map columns in parquet source tables - sambaiz-net # {&amp;#34;aaa&amp;#34;:123,&amp;#34;ccc&amp;#34;:[123],&amp;#34;eee&amp;#34;:{&amp;#34;fff&amp;#34;:123},&amp;#34;hhh&amp;#34;:null} df = spark.read.json(&amp;#34;s3://hogefuga/testjson/&amp;#34;) df.printSchema() &amp;#39;&amp;#39;&amp;#39; root |-- aaa: long (nullable = true) |-- ccc: array (nullable = true) | |-- element: long (containsNull = true) |-- eee: struct (nullable = true) | |-- fff:</description>
    </item>
    
    <item>
      <title>Spark Web UI: Monitor Job Stages, Tasks distribution and SQL plan</title>
      <link>https://www.sambaiz.net/en/article/382/</link>
      <pubDate>Thu, 30 Sep 2021 13:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/382/</guid>
      <description>Spark Web UI is a tool for monitoring Jobs and Executors. Get Dockerfile that runs Spark based on maven:3.6-amazoncorretto-8 from aws-glue-samples, and start History Server with the path of EventLog output by Glue and authentication information to get it. $ git clone https://github.com/aws-samples/aws-glue-samples.git $ cd aws-glue-samples/utilities/Spark_UI/glue-3_0/ $ docker build -t glue/sparkui:latest . $ docker run -it -e SPARK_HISTORY_OPTS=&amp;#34;$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=s3a://path_to_eventlog -Dspark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID -Dspark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY&amp;#34; -p 18080:18080 glue/sparkui:latest &amp;#34;/opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer&amp;#34; Now, you can access</description>
    </item>
    
    <item>
      <title>Athena (Presto) and Glue (Spark) can return different values when running the same query</title>
      <link>https://www.sambaiz.net/en/article/370/</link>
      <pubDate>Sat, 03 Jul 2021 23:13:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/370/</guid>
      <description>AWS has multiple managed services that aggregate with SQL-like queries. If queries are executed ad-hoc, Presto-based Athena, which can quickly and easily query to tables in Glue&amp;rsquo;s data catalog, is handy, while if heavy queries are executed in batch, Spark-based Glue, which can avoid resource and time limitation, can be better. Therefore, they can be used properly depending on the case.
Redshift Serverless and other serverless aggregation services, run query with Glue Data Catalog - sambaiz-net</description>
    </item>
    
    <item>
      <title>Enable Job Bookmark of AWS Glue to process from the records following ones executed previously</title>
      <link>https://www.sambaiz.net/en/article/333/</link>
      <pubDate>Fri, 16 Apr 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/333/</guid>
      <description>Job Bookmark of AWS Glue is a feature that saves what records are processed, and prevent it from being executed next time. Parquet and ORC, which were not supported before 1.0, are now supported. AWS GlueでCSVを加工しParquetに変換してパーティションを切りA</description>
    </item>
    
    <item>
      <title>Python with structural subtyping by Protocol</title>
      <link>https://www.sambaiz.net/en/article/325/</link>
      <pubDate>Fri, 12 Feb 2021 02:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/325/</guid>
      <description>In Python that has no interface syntax, it can assure that the function can be called by checking the function existance with hasattr(). However, this method needs to insert asserting each time and the error cannot be known until runtime. class ImplClass(): def foo(self): print(&amp;#34;ok&amp;#34;) class NoImplClass(): pass def call(d): assert hasattr(d, &amp;#39;foo&amp;#39;) d.foo() if __name__ == &amp;#34;__main__&amp;#34;: call(ImplClass()) # =&amp;gt; ok call(NoImplClass()) # =&amp;gt; AssertionError If you describe Type</description>
    </item>
    
    <item>
      <title>What is Apache Spark, RDD, DataFrame, DataSet, Action and Transformation</title>
      <link>https://www.sambaiz.net/en/article/208/</link>
      <pubDate>Wed, 13 Feb 2019 21:17:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/208/</guid>
      <description>What is Spark? Spark is high-performance general-purpose distributed processing system. It is used with distributed storage such as HDFS and S3, and cluster managers such as Hadoop YARN. HDFS(Hadoop Distributed File System)とは - sambaiz-net How Hadoop YARN allocates resources to applications and check how much resources are allocated - sambaiz-net It can be processed faster than Hadoop&amp;rsquo;s MapReduce by storing the intermediate</description>
    </item>
    
    <item>
      <title>Launch Hive execution environment with Cloudera Docker Image and execute query to JSON log</title>
      <link>https://www.sambaiz.net/en/article/128/</link>
      <pubDate>Thu, 24 Aug 2017 09:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/128/</guid>
      <description>What is Hive Hive is a data warehouse software built on Hadoop, which can access data sources such as HDFS with HiveSQL, an extended SQL. Sending a query, the job runs on MapReduce, Spark or Tez. It has fault tolerance and is mainly used in batch processing. What is HDFS(Hadoop Distributed File System) - sambaiz-net Presto, which access data sources with SQL likewise, can execute the query faster than Hive</description>
    </item>
    
    <item>
      <title>What is HDFS(Hadoop Distributed File System)</title>
      <link>https://www.sambaiz.net/en/article/126/</link>
      <pubDate>Mon, 14 Aug 2017 22:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/126/</guid>
      <description>What is HDFS HDFS stands for Hadoop Distributed File System, one of the implementations of file systems for Hadoop, and other implementations include local files and S3. It distributes the disk I/O, which is a big problem when the data size is enormous and makes a block size, a unit for reading and writing, big to reduce the seek cost and improve the throughput.
You can see disk I/O can be a bottleneck from the data that while communication within the data center takes about 0.</description>
    </item>
    
    <item>
      <title>Options for SSH port forwarding</title>
      <link>https://www.sambaiz.net/en/article/42/</link>
      <pubDate>Sat, 17 Dec 2016 12:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/42/</guid>
      <description>Forward connection to localhost:8080 to example2.com:80 through example.com. This is generally used when the target server cannot be accessed directly from the local or when the port is not open to the public.
-L: Do port forwarding -N: Don&amp;rsquo;t execute commands -f: Run in the background $ ssh hoge@example.com -Nf -L 8080:example2.com:80 $ curl localhost:8080 # =&amp;gt; example2.com:80 </description>
    </item>
    
    <item>
      <title>Increase the maximum number of file descriptors</title>
      <link>https://www.sambaiz.net/en/article/41/</link>
      <pubDate>Thu, 08 Dec 2016 21:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/41/</guid>
      <description>What is file descriptors? File desecriptors is an identifier for interacting with an outside of a process. In POSIX, it is an int type, 0 is stdin, 1 is stdout, and 2 is stderr. It is assigned by a system call such as open() to open files and devices, and socket() to create sockets to communicate with other processes. Maximum number of file descriptors There is a limit on the</description>
    </item>
    
    <item>
      <title>About JVM Heap space and Full GC</title>
      <link>https://www.sambaiz.net/en/article/35/</link>
      <pubDate>Mon, 14 Nov 2016 23:46:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/35/</guid>
      <description>Heap space Heap space is a dynamically allocated memory space that is divided into a new generation and an old generation in JVM. By the way, the heap space containing the loaded classes and methods was called the Permanent space, but since Java8, it has been replaced by the Metaspace and has been placed in native memory. New generation The New generation is further divided into the following space. Eden:</description>
    </item>
    
  </channel>
</rss>
