<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>log on sambaiz-net</title>
    <link>https://www.sambaiz.net/en/tags/log/</link>
    <description>Recent content in log on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Sun, 04 Sep 2022 14:44:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/en/tags/log/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Aggregate logs of spark running on an EMR cluster with Fluent Bit</title>
      <link>https://www.sambaiz.net/en/article/416/</link>
      <pubDate>Sun, 04 Sep 2022 14:44:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/416/</guid>
      <description>If Spark jobs run on Cluster mode, the logs are not outputted to step/ directory, so it is hard to check it on the console. Therefore, aggregating them to New Relic, etc. is useful.
Launch an EMR cluster with AWS CLI and run Spark applications - sambaiz-net
Monitor infrastructure and applications with New Relic - sambaiz-net
Option 1. Sending logs with self installed fluent bit Install Fluent Bit that is memory saving fluentd, and send logs with it.</description>
    </item>
    
    <item>
      <title>Characteristics of Metrics and Events in New Relic and queries in NRQL</title>
      <link>https://www.sambaiz.net/en/article/408/</link>
      <pubDate>Thu, 09 Jun 2022 20:05:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/408/</guid>
      <description>New Relic categorizes telemetry data into Metrics, Events, Logs and Traces, called MELT. Sent data can be queried in NRQL, standard SQL-like queries, shown on a dashboard, and targeted as alert conditions, but sometimes desired value can&amp;rsquo;t be obtained depending on the data type. Metrics Periodically aggregated and sent data. Having aggregated data, the transfer cost can be reduced and track the long-term trend, but there is a trade-off that</description>
    </item>
    
    <item>
      <title>About newrelic-lambda-extension and how it works telemetry without CloudWatch Logs</title>
      <link>https://www.sambaiz.net/en/article/401/</link>
      <pubDate>Fri, 08 Apr 2022 12:11:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/401/</guid>
      <description>Theare are two methods to send Lambda logs to New Relic. First is a conventional method that uses a Lambda function aws-log-ingestion to subscribe and transfer CloudWatch Logs, and second is a method that uses a Lambda layer newrelic-lambda-extension. The latter send trace logs etc. without outputting to CloudWatch Logs so it can minimize the cost. Install Doing newrelic-lambda integrations install, Secret containing API Key Layer refers is deployed. $</description>
    </item>
    
    <item>
      <title>Monitor infrastructure and applications with New Relic</title>
      <link>https://www.sambaiz.net/en/article/399/</link>
      <pubDate>Wed, 30 Mar 2022 19:11:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/399/</guid>
      <description>New Relic is a SaaS that monitors infrastructure and applications, and there is Datadog as a similar service. It seems that the pricing plans were changed drastically in 2020, and it charges according to the transfer volume and the number of admin users. Therefore compared to Datadog, which charges for hosts and additional features, there is an advantage when managing a large number of instances with a small number of</description>
    </item>
    
    <item>
      <title>Generate data with TPC-DS Connector for Glue</title>
      <link>https://www.sambaiz.net/en/article/393/</link>
      <pubDate>Tue, 18 Jan 2022 21:26:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/393/</guid>
      <description>Previously, I tried to generate 250GB of data with Athena&amp;rsquo;s TPC-DS Connector and output it to S3 but it timed out even if I increased the Lambda resource to the maximum, so I do it with Glue this time.
Generate data with TPC-DS Connector in Athena&amp;rsquo;s Federated Query - sambaiz-net
Subscribe and activate TPC-DS connector for Glue.
Write a script like following. The scale is in GB, the same as Athena&amp;rsquo;s one.</description>
    </item>
    
    <item>
      <title>Redshift Serverless and other serverless aggregation services, run query with Glue Data Catalog</title>
      <link>https://www.sambaiz.net/en/article/392/</link>
      <pubDate>Sun, 26 Dec 2021 22:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/392/</guid>
      <description>Redshift Serverless Redshift Serverless is a new feature that can use Redshift, a petabyte-scale DWH without launching an instance, announced at this year&amp;rsquo;s re:Invent. It is available existing features such as Redshift Spectrum, refer to S3 directly, Federated Query to RDS, and Redshift ML. I&amp;rsquo;m happy with this update as it is costly to keep the instance running for occasional usage such as analytics. The cost is charged for at</description>
    </item>
    
    <item>
      <title>Generate data with TPC-DS Connector in Athena&#39;s Federated Query</title>
      <link>https://www.sambaiz.net/en/article/391/</link>
      <pubDate>Sat, 25 Dec 2021 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/391/</guid>
      <description>Athena&amp;rsquo;s Federated Query is a feature to execute queries on non-S3 data sources such as DynamoDB and RDS through Lambda function which is a data sources connector. Implement Athena&amp;rsquo;s data source connectors and user defined functions (UDF) - sambaiz-net This article uses TPC-DS Connector in the AWS official repository. It generates the data of TPC-DS, which is a database benchmark in Decision Support. Although it is in the official repository,</description>
    </item>
    
    <item>
      <title>Columnar format Parquet structure and read optimization</title>
      <link>https://www.sambaiz.net/en/article/386/</link>
      <pubDate>Fri, 03 Dec 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/386/</guid>
      <description>Parquet is a columnar format mainly used in the Hadoop ecosystem. Compared to row-based formats like CSV, unnecessary columns can be skipped. Besides, there is a mechanics to read only rows that are needed, so queries can be executed efficiently.
Format Rows are horizontally partitioned into some Row Groups, and the Column Chunks of each column are arranged in order. Column Chunks are divided into Pages, and compression and encoding are performed in that unit.</description>
    </item>
    
  </channel>
</rss>
