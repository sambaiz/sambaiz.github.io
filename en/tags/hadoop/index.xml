<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hadoop on sambaiz-net</title>
    <link>https://www.sambaiz.net/en/tags/hadoop/</link>
    <description>Recent content in hadoop on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Wed, 22 Jun 2022 00:05:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/en/tags/hadoop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Launch an EMR cluster with AWS CLI and run Spark applications</title>
      <link>https://www.sambaiz.net/en/article/409/</link>
      <pubDate>Wed, 22 Jun 2022 00:05:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/409/</guid>
      <description>Amazon EMR is the service that launches a cluster installed Spark, Hive, and Presto on EC2 or EKS. While Glue, the managed Spark service, can easily run Spark ETL jobs with serverless, EMR has the advantage of excellent cost performance, and also fine-tuning is available, but now that EMR Serverless has been released, the difference has narrowed a little. Glue also has handy features such as Connector, which makes various</description>
    </item>
    
    <item>
      <title>Columnar format Parquet structure and read optimization</title>
      <link>https://www.sambaiz.net/en/article/386/</link>
      <pubDate>Fri, 03 Dec 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/386/</guid>
      <description>Parquet is a columnar format mainly used in the Hadoop ecosystem. Compared to row-based formats like CSV, unnecessary columns can be skipped. Besides, there is a mechanics to read only rows that are needed, so queries can be executed efficiently.
Format  Rows are horizontally partitioned into some Row Groups, and the Column Chunks of each column are arranged in order. Column Chunks are divided into Pages, and compression and encoding are performed in that unit.</description>
    </item>
    
    <item>
      <title>Launch Hive execution environment with Cloudera Docker Image and execute query to JSON log</title>
      <link>https://www.sambaiz.net/en/article/128/</link>
      <pubDate>Thu, 24 Aug 2017 09:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/128/</guid>
      <description>What is Hive Hive is a data warehouse software built on Hadoop, which can access data sources such as HDFS with HiveSQL, an extended SQL. Sending a query, the job runs on MapReduce, Spark or Tez. It has fault tolerance and is mainly used in batch processing. What is HDFS(Hadoop Distributed File System) - sambaiz-net Presto, which access data sources with SQL likewise, can execute the query faster than Hive</description>
    </item>
    
    <item>
      <title>What is HDFS(Hadoop Distributed File System)</title>
      <link>https://www.sambaiz.net/en/article/126/</link>
      <pubDate>Mon, 14 Aug 2017 22:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/126/</guid>
      <description>What is HDFS HDFS stands for Hadoop Distributed File System, one of the implementations of file systems for Hadoop, and other implementations include local files and S3. It distributes the disk I/O, which is a big problem when the data size is enormous and makes a block size, a unit for reading and writing, big to reduce the seek cost and improve the throughput.
You can see disk I/O can be a bottleneck from the data that while communication within the data center takes about 0.</description>
    </item>
    
  </channel>
</rss>
