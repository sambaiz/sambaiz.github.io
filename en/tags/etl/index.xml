<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>etl on sambaiz-net</title>
    <link>https://www.sambaiz.net/en/tags/etl/</link>
    <description>Recent content in etl on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Mon, 02 Jan 2023 14:53:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/en/tags/etl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Register an EKS cluster launched with CDK to EMR on EKS and run Spark jobs</title>
      <link>https://www.sambaiz.net/en/article/434/</link>
      <pubDate>Mon, 02 Jan 2023 14:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/434/</guid>
      <description>EMR on EKS is a feature to run Spark on EKS. While normal EMR also manages Hadoop clusters, EMR on EKS is only responsible for starting containers. Launch an EMR cluster with AWS CLI and run Spark applications - sambaiz-net By running on Kubernetes, you can use tools and functions for Kubernetes to manage and monitor, and utilize resources left over if you have an existing cluster. It also enables</description>
    </item>
    
    <item>
      <title>Retry processing consisting of multiple Tasks with Callbacks in Airflow</title>
      <link>https://www.sambaiz.net/en/article/432/</link>
      <pubDate>Sun, 18 Dec 2022 17:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/432/</guid>
      <description>When processing a task in an EMR cluster, add a Step to the EMR cluster with EmrAddStepsOperator, and then wait for its execution to end with EmrStepSensor. When the Step fails, only the Sensor fails, so there is a problem that the Step is not re-executed even if it is retried.
Create an environment of Amazon Managed Workflow for Apache Airflow (MWAA) with CDK and run a workflow - sambaiz-net</description>
    </item>
    
    <item>
      <title>Express dependencies on past tasks in Airflow</title>
      <link>https://www.sambaiz.net/en/article/429/</link>
      <pubDate>Wed, 30 Nov 2022 09:34:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/429/</guid>
      <description>If past aggregated values are required for periodic aggregation and such a workflow is simply executed periodically, subsequent processing will fail in a chain reaction when the processing fails or hasn&amp;rsquo;t been completed in time. Airflow allows you to describe dependencies on past tasks in the following way. This makes it possible to wait for the past aggregation to finish or to re-execute only dependent tasks collectively in the event of failure.</description>
    </item>
    
    <item>
      <title>Create an environment of Amazon Managed Workflow for Apache Airflow (MWAA) with CDK and run a workflow</title>
      <link>https://www.sambaiz.net/en/article/428/</link>
      <pubDate>Mon, 28 Nov 2022 19:34:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/428/</guid>
      <description>Amazon Managed Workflow for Apache Airflow (MWAA) is a managed service of Apache Airflow. Unlike Step Functions that is serverless, it costs per an instance hour, but Airflow&amp;rsquo;s abundant features and third-party&amp;rsquo;s, including AWS, providers packages are available. Run Apache Airflow with Docker Compose and execute a workflow - sambaiz-net Step Functions doesn&amp;rsquo;t support execution from the middle of the workflow currently, so retrying a very long workflow can be</description>
    </item>
    
    <item>
      <title>Run Apache Airflow with Docker Compose and execute a workflow</title>
      <link>https://www.sambaiz.net/en/article/425/</link>
      <pubDate>Sat, 19 Nov 2022 16:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/425/</guid>
      <description>Apache Airflow is an OSS that schedules workflows and visualize pipelines. It is scalable and has abundant features. Also, it can be extended with your own Operators in addition to third-party, such as AWS and Slack, providers packages existing in the repository.
Run Airflow Download docker-compose.yaml.
$ curl -LfO &amp;#39;https://airflow.apache.org/docs/apache-airflow/2.4.3/docker-compose.yaml&amp;#39; $ cat docker-compose.yaml ... x-airflow-common: &amp;amp;airflow-common image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.4.3} ... volumes: - ./dags:/opt/airflow/dags - ./logs:/opt/airflow/logs - ./plugins:/opt/airflow/plugins ... services: postgres: image: postgres:13 .</description>
    </item>
    
    <item>
      <title>Develop Spark Applications in Scala, deploy with GitHub Actions, and perform remote debugging on EMR</title>
      <link>https://www.sambaiz.net/en/article/420/</link>
      <pubDate>Fri, 21 Oct 2022 23:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/420/</guid>
      <description>Spark provides Java and Python APIs in addition to Scala, which is used for developing Spark itself. You can choose among them depending on the technical stack and technologies used in other components, etc.
While Python has highly compatible with data analysis and machine learning skill sets and easy to edit and run on Glue Studio, the error is hard to understand, and the performance also has disadvantages because it needs to exchange the data between JVM and Python Workers.</description>
    </item>
    
    <item>
      <title>Build Spark and debug it remotely at IntelliJ</title>
      <link>https://www.sambaiz.net/en/article/419/</link>
      <pubDate>Sun, 09 Oct 2022 19:06:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/419/</guid>
      <description>Build at the command prompt $ git clone --branch v3.3.0 --depth 1 https://github.com/apache/spark.git Install Java 8 with asdf. $ brew install asdf $ echo -e &amp;#34;\n. $(brew --prefix asdf)/libexec/asdf.sh&amp;#34; &amp;gt;&amp;gt; ${ZDOTDIR:-~}/.zshrc $ asdf --version v0.10.2 $ asdf plugin-add java $ asdf list-all java $ asdf install java corretto-8.342.07.3 $ asdf global java corretto-8.342.07.3 $ echo &amp;#34;. ~/.asdf/plugins/java/set-java-home.zsh&amp;#34; &amp;gt;&amp;gt; ~/.zprofile $ java -version openjdk version &amp;#34;1.8.0_342&amp;#34; OpenJDK Runtime Environment Corretto-8.342.07.3 (build</description>
    </item>
    
    <item>
      <title>Why can Athena v2 fail to query map columns in parquet source tables</title>
      <link>https://www.sambaiz.net/en/article/415/</link>
      <pubDate>Tue, 16 Aug 2022 21:26:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/415/</guid>
      <description>Output logs containing map fields as json etc., convert it to parquet with Glue Studio, and execute queries with Athena, then the queries can succeed or fail depending on the table.
Columnar format Parquet structure and read optimization - sambaiz-net
type Log struct { A map[string]int } =&amp;gt; {&amp;#34;A&amp;#34;:{&amp;#34;B&amp;#34;:10,&amp;#34;C&amp;#34;:20}} The parquet metadata is as follows and the information about map is lost.
$ parquet meta test.parquet File path: test.parquet Created by: parquet-glue version 1.</description>
    </item>
    
    <item>
      <title>Settings for running Spark on EMR</title>
      <link>https://www.sambaiz.net/en/article/414/</link>
      <pubDate>Sat, 13 Aug 2022 19:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/414/</guid>
      <description>EMR and Glue are managed services that run Spark applications on AWS. Glue is easy to run ETL jobs with serverless, while EMR allows fine-tuning of resources and parameters. In other words, if the settings are not appropriate, the resources cannot be fully used, and tasks can fail due to OOM even if there is excess memory.
In the CLI, settings can be passed as json string or a file with &amp;ndash;configurations.</description>
    </item>
    
    <item>
      <title>Launch an EMR cluster with AWS CLI and run Spark applications</title>
      <link>https://www.sambaiz.net/en/article/409/</link>
      <pubDate>Wed, 22 Jun 2022 00:05:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/409/</guid>
      <description>Amazon EMR is the service that launches a cluster installed Spark, Hive, and Presto on EC2 or EKS. While Glue, the managed Spark service, can easily run Spark ETL jobs with serverless, EMR has the advantage of excellent cost performance by spot instances etc., and also fine-tuning is available, but now that EMR Serverless has been released, the difference has narrowed a little. Glue also has handy features such as</description>
    </item>
    
    <item>
      <title>Settings for querying tables of other accounts with Athena</title>
      <link>https://www.sambaiz.net/en/article/405/</link>
      <pubDate>Tue, 17 May 2022 23:51:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/405/</guid>
      <description>Redshift Serverless and other serverless aggregation services, run query with Glue Data Catalog - sambaiz-net Borrower Account that executes the query needs to access the resources of Owner Account having Data Catalog and the data. There are some ways to access resources of other accounts, such as access tokens or AssumeRole, but in this case, the role used to execute queries also needs the permission of the Borrower Account&amp;rsquo;s Athena,</description>
    </item>
    
    <item>
      <title>Implement Athena&#39;s data source connectors and user defined functions (UDF)</title>
      <link>https://www.sambaiz.net/en/article/402/</link>
      <pubDate>Sat, 23 Apr 2022 18:09:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/402/</guid>
      <description>Athena has a feature called Federate Query that can access data sources other than S3 using Lambda as a connector, and the official repository provides connectors for various data sources such as BigQuery and Snowflake, but you can also implement your own. This article, implement the minimum connector while referring to Example Connector and run it. The full codes has been pushed to GitHub.
Generate data with TPC-DS Connector in Athena&amp;rsquo;s Federated Query - sambaiz-net</description>
    </item>
    
    <item>
      <title>Compare Redshift Serverless and Athena performances by TPC-DS queries</title>
      <link>https://www.sambaiz.net/en/article/397/</link>
      <pubDate>Sun, 20 Feb 2022 01:49:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/397/</guid>
      <description>Redshift Serverless and other serverless aggregation services, run query with Glue Data Catalog - sambaiz-net
Compare the performance between Redshift Serverless (Preview) and Athena by queries of TPC-DS, which is a database benchmark.
(PS: 2022-07-13) Following values were calculated at the rate at the time of preview. When it became GA, the rate dropped by about 30%.
Generate data with TPC-DS Connector for Glue - sambaiz-net
First, executed the following query to the json and parquet data.</description>
    </item>
    
    <item>
      <title>Generate data with TPC-DS Connector for Glue</title>
      <link>https://www.sambaiz.net/en/article/393/</link>
      <pubDate>Tue, 18 Jan 2022 21:26:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/393/</guid>
      <description>Previously, I tried to generate 250GB of data with Athena&amp;rsquo;s TPC-DS Connector and output it to S3 but it timed out even if I increased the Lambda resource to the maximum, so I do it with Glue this time.
Generate data with TPC-DS Connector in Athena&amp;rsquo;s Federated Query - sambaiz-net
Subscribe and activate TPC-DS connector for Glue.
Write a script like following. The scale is in GB, the same as Athena&amp;rsquo;s one.</description>
    </item>
    
    <item>
      <title>Redshift Serverless and other serverless ETL services, run query with Glue Data Catalog</title>
      <link>https://www.sambaiz.net/en/article/392/</link>
      <pubDate>Sun, 26 Dec 2021 22:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/392/</guid>
      <description>Redshift Serverless Redshift Serverless is a new feature that can use Redshift, a petabyte-scale DWH without launching an instance, announced at this year&amp;rsquo;s re:Invent. It is available existing features such as Redshift Spectrum, refer to S3 directly, Federated Query to RDS, and Redshift ML. I&amp;rsquo;m happy with this update as it is costly to keep the instance running for occasional usage such as analytics. The cost is charged for at</description>
    </item>
    
    <item>
      <title>Generate data with TPC-DS Connector in Athena&#39;s Federated Query</title>
      <link>https://www.sambaiz.net/en/article/391/</link>
      <pubDate>Sat, 25 Dec 2021 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/391/</guid>
      <description>Athena&amp;rsquo;s Federated Query is a feature to execute queries on non-S3 data sources such as DynamoDB and RDS through Lambda function which is a data sources connector. Implement Athena&amp;rsquo;s data source connectors and user defined functions (UDF) - sambaiz-net This article uses TPC-DS Connector in the AWS official repository. It generates the data of TPC-DS, which is a database benchmark in Decision Support. Although it is in the official repository,</description>
    </item>
    
    <item>
      <title>Treat Spark struct as map to expand to multiple rows with explode</title>
      <link>https://www.sambaiz.net/en/article/384/</link>
      <pubDate>Wed, 13 Oct 2021 02:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/384/</guid>
      <description>When you read data without specifying schema in Spark, the schema is automatically determined from the input as follows. Why can Athena v2 fail to query map columns in parquet source tables - sambaiz-net # {&amp;#34;aaa&amp;#34;:123,&amp;#34;ccc&amp;#34;:[123],&amp;#34;eee&amp;#34;:{&amp;#34;fff&amp;#34;:123},&amp;#34;hhh&amp;#34;:null} df = spark.read.json(&amp;#34;s3://hogefuga/testjson/&amp;#34;) df.printSchema() &amp;#39;&amp;#39;&amp;#39; root |-- aaa: long (nullable = true) |-- ccc: array (nullable = true) | |-- element: long (containsNull = true) |-- eee: struct (nullable = true) | |-- fff:</description>
    </item>
    
    <item>
      <title>Spark Web UI: Monitor Job Stages, Tasks distribution and SQL plan</title>
      <link>https://www.sambaiz.net/en/article/382/</link>
      <pubDate>Thu, 30 Sep 2021 13:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/382/</guid>
      <description>Spark Web UI is a tool for monitoring Jobs and Executors. Get Dockerfile that runs Spark based on maven:3.6-amazoncorretto-8 from aws-glue-samples, and start History Server with the path of EventLog output by Glue and authentication information to get it. $ git clone https://github.com/aws-samples/aws-glue-samples.git $ cd aws-glue-samples/utilities/Spark_UI/glue-3_0/ $ docker build -t glue/sparkui:latest . $ docker run -it -e SPARK_HISTORY_OPTS=&amp;#34;$SPARK_HISTORY_OPTS -Dspark.history.fs.logDirectory=s3a://path_to_eventlog -Dspark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID -Dspark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY&amp;#34; -p 18080:18080 glue/sparkui:latest &amp;#34;/opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer&amp;#34; Now, you can access</description>
    </item>
    
    <item>
      <title>Athena (Presto) and Glue (Spark) can return different values when running the same query</title>
      <link>https://www.sambaiz.net/en/article/370/</link>
      <pubDate>Sat, 03 Jul 2021 23:13:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/370/</guid>
      <description>AWS has multiple managed services that aggregate with SQL-like queries. If queries are executed ad-hoc, Presto-based Athena, which can quickly and easily query to tables in Glue&amp;rsquo;s data catalog, is handy, while if heavy queries are executed in batch, Spark-based Glue, which can avoid resource and time limitation, can be better. Therefore, they can be used properly depending on the case.
Redshift Serverless and other serverless aggregation services, run query with Glue Data Catalog - sambaiz-net</description>
    </item>
    
    <item>
      <title>Enable Job Bookmark of AWS Glue to process from the records following ones executed previously</title>
      <link>https://www.sambaiz.net/en/article/333/</link>
      <pubDate>Fri, 16 Apr 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/333/</guid>
      <description>Job Bookmark of AWS Glue is a feature that saves what records are processed, and prevent it from being executed next time. Parquet and ORC, which were not supported before 1.0, are now supported. AWS GlueでCSVを加工しParquetに変換してパーティションを切りA</description>
    </item>
    
    <item>
      <title>What is Apache Spark, RDD, DataFrame, DataSet, Action and Transformation</title>
      <link>https://www.sambaiz.net/en/article/208/</link>
      <pubDate>Wed, 13 Feb 2019 21:17:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/208/</guid>
      <description>What is Spark? Spark is high-performance general-purpose distributed processing system. It is used with distributed storage such as HDFS and S3, and cluster managers such as Hadoop YARN. HDFS(Hadoop Distributed File System)とは - sambaiz-net How Hadoop YARN allocates resources to applications and check how much resources are allocated - sambaiz-net It can be processed faster than Hadoop&amp;rsquo;s MapReduce by storing the intermediate</description>
    </item>
    
    <item>
      <title>Launch Hive execution environment with Cloudera Docker Image and execute query to JSON log</title>
      <link>https://www.sambaiz.net/en/article/128/</link>
      <pubDate>Thu, 24 Aug 2017 09:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/128/</guid>
      <description>What is Hive Hive is a data warehouse software built on Hadoop, which can access data sources such as HDFS with HiveSQL, an extended SQL. Sending a query, the job runs on MapReduce, Spark or Tez. It has fault tolerance and is mainly used in batch processing. What is HDFS(Hadoop Distributed File System) - sambaiz-net Presto, which access data sources with SQL likewise, can execute the query faster than Hive</description>
    </item>
    
  </channel>
</rss>
