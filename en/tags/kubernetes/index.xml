<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on sambaiz-net</title>
    <link>https://www.sambaiz.net/en/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on sambaiz-net</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Fri, 03 May 2024 17:46:00 +0900</lastBuildDate>
    <atom:link href="https://www.sambaiz.net/en/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Run tc command to set qdisc and limit network delays and bandwidth</title>
      <link>https://www.sambaiz.net/en/article/484/</link>
      <pubDate>Fri, 03 May 2024 17:46:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/484/</guid>
      <description>tc is a command or subsystem that configures a qdisc (queueing discipline) that queues packets and controls traffic before the kernel sends them to the network interface. Kubernetes CNI bandwidth plugin is also realized by tc, and can limit the bandwidth of pods with the following annotation. annotations: kubernetes.io/ingress-bandwidth: 1M kubernetes.io/egress-bandwidth: 1M I tried running it on an Amazon Linux 2023 t2.micro EC2 instance. $ ip link ... 2: enX0:</description>
    </item>
    <item>
      <title>Update replicas using K8s HorizontalPodAutoscaler and set determination interval and increase/decrease limit</title>
      <link>https://www.sambaiz.net/en/article/482/</link>
      <pubDate>Mon, 29 Apr 2024 19:01:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/482/</guid>
      <description>HorizontalPodAutoscaler (HPA) updates the replicas of the scaleTargetRef resource based on metrics. apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: testapp-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: testapp minReplicas: 1 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 If replicas is included in the Deployment&amp;rsquo;s manifest, it may be overwritten when it is applied, so it is better not to include it. However, note that</description>
    </item>
    <item>
      <title>Priority of K8s pods and preemption</title>
      <link>https://www.sambaiz.net/en/article/481/</link>
      <pubDate>Sat, 27 Apr 2024 21:16:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/481/</guid>
      <description>When you specify the priorityClassName of a Pod, the value of the corresponding PriorityClass becomes the Priority of the Pod.&#xA;apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: low-priority value: -10 globalDefault: false description: &amp;#39;pods can be preempted&amp;#39; apiVersion: apps/v1 kind: Deployment spec: template: spec: priorityClassName: low-priority When a Pod is scheduled and its preemptionPolicy is the default PreemptLowerPriority, preempt other Pods with a lower priority to reserve requested resources. Priority is also referred on eviction when a node runs out of resources.</description>
    </item>
    <item>
      <title>Get communication in Kubernetes cluster with Pixie&#39;s PxL script</title>
      <link>https://www.sambaiz.net/en/article/478/</link>
      <pubDate>Sun, 07 Apr 2024 21:06:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/478/</guid>
      <description>Install Pixie CLI and login. If you use Pixie from New Relic, pass api key included in the install commmand.&#xA;Install newrelic-bundle to EKS cluster with CDK and monitor it - sambaiz-net&#xA;$ bash -c &amp;#34;$(curl -fsSL https://withpixie.ai/install.sh)&amp;#34; $ px version Pixie CLI 0.8.2+Distribution.401c92c.20230531033620.1.jenkins $ px auth login --api_key=&amp;#39;xxxxx&amp;#39; Pixie CLI Authentication Successful Let&amp;rsquo;s write PxL Script to read and display the table of data collected by Pixie. Built-in scripts are helpful to write.</description>
    </item>
    <item>
      <title>Pods evicted without waiting for terminationGracePeriodSeconds due to lack resources of Kubernetes nodes</title>
      <link>https://www.sambaiz.net/en/article/476/</link>
      <pubDate>Sun, 31 Mar 2024 16:07:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/476/</guid>
      <description>If you specify only requests resources of memory or ephemeral-storage that includes emptyDir volume, pods can consume resources more than settings, and kubelet monitors following signals and may evict them to avoid running out of node resources.&#xA;memory.available nodefs.available nodefs.inodesFree imagefs.available imagefs.inodesFree pid.available These thresholds can be set using evictionSoft and evictionHard in KubeletConfiguration, and if the soft value is reached, there is an eviction-max-pod-grace-period until it is evicted, but if the hard value is also reached, pods are immediately evicted.</description>
    </item>
    <item>
      <title>Install ExternalDNS to an EKS cluster with CDK, and register Service or Ingress host to Route53</title>
      <link>https://www.sambaiz.net/en/article/475/</link>
      <pubDate>Wed, 20 Mar 2024 15:01:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/475/</guid>
      <description>ExternalDNS is an application that synchronizes Service or Ingress host with external DNS records. Route53 records can also be created using CDK, but it is hard to handle ELBs that are created asynchronously after cluster.addManifest() and are managed by controllers.&#xA;First, let&amp;rsquo;s create a Role to edit records.&#xA;const externalDNSRole = new iam.Role(scope, &amp;#39;ExternalDNSRole&amp;#39;, { roleName: `ExternalDNSRole-${cluster.clusterName}`, assumedBy: new iam.WebIdentityPrincipal( cluster.openIdConnectProvider.openIdConnectProviderArn, { &amp;#34;StringEquals&amp;#34;: new cdk.CfnJson(scope, &amp;#39;ExternalDNSRoleStringEquals&amp;#39;, { value: { [`${cluster.clusterOpenIdConnectIssuer}:aud`]: &amp;#39;sts.</description>
    </item>
    <item>
      <title>What do etcd, a distributed KVS with Raft, a consensus algorithm choose in the CAP/PACELC Theorem</title>
      <link>https://www.sambaiz.net/en/article/473/</link>
      <pubDate>Mon, 11 Mar 2024 23:55:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/473/</guid>
      <description>etcd is a distributed KVS that is also used in Kubernetes. Raspberry PiでおうちKubernetesクラスタを構築する - sambaiz-net Kubernetes docs say that etcd is a consistent and highly-available key value store, and so I wondered it compromises partition tolerance (P) in the CAP theorem. However, it feels difficult to</description>
    </item>
    <item>
      <title>Run logrotate sidecar in Kubernetes</title>
      <link>https://www.sambaiz.net/en/article/472/</link>
      <pubDate>Thu, 07 Mar 2024 01:05:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/472/</guid>
      <description>Develop an application outputting logs, package main import ( &amp;#34;os&amp;#34; &amp;#34;time&amp;#34; &amp;#34;github.com/sirupsen/logrus&amp;#34; ) func main() { logFile, err := os.OpenFile(&amp;#34;/var/log/app/test.log&amp;#34;, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666) if err != nil { logrus.Fatal(&amp;#34;failed to open file&amp;#34;, err) } logrus.SetOutput(logFile) i := 0 ticker := time.NewTicker(1 * time.Millisecond) for { &amp;lt;-ticker.C logrus.Print(i) i++ } } create image logrotate installed, $ cat Dockerfile_logrotate FROM alpine:latest RUN apk add --no-cache logrotate # RUN echo &amp;#39;/usr/sbin/logrotate /etc/logrotate.d/logrotate.conf&amp;#39; &amp;gt; /etc/periodic/daily/logrotate</description>
    </item>
    <item>
      <title>Install AWS Load Balancer Controller on EKS cluster and set up ALB Ingress</title>
      <link>https://www.sambaiz.net/en/article/471/</link>
      <pubDate>Mon, 26 Feb 2024 23:55:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/471/</guid>
      <description>AWS Load Balancer Controller is a controller that creates and manages ELBs for Kubernetes resources. It can be installed with a service account granted the required policies and Helm Chart, but if you create the cluster with CDK, this is done by specifying the version in albController. new eks.Cluster(this, &amp;#39;Cluster&amp;#39;, { ... albController: { version: eks.AlbControllerVersion.V2_6_2, }, }) An ALB is created by creating the Ingress as follows. Subnets can</description>
    </item>
    <item>
      <title>Live reload Go application running on local K8s using air and remote debug using delve from VSCode</title>
      <link>https://www.sambaiz.net/en/article/470/</link>
      <pubDate>Wed, 21 Feb 2024 19:41:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/470/</guid>
      <description>Live reload using air air is a tool that performs live reload for Go applications. $ go install github.com/cosmtrek/air@latest air init makes a settings file. $ air init $ cat .air.toml root = &amp;#34;.&amp;#34; tmp_dir = &amp;#34;tmp&amp;#34; [build] args_bin = [] bin = &amp;#34;./tmp/main&amp;#34; cmd = &amp;#34;go build -o ./tmp/main .&amp;#34; delay = 1000 ... For server code as follows, package main import ( &amp;#34;context&amp;#34; &amp;#34;log&amp;#34; &amp;#34;net/http&amp;#34; &amp;#34;os/signal&amp;#34; &amp;#34;syscall&amp;#34; )</description>
    </item>
    <item>
      <title>Describe resources required only in the environment with Kustomize Component</title>
      <link>https://www.sambaiz.net/en/article/469/</link>
      <pubDate>Thu, 15 Feb 2024 09:41:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/469/</guid>
      <description>I tried to run a DB only in the local environment with Kustomize, but an error occurred as follows. kustomizeでkubernetesのmanifestを環境ごとに生成する - sambaiz-net $ tree manifest manifest ├── base │ ├</description>
    </item>
    <item>
      <title>Install newrelic-bundle to EKS cluster with CDK and monitor it</title>
      <link>https://www.sambaiz.net/en/article/466/</link>
      <pubDate>Sat, 13 Jan 2024 21:07:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/466/</guid>
      <description>NewRelic Kubernetes Integration has various components, and nri-bundle Helm Chart bundles them. Proceeding with Guided install, the parameters to pass are generated, so I copied them to CDK. Looking at the chart, credentials can be passed as strings as it is or Secrets, so I imported them from SecretsManager with External Secrets. Install External Secrets Operator with CDK and make Secrets Manager data available as Kubernetes Secret - sambaiz-net const</description>
    </item>
    <item>
      <title>Install External Secrets Operator with CDK and make Secrets Manager data available as Kubernetes Secret</title>
      <link>https://www.sambaiz.net/en/article/465/</link>
      <pubDate>Thu, 11 Jan 2024 20:15:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/465/</guid>
      <description>External Secrets Operator (ESO) is an Operator that creates Kubernetes Secrets from secrets stored in external services, and is the successor to the deprecated Kubernetes External Secrets (KES). In this article, I install it with CDK and create a Secret that contains data from Secrets Manager. First, create an IAM Role to be granted to a ServiceAccount and install the operator with Helm Chart. const externalSecretsRole = new iam.Role(this, &amp;#39;ExternalSecretsRole&amp;#39;,</description>
    </item>
    <item>
      <title>IP address exhaustion with EKS cluster and migration to IPv6</title>
      <link>https://www.sambaiz.net/en/article/460/</link>
      <pubDate>Fri, 15 Dec 2023 01:02:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/460/</guid>
      <description>When I ran a large application on an EKS cluster, the subnet&amp;rsquo;s IP addresses ran out. Reason for IP address exhaustion Pod IP addresses are assigned by ipamd (IP Address Management Daemon) of VPC CNI, and these are from the instance&amp;rsquo;s secondary IP addresses. The number of secondary IP addresses are controlled with WARM_ENI_TARGET (Default: 1), which is the number of spare ENIs, or WARM_IP_TARGET (Default: None), which is the</description>
    </item>
    <item>
      <title>Setting security groups for node instances in an EKS cluster created with CDK</title>
      <link>https://www.sambaiz.net/en/article/459/</link>
      <pubDate>Tue, 05 Dec 2023 20:59:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/459/</guid>
      <description>securityGroup that can be passed when creating an aws_eks.Cluster is not associated with the node instances but with the control plane&amp;rsquo;s ENI.&#xA;There&amp;rsquo;s defaultCapacity field in Cluster, so I attempted to add a SG through it, but nothing happened. When an EKS cluster is created, if the DefaultCapacityType is NODEGROUP, which is the default, it creates a managed node group, which consists of defaultCapacity nodes, where operations such as drains are managed by EKS.</description>
    </item>
    <item>
      <title>Install Karpenter on an EKS cluster with CDK to auto-scale flexibility and quickly</title>
      <link>https://www.sambaiz.net/en/article/455/</link>
      <pubDate>Fri, 13 Oct 2023 20:35:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/455/</guid>
      <description>Karpenter is an OSS that scales a Kubernetes cluster developed by AWS. Compared to Cluster Autoscaler, it can perform fast and flexible provisioning not through Auto Scaling Group. It supports only EKS so far, but it seems to have possibility to support other cloud providers.&#xA;(PS: 2023-12-06) now available on AKS&#xA;In this article, I install Karpenter on an EKS cluster with CDK, and confirm the cluster to be auto-scaled when the number of replicas is changed.</description>
    </item>
    <item>
      <title>Launch an EKS cluster and register it to EMR on EKS with CDK to run Spark jobs</title>
      <link>https://www.sambaiz.net/en/article/434/</link>
      <pubDate>Mon, 02 Jan 2023 14:53:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/434/</guid>
      <description>EMR on EKS is a feature to run Spark on EKS. While normal EMR also manages Hadoop clusters, EMR on EKS is only responsible for starting containers. Launch an EMR cluster with AWS CLI and run Spark applications - sambaiz-net By running on Kubernetes, you can use tools and functions for Kubernetes to manage and monitor, and utilize resources left over if you have an existing cluster. It also enables</description>
    </item>
  </channel>
</rss>
