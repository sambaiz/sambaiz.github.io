<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Iceberg on sambaiz-net</title>
    <link>https://www.sambaiz.net/en/tags/iceberg/</link>
    <description>Recent content in Iceberg on sambaiz-net</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Fri, 18 Jul 2025 09:39:00 +0900</lastBuildDate>
    <atom:link href="https://www.sambaiz.net/en/tags/iceberg/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Creating and updating only the schema of Iceberg tables with dbt-athena</title>
      <link>https://www.sambaiz.net/en/article/539/</link>
      <pubDate>Fri, 18 Jul 2025 09:39:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/539/</guid>
      <description>&lt;p&gt;Currently, I don&amp;rsquo;t have the best practice to manage Iceberg tables in AWS. This is because there&amp;rsquo;s an &lt;a href=&#34;https://github.com/aws-cloudformation/cloudformation-coverage-roadmap/issues/1919&#34;&gt;issue&lt;/a&gt; where Iceberg tables updated with CDK (CloudFormation) are no longer treated as Iceberg tables. Creating them from Athena Notebooks with Spark is the easiest approach, but I was looking for a method that would be more suitable for operations from a review and deployment perspective, and I reached to &lt;a href=&#34;https://docs.getdbt.com/docs/core/connect-data-platform/athena-setup&#34;&gt;dbt-athena&lt;/a&gt;, the official dbt adapter for Athena.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streaming Writes to Iceberg Tables with Kinesis Data Firehose</title>
      <link>https://www.sambaiz.net/en/article/535/</link>
      <pubDate>Sat, 10 May 2025 13:06:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/535/</guid>
      <description>&lt;p&gt;Unlike regular tables that can be read just by placing files, Iceberg tables require to perform INSERTs to update metadata. If you use batch processing for this, there will be a lag before the latest data becomes available. To minimize this lag, you could implement your own streaming process using Kinesis Data Streams, but by &lt;a href=&#34;https://docs.aws.amazon.com/firehose/latest/dev/apache-iceberg-destination.html&#34;&gt;setting an Iceberg table as the Firehose destination&lt;/a&gt;, you can easily write to it.&lt;/p&gt;&#xA;&lt;p&gt;If you do Direct PUT to Firehose, you don&amp;rsquo;t need to go through KDS, but be careful not to hit the &lt;a href=&#34;https://docs.aws.amazon.com/firehose/latest/dev/apache-iceberg-considerations.html&#34;&gt;throughput limits&lt;/a&gt;. You can&amp;rsquo;t include multiple JSONs in a single record, so it&amp;rsquo;s hard to work around this. If you use KDS as the data source, you can aggregate with KPL.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Create BigQuery Tables for Apache Iceberg using dbt and Read them from Snowflake</title>
      <link>https://www.sambaiz.net/en/article/531/</link>
      <pubDate>Sun, 20 Apr 2025 23:55:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/531/</guid>
      <description>&lt;p&gt;When reading data stored in BigQuery from Snowflake, direct access is not possible, so it&amp;rsquo;s necessary to either reload it into Snowflake tables or export it to GCS. However, regarding the former approach, there are concerns about data and schema discrepancy due to losing the single source of truth, and in either case, when using both BigQuery and Snowflake, storage costs are doubled. For the latter approach, instead of storing data in BigQuery storage, it&amp;rsquo;s possible to operate it as an external table, but this requires an external engine like Spark for writing data.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Creating Iceberg Tables in S3 Tables from EMR Serverless, inserting data, and querying from Athena</title>
      <link>https://www.sambaiz.net/en/article/528/</link>
      <pubDate>Tue, 18 Feb 2025 20:47:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/528/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables.html&#34;&gt;S3 Tables&lt;/a&gt; is a storage specialized for Iceberg tables that was &lt;a href=&#34;https://aws.amazon.com/jp/blogs/news/aws-weekly-20241202-1/&#34;&gt;announced&lt;/a&gt; at re:Invent 2024. &lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-schema.html&#34;&gt;S3 Metadata&lt;/a&gt;, which was announced at the same time and enables to query object metadata such as last modified date, also writes them to S3 Tables.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/523/&#34;&gt;Walk through Iceberg metadata contents by creating tables, modifying schema and write mode, and writing data in Spark - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;The &lt;a href=&#34;https://aws.amazon.com/jp/s3/pricing/&#34;&gt;price&lt;/a&gt; in the Tokyo region is 0.0288 USD/GB (up to 50TB/month), which is about 10% higher than the standard S3 rate of USD 0.025/GB. In addition to this, there is a monitoring cost for the number of objects, and since there are currently no &lt;a href=&#34;https://aws.amazon.com/s3/storage-classes/&#34;&gt;storage classes&lt;/a&gt;, it isn&amp;rsquo;t possible to apply lifecycle to change old data to a cheaper tier, and &lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering.html&#34;&gt;Intelligent Tiering&lt;/a&gt;, which automatically changes the tier of objects that aren&amp;rsquo;t recently accessed. Besides, it should be noted that lifecycle transition requests incur cost about $0.01/1000 objects, and Intelligent-Tiering monitoring incurs cost about $0.0025/1000 objects.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Register Iceberg Tables in Glue Data Catalog to query from Athena and Snowflake</title>
      <link>https://www.sambaiz.net/en/article/525/</link>
      <pubDate>Thu, 30 Jan 2025 20:31:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/525/</guid>
      <description>&lt;p&gt;If you register Iceberg tables in the Glue Data Catalog, you can not only reference them from Athena and EMR etc. like other tables, but you can also create an &lt;a href=&#34;https://docs.snowflake.com/en/user-guide/tables-iceberg&#34;&gt;ICEBERG TABLE&lt;/a&gt; in Snowflake without specifying the schema, avoiding duplicate metadata management. It even has a feature for automatic &lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/enable-compaction.html&#34;&gt;compaction&lt;/a&gt; and &lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/enable-orphan-file-deletion.html&#34;&gt;deleting orphan files&lt;/a&gt; that are created when a job fails and not referenced from metadata. With traditional Hive partitions, partition information is registered in the Glue Data Catalog and retrieved via the GetPartitions API, which can cause latency and throttling issues when there are many partitions. Iceberg stores partition information in manifest files on S3, avoiding this problem.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Walk through Iceberg metadata contents by creating tables, modifying schema and write mode, and writing data in Spark</title>
      <link>https://www.sambaiz.net/en/article/523/</link>
      <pubDate>Sat, 25 Jan 2025 23:18:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/523/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://iceberg.apache.org/&#34;&gt;Apache Iceberg&lt;/a&gt; is an open table format that enables efficient processing of large amounts of data, and can be used by various systems such as &lt;a href=&#34;https://iceberg.apache.org/spark-quickstart/&#34;&gt;Spark&lt;/a&gt;, &lt;a href=&#34;https://trino.io/docs/current/connector/iceberg.html&#34;&gt;Trino&lt;/a&gt; and &lt;a href=&#34;https://docs.snowflake.com/en/user-guide/tables-iceberg&#34;&gt;Snowflake&lt;/a&gt;. In addition to ACID transaction, Schema Evolution, which allow you to change the table schema without rewriting the original data, and Time Travel, &lt;a href=&#34;https://iceberg.apache.org/docs/latest/partitioning/#icebergs-hidden-partitioning&#34;&gt;Hidden Partitioning&lt;/a&gt;, which does not rely on physical structures such as Hive&amp;rsquo;s day=xxxx/ and enables to do PARTITION BY logical values ​​that are not included in the table, such as day(event_ts) and month(event_ts), allows queries such as &amp;ldquo;WHERE event_ts BETWEEN 2025-01-01 AND 2025-03-01&amp;rdquo; without being aware of the granularity of the partitions, and &lt;a href=&#34;https://iceberg.apache.org/docs/latest/evolution/#partition-evolution&#34;&gt;Partition evolution&lt;/a&gt;, which changes the granularity without breaking the query, can also be performed.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
