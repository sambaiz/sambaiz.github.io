<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Llm on sambaiz-net</title>
    <link>https://www.sambaiz.net/en/tags/llm/</link>
    <description>Recent content in Llm on sambaiz-net</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Tue, 27 Aug 2024 20:06:00 +0900</lastBuildDate>
    <atom:link href="https://www.sambaiz.net/en/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Handle Actions that take too long or are too heavy for Agents for Bedrock by using RETURN_CONTROL and processing them in a Go client</title>
      <link>https://www.sambaiz.net/en/article/496/</link>
      <pubDate>Tue, 27 Aug 2024 20:06:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/496/</guid>
      <description>Agents for Bedrock can register and invoke Lambda functions as Actions. However, for processes that take a long time, there&amp;rsquo;s a risk of hitting Lambda timeouts or resource limits. Moreover, when considering running heavy processes in parallel or notifying users of progress, it can be inconvenient to call these from the Agent. ReturnControls solves this by not handling Actions on the Agent side, but instead returning the Action that should</description>
    </item>
    <item>
      <title>Create Agents for Bedrock using CDK and check that Lambda functions are called based on input</title>
      <link>https://www.sambaiz.net/en/article/495/</link>
      <pubDate>Tue, 27 Aug 2024 09:39:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/495/</guid>
      <description>Agents for Amazon Bedrock is a feature that builds generative AI agents using various foundation models of Bedrock to perform multi-step processes. It can call Lambda functions as needed and connect with services like OpenSearch Serverless to perform RAG. The Lambda functions called by Agents need to return responses in the following format. Also, it is necessary to set permissions for Bedrock for invocation in the Lambda&amp;rsquo;s Resource-based policy, not</description>
    </item>
    <item>
      <title>Generate SQL from natural language and access databases with AgentExecutor of Langchain&#39;s SQL Database Toolkits</title>
      <link>https://www.sambaiz.net/en/article/488/</link>
      <pubDate>Sun, 26 May 2024 20:43:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/488/</guid>
      <description>Langchain&amp;rsquo;s SQL Database Toolkits provides a feature to generate SQL and access databases.&#xA;create_sql_agent() adds tools such as&#xA;query_sql_database_tool: Execute SQL info_sql_database_tool: Get the schema of the table list_sql_database_tool: Get list of table names query_sql_checker_tool: Rewrite if the SQL is incorrect in SQLDatabaseToolkit.get_tools(), creates LLM, fills them in prompt etc., and returns AgentExecutor.&#xA;Create an LLM agent from a graph containing a cycle using the LangChain ecosystem LangGraph - sambaiz-net</description>
    </item>
    <item>
      <title>Create an LLM agent from a graph containing a cycle using the LangChain ecosystem LangGraph</title>
      <link>https://www.sambaiz.net/en/article/485/</link>
      <pubDate>Sun, 12 May 2024 21:38:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/485/</guid>
      <description>LangGraph is an ecosystem of LangChain, and a Runnable is created by adding nodes to a StateGraph or MessageGraph whose state is List[BaseMessage], connecting them with add_edge() or add_conditional_edges(), and calling compile(). LangChain Expression Language (LCEL) can also connect Runnables to create a DAG, but LangGraph can also express cycles.&#xA;from langchain_openai import ChatOpenAI from langchain_core.messages import HumanMessage from langgraph.graph import END, StateGraph from langchain_core.tools import tool from langgraph.prebuilt import ToolNode from typing import TypedDict, Annotated import json def add_messages(left: list, right: list): return left + right class AgentState(TypedDict): messages: Annotated[list, add_messages] # export OPENAI_API_KEY=your-api-key model = ChatOpenAI(model=&amp;#34;gpt-4-turbo&amp;#34;, temperature=0) # graph = MessageGraph() graph = StateGraph(AgentState) @tool def multiply(left: int, right: int): &amp;#34;&amp;#34;&amp;#34;Multiplies two numbers together.</description>
    </item>
    <item>
      <title>Fine-tuning OpenAI&#39;s GPT with Japanese Prime Minister&#39;s speech in the Diet</title>
      <link>https://www.sambaiz.net/en/article/452/</link>
      <pubDate>Mon, 11 Sep 2023 23:22:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/452/</guid>
      <description>OpenAI provides APIs to create conversation and convert text to vector, and also fine-tune models with your own dataset, which enables us to improve the quality of the output or save the cost of few-shot learning. # !pip install openai import openai import numpy as np response = openai.ChatCompletion.create( model=&amp;#34;gpt-3.5-turbo&amp;#34;, messages=[{ &amp;#34;role&amp;#34;: &amp;#34;user&amp;#34;, # &amp;#34;system&amp;#34;, &amp;#34;user&amp;#34;, or &amp;#34;assistant&amp;#34; &amp;#34;content&amp;#34;: &amp;#34;Is this a pen?&amp;#34; }], temperature=0.5, ) print(response.choices[0].message) &amp;#39;&amp;#39;&amp;#39; { &amp;#34;role&amp;#34;:</description>
    </item>
  </channel>
</rss>
