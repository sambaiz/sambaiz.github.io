<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Spark on sambaiz-net</title>
    <link>https://www.sambaiz.net/en/tags/spark/</link>
    <description>Recent content in Spark on sambaiz-net</description>
    <generator>Hugo</generator>
    <language>en</language>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Tue, 18 Feb 2025 20:47:00 +0900</lastBuildDate>
    <atom:link href="https://www.sambaiz.net/en/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Creating Iceberg Tables in S3 Tables from EMR Serverless, inserting data, and querying from Athena</title>
      <link>https://www.sambaiz.net/en/article/528/</link>
      <pubDate>Tue, 18 Feb 2025 20:47:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/528/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables.html&#34;&gt;S3 Tables&lt;/a&gt; is a storage specialized for Iceberg tables that was &lt;a href=&#34;https://aws.amazon.com/jp/blogs/news/aws-weekly-20241202-1/&#34;&gt;announced&lt;/a&gt; at re:Invent 2024. &lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-schema.html&#34;&gt;S3 Metadata&lt;/a&gt;, which was announced at the same time and enables to query object metadata such as last modified date, also writes them to S3 Tables.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/523/&#34;&gt;Walk through Iceberg metadata contents by creating tables, modifying schema and write mode, and writing data in Spark - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Create a Table bucket. When you enable &lt;a href=&#34;https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-integrating-aws.html&#34;&gt;integration&lt;/a&gt; with analytics services like Athena, a role for Lake Formation and a catalog named s3tablescatalog are created, allowing you to query from these services.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Running Spark MLlib on EMR Serverless from EMR Studio&#39;s Jupyter Notebook</title>
      <link>https://www.sambaiz.net/en/article/527/</link>
      <pubDate>Tue, 11 Feb 2025 14:52:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/527/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio.html&#34;&gt;EMR Studio&lt;/a&gt; is a Jupyter Notebook-based IDE that runs on EMR clusters. While cluster and S3 charges apply, the service itself is free. With &lt;a href=&#34;https://aws.amazon.com/emr/serverless&#34;&gt;EMR Serverless&lt;/a&gt;, you can use it easily without having to maintain constantly running clusters.&lt;/p&gt;&#xA;&lt;p&gt;As a similar service, Athena for Apache Spark is available and while it&amp;rsquo;s easier to use as an extension of regular Athena, it has some &lt;a href=&#34;https://docs.aws.amazon.com/athena/latest/ug/notebooks-spark-considerations-and-limitations.html&#34;&gt;limitations&lt;/a&gt;, such as no support for MLlib. Additionally, EMR Studio offers richer features like &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-create-studio.html&#34;&gt;external IdP authentication&lt;/a&gt; and &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-studio-git-repo.html&#34;&gt;Git integration&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Register Iceberg Tables in Glue Data Catalog to query from Athena and Snowflake</title>
      <link>https://www.sambaiz.net/en/article/525/</link>
      <pubDate>Thu, 30 Jan 2025 20:31:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/525/</guid>
      <description>&lt;p&gt;If you register Iceberg tables in the Glue Data Catalog, you can not only reference them from Athena and EMR etc. like other tables, but you can also create an &lt;a href=&#34;https://docs.snowflake.com/en/user-guide/tables-iceberg&#34;&gt;ICEBERG TABLE&lt;/a&gt; in Snowflake without specifying the schema, avoiding duplicate metadata management. It even has a feature for automatic &lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/enable-compaction.html&#34;&gt;compaction&lt;/a&gt; and &lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/enable-orphan-file-deletion.html&#34;&gt;deleting orphan files&lt;/a&gt; that are created when a job fails and not referenced from metadata.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/523/&#34;&gt;Walk through Iceberg metadata contents by creating tables, modifying schema and write mode, and writing data in Spark - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Walk through Iceberg metadata contents by creating tables, modifying schema and write mode, and writing data in Spark</title>
      <link>https://www.sambaiz.net/en/article/523/</link>
      <pubDate>Sat, 25 Jan 2025 23:18:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/523/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://iceberg.apache.org/&#34;&gt;Apache Iceberg&lt;/a&gt; is an open table format that enables efficient processing of large amounts of data, and can be used by various systems such as &lt;a href=&#34;https://iceberg.apache.org/spark-quickstart/&#34;&gt;Spark&lt;/a&gt;, &lt;a href=&#34;https://trino.io/docs/current/connector/iceberg.html&#34;&gt;Trino&lt;/a&gt; and &lt;a href=&#34;https://docs.snowflake.com/en/user-guide/tables-iceberg&#34;&gt;Snowflake&lt;/a&gt;. In addition to Schema Evolution, which allow you to change the table schema without rewriting the original data, and Time Travel, &lt;a href=&#34;https://iceberg.apache.org/docs/latest/partitioning/#icebergs-hidden-partitioning&#34;&gt;Hidden Partitioning&lt;/a&gt;, which does not rely on physical structures such as Hive&amp;rsquo;s day=xxxx/ and enables to do PARTITION BY logical values ​​that are not included in the table, such as day(event_ts) and month(event_ts), allows queries such as &amp;ldquo;WHERE event_ts BETWEEN 2025-01-01 AND 2025-03-01&amp;rdquo; without being aware of the granularity of the partitions, and &lt;a href=&#34;https://iceberg.apache.org/docs/latest/evolution/#partition-evolution&#34;&gt;Partition evolution&lt;/a&gt;, which changes the granularity without breaking the query, can also be performed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Avoiding OOM in count-distinct operations on massive datasets using HyperLogLog&#43;&#43;, a probabilistic cardinality estimation algorithm</title>
      <link>https://www.sambaiz.net/en/article/497/</link>
      <pubDate>Mon, 02 Sep 2024 21:08:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/497/</guid>
      <description>&lt;p&gt;When counting unique elements, such as the number of users in access logs, we often execute queries like &amp;ldquo;count(distinct column_name)&amp;rdquo;. For massive datasets, regular count() operations can be scaled by splitting the data and summing the results. However, this method doesn&amp;rsquo;t work with distinct operations, which can lead to extreme slowdowns due to memory shortages or, in the worst case, failure due to OOM errors.&lt;/p&gt;&#xA;&lt;p&gt;This issue is known as the &lt;a href=&#34;https://en.wikipedia.org/wiki/Count-distinct_problem&#34;&gt;Count-distinct problem (cardinality estimation problem)&lt;/a&gt;, and several algorithms have been proposed to estimate cardinality (unique count) without retaining all elements. One of them is HyperLogLog, and its improved version, HyperLogLog++, is widely implemented as &lt;a href=&#34;https://github.com/apache/spark/blob/c58148da5496245403b55c3fc423d35f3a669c79/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala#L469&#34;&gt;approx_count_distinct()&lt;/a&gt; in Spark and &lt;a href=&#34;https://github.com/prestodb/presto/blob/2bceaa02b982f2b10d4d577fac9b3c8944c36ace/presto-main/src/main/java/com/facebook/presto/operator/aggregation/ApproximateCountDistinctAggregation.java#L40&#34;&gt;approx_distinct()&lt;/a&gt; in Presto.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Share variables to executors using Spark&#39;s Broadcast variables and Accumulator</title>
      <link>https://www.sambaiz.net/en/article/494/</link>
      <pubDate>Thu, 22 Aug 2024 09:39:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/494/</guid>
      <description>&lt;p&gt;There are two mechanisms for sharing variables among executors: &lt;a href=&#34;https://spark.apache.org/docs/3.5.1/rdd-programming-guide.html#broadcast-variables&#34;&gt;Broadcast variables&lt;/a&gt;  and &lt;a href=&#34;https://spark.apache.org/docs/3.5.1/rdd-programming-guide.html#accumulators&#34;&gt;Accumulator&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Broadcast variables are read-only variables that each node caches. Common data is broadcast automatically per stage, and it is deserialized before tasks. If there is data that multiple stages use or nodes should have in deserialized form, explicit broadcasting using SparkContext.broadcast() could be helpful.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/382/&#34;&gt;Spark Web UI: Monitor Job Stages, Tasks distribution and SQL plan - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Accumulator are variables that can be only added.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Call Livy&#39;s REST API to run a Spark job</title>
      <link>https://www.sambaiz.net/en/article/489/</link>
      <pubDate>Wed, 29 May 2024 23:27:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/489/</guid>
      <description>&lt;p&gt;Call Livy&amp;rsquo;s &lt;a href=&#34;https://livy.apache.org/docs/latest/rest-api.html&#34;&gt;REST API&lt;/a&gt; to run a Spark job.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/486/&#34;&gt;Install Livy on EMR on EKS and run Spark jobs from local Jupyter notebooks with Sparkmagic - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;First, create a &lt;a href=&#34;https://livy.apache.org/docs/latest/rest-api.html#session&#34;&gt;Session&lt;/a&gt; and wait until the status becomes idle.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2;&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; requests&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; json&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; time&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;livy_url &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;http://&amp;lt;livy_url&amp;gt;:8998&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# config for EMR on EKS&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;config &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;kind&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;spark&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;driverMemory&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;1000M&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;driverCores&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;conf&amp;#34;&lt;/span&gt;: {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;spark.kubernetes.namespace&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;emr&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;spark.kubernetes.container.image&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;public.ecr.aws/emr-on-eks/spark/emr-7.1.0:latest&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;spark.kubernetes.authenticate.driver.serviceAccountName&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;emr-containers-sa-spark-driver-*****&amp;#34;&lt;/span&gt;, &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;spark.kubernetes.file.upload.path&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;s3://&amp;lt;bucket&amp;gt;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  }&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;create_session&lt;/span&gt;():&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  response &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;post(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f1fa8c&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;{&lt;/span&gt;livy_url&lt;span style=&#34;color:#f1fa8c&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;/sessions&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    data&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;json&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;dumps(config),&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    headers&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#39;Content-Type&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#39;application/json&amp;#39;&lt;/span&gt;}&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  )&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#ff79c6&#34;&gt;if&lt;/span&gt; response&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;status_code &lt;span style=&#34;color:#ff79c6&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;201&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;Failed to create session:&amp;#34;&lt;/span&gt;, response&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;content)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    exit(&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#ff79c6&#34;&gt;return&lt;/span&gt; response&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;json()[&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#39;id&amp;#39;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#50fa7b&#34;&gt;wait_for_session_idle&lt;/span&gt;(session_id):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#ff79c6&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;True&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    response &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;get(&lt;span style=&#34;color:#f1fa8c&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;{&lt;/span&gt;livy_url&lt;span style=&#34;color:#f1fa8c&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;/sessions/&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;{&lt;/span&gt;session_id&lt;span style=&#34;color:#f1fa8c&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    state &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; response&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;json()[&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#39;state&amp;#39;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#f1fa8c&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;Session state: &lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;{&lt;/span&gt;state&lt;span style=&#34;color:#f1fa8c&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;if&lt;/span&gt; state &lt;span style=&#34;color:#ff79c6&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#39;dead&amp;#39;&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;Session is dead:&amp;#34;&lt;/span&gt;, session_id)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      exit(&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#ff79c6&#34;&gt;if&lt;/span&gt; state &lt;span style=&#34;color:#ff79c6&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#39;idle&amp;#39;&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#ff79c6&#34;&gt;break&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    time&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;sleep(&lt;span style=&#34;color:#bd93f9&#34;&gt;5&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;session_id &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; create_session()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wait_for_session_idle(session_id)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Pass codes to the session, create a &lt;a href=&#34;https://livy.apache.org/docs/latest/rest-api.html#statement&#34;&gt;Statement&lt;/a&gt;, wait for completion, and get the result.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Install Livy on EMR on EKS and run Spark jobs from local Jupyter notebooks with Sparkmagic</title>
      <link>https://www.sambaiz.net/en/article/486/</link>
      <pubDate>Wed, 22 May 2024 23:55:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/486/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://livy.apache.org/&#34;&gt;Apache Livy&lt;/a&gt; provides a REST API for interacting with Spark clusters, and &lt;a href=&#34;https://github.com/jupyter-incubator/sparkmagic&#34;&gt;Sparkmagic&lt;/a&gt; calls this API to run jobs on remote Spark clusters from Jupyter Notebooks. It is also used by Athena for Apache Spark, and being able to run jobs interactively and check the results is useful for debugging and running ad-hoc queries.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/438/&#34;&gt;Athena for Apache Spark の Notebook で DataFrame.toPandas().plot() した際の日本語が文字化けしないようにする - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Clustering by k-means method with MLlib of Spark</title>
      <link>https://www.sambaiz.net/en/article/446/</link>
      <pubDate>Sun, 09 Apr 2023 17:08:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/446/</guid>
      <description>&lt;p&gt;Spark has &lt;a href=&#34;https://spark.apache.org/docs/latest/ml-guide.html&#34;&gt;MLlib&lt;/a&gt; which is a library for machine learning.&#xA;This article, try clustering using &lt;a href=&#34;https://spark.apache.org/docs/latest/ml-clustering.html#k-means&#34;&gt;Kmeans&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;K-means is a clustering method that randomly assigns each data to one of a pre-determined number of clusters first, computes the center of each cluster,&#xA;and then updates the cluster assignment of each data to the cluster whose center is closest, which repeats until convergence.&#xA;Kmeans is implemented in k-means++ that converges faster, and its default distance measure is euclidean.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Make EMR clusters&#39; scale-in faster with Task nodes</title>
      <link>https://www.sambaiz.net/en/article/445/</link>
      <pubDate>Sun, 19 Mar 2023 22:51:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/445/</guid>
      <description>&lt;p&gt;EMR cluster &lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-master-core-task-nodes.html&#34;&gt;consists&lt;/a&gt; of a Master (primary) node and Core nodes and Task nodes.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/427/&#34;&gt;How Hadoop YARN allocates resources to applications and check how much resources are allocated - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Resources of both core nodes and task nodes are used to run tasks, but core nodes are HDFS&amp;rsquo;s DataNode while task nodes aren&amp;rsquo;t.&#xA;Therefore, core nodes need to be decommissioned and then terminated to prevent data loss, but replication bandwidth is limited to prevent spikes, so &lt;a href=&#34;https://aws.amazon.com/jp/blogs/big-data/best-practices-for-resizing-and-automatic-scaling-in-amazon-emr/&#34;&gt;it takes time&lt;/a&gt; to scale in.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Launch an EKS cluster and register it to EMR on EKS with CDK to run Spark jobs</title>
      <link>https://www.sambaiz.net/en/article/434/</link>
      <pubDate>Mon, 02 Jan 2023 14:53:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/434/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/emr/latest/EMR-on-EKS-DevelopmentGuide/emr-eks.html&#34;&gt;EMR on EKS&lt;/a&gt; is a feature to run Spark on EKS.&#xA;While normal EMR also manages Hadoop clusters, EMR on EKS is only responsible for starting containers.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/409/&#34;&gt;Launch an EMR cluster with AWS CLI and run Spark applications - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;By running on Kubernetes, you can use tools and functions for Kubernetes to manage and monitor, and utilize resources left over if you have an existing cluster.&#xA;It also enables to use knowledge about Kubernetes learned from other applications for aggregation and analysis, and also may be vice versa.&#xA;However, some knowledge about Kubernetes and EKS is required.&#xA;You can start using normal EMR without knowledge of YARN, but I think EMR on EKS will be difficult to even operate with no knowledge.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Develop Spark Applications in Scala, deploy with GitHub Actions, and perform remote debugging on EMR</title>
      <link>https://www.sambaiz.net/en/article/420/</link>
      <pubDate>Fri, 21 Oct 2022 23:36:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/420/</guid>
      <description>&lt;p&gt;Spark provides Java and Python APIs in addition to Scala, which is used for developing Spark itself.&#xA;You can choose among them depending on the technical stack and technologies used in other components, etc.&lt;/p&gt;&#xA;&lt;p&gt;While Python has highly compatible with data analysis and machine learning skill sets and easy to edit and run on Glue Studio, the error is hard to understand, and the performance also has disadvantages because it needs to exchange the data between JVM and Python Workers.&#xA;Also, if the Python interpreter, which is not controlled by the JVM, try to use more memory than a resource manager such as YARN has allocated, the executor can be killed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Build Spark and debug it remotely at IntelliJ</title>
      <link>https://www.sambaiz.net/en/article/419/</link>
      <pubDate>Sun, 09 Oct 2022 19:06:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/419/</guid>
      <description>&lt;h2 id=&#34;build-at-the-command-prompt&#34;&gt;&lt;a href=&#34;https://spark.apache.org/docs/3.3.0/building-spark.html&#34;&gt;Build at the command prompt&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git clone &lt;span style=&#34;color:#6272a4&#34;&gt;--branch v3.3.0 --depth 1 https://github.com/apache/spark.git &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Install Java 8 with &lt;a href=&#34;https://asdf-vm.com/guide/getting-started.html#_3-install-asdf&#34;&gt;asdf&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2;&#34;&gt;&lt;code class=&#34;language-sql&#34; data-lang=&#34;sql&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ brew install asdf&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ echo &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;e &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;\n. $(brew --prefix asdf)/libexec/asdf.sh&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; ${ZDOTDIR:&lt;span style=&#34;color:#ff79c6&#34;&gt;-~&lt;/span&gt;}&lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt;.zshrc&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ asdf &lt;span style=&#34;color:#6272a4&#34;&gt;--version&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;&lt;/span&gt;v0.&lt;span style=&#34;color:#bd93f9&#34;&gt;10&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ asdf plugin&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;add&lt;/span&gt; java&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ asdf list&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;all&lt;/span&gt; java&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ asdf install java corretto&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;8&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;342&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;07&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;3&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ asdf &lt;span style=&#34;color:#ff79c6&#34;&gt;global&lt;/span&gt; java corretto&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;8&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;342&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;07&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;3&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ echo &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;. ~/.asdf/plugins/java/set-java-home.zsh&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;~/&lt;/span&gt;.zprofile&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ java &lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;version&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;openjdk &lt;span style=&#34;color:#ff79c6&#34;&gt;version&lt;/span&gt; &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;1.8.0_342&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;OpenJDK Runtime Environment Corretto&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;8&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;342&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;07&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;3&lt;/span&gt; (build &lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;8&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;0&lt;/span&gt;_342&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;b07)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;OpenJDK &lt;span style=&#34;color:#bd93f9&#34;&gt;64&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;Bit&lt;/span&gt; Server VM Corretto&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;8&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;342&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;07&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;3&lt;/span&gt; (build &lt;span style=&#34;color:#bd93f9&#34;&gt;25&lt;/span&gt;.&lt;span style=&#34;color:#bd93f9&#34;&gt;342&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;-&lt;/span&gt;b07, mixed &lt;span style=&#34;color:#ff79c6&#34;&gt;mode&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Check the build&amp;rsquo;s success.&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;export&lt;/span&gt; &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;MAVEN_OPTS&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;-Xss64m -Xmx2g -XX:ReservedCodeCacheSize=1g&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ ./build/mvn -DskipTests clean package&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;build-at-intellij&#34;&gt;&lt;a href=&#34;https://spark.apache.org/developer-tools.html&#34;&gt;Build at IntelliJ&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Open codes as Maven Project from &amp;ldquo;New &amp;gt; Project from Existing Sources.&amp;rdquo;&#xA;There is JDK in ~/.asdf/installs/java/, so make hidden directory visible with &amp;ldquo;Command + Shift + .&amp;rdquo;, and choose it.&#xA;After that, run &amp;ldquo;Generate Sources and Update Folders For All Projects&amp;rdquo; from Maven window, and then Build Project becomes successful.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Aggregate logs of spark running on an EMR cluster with Fluent Bit</title>
      <link>https://www.sambaiz.net/en/article/416/</link>
      <pubDate>Sun, 04 Sep 2022 14:44:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/416/</guid>
      <description>&lt;p&gt;If Spark jobs run on Cluster mode, the logs are not outputted to step/ directory, so it is hard to check it on the console,&#xA;so try aggregating them to New Relic.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/409/&#34;&gt;Launch an EMR cluster with AWS CLI and run Spark applications - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/399/&#34;&gt;Monitor infrastructure and applications with New Relic - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;option-1-sending-logs-with-self-installed-fluent-bit&#34;&gt;Option 1. Sending logs with self installed fluent bit&lt;/h2&gt;&#xA;&lt;p&gt;Install &lt;a href=&#34;https://docs.fluentbit.io/manual/about/fluentd-and-fluent-bit&#34;&gt;Fluent Bit&lt;/a&gt; that is memory saving fluentd, and send logs with it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Settings for running Spark on EMR</title>
      <link>https://www.sambaiz.net/en/article/414/</link>
      <pubDate>Sat, 13 Aug 2022 19:53:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/414/</guid>
      <description>&lt;p&gt;EMR and Glue are managed services that run Spark applications on AWS.&#xA;Glue is easy to run ETL jobs with serverless, while EMR allows fine-tuning of resources and parameters.&#xA;In other words, if the settings are not appropriate, the resources cannot be fully used, and tasks can fail due to OOM even if there is excess memory.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;img_container&#34;&gt;&lt;a href=&#34;https://www.sambaiz.net/article/414/images/resource.png&#34;&gt;&#xA;    &lt;img style=&#34;max-width: 100%; width: auto; height: auto;&#34; src=&#34;https://www.sambaiz.net/article/414/images/resource_hu_c68de87341fa4011.png&#34; width=&#34;600&#34; height=&#34;356&#34; alt=&#34;a resource that isn&amp;#39;t fully used&#34;&gt;&#xA;&lt;/a&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;p&gt;In the CLI, settings can be passed as json string or a file with &amp;ndash;configurations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Launch an EMR cluster with AWS CLI and run Spark applications</title>
      <link>https://www.sambaiz.net/en/article/409/</link>
      <pubDate>Wed, 22 Jun 2022 00:05:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/409/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/jp/emr/&#34;&gt;Amazon EMR&lt;/a&gt; is the service that launches a cluster installed Spark, Hive, and Presto on EC2 or EKS. While Glue, the managed Spark service, can easily run Spark ETL jobs with serverless, EMR has the advantage of excellent cost performance by spot instances etc., and also fine-tuning is available, but now that &lt;a href=&#34;https://aws.amazon.com/jp/emr/serverless/&#34;&gt;EMR Serverless&lt;/a&gt; has been released, the difference has narrowed a little. Glue also has handy features such as &lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/aws-glue-api-crawler-pyspark-extensions-dynamic-frame.html&#34;&gt;DynamicFrame&lt;/a&gt;, which doesn&amp;rsquo;t need to be specified the schema, and Bookmark, which makes processing being executed from the previous continuation. However, if heavy processing is executed repeatedly, it can be expensive, besides, it causes to reach quotas such as DPU, so it would be better to use it properly.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Redshift Serverless and other serverless ETL services, run query with Glue Data Catalog</title>
      <link>https://www.sambaiz.net/en/article/392/</link>
      <pubDate>Sun, 26 Dec 2021 22:03:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/392/</guid>
      <description>&lt;h2 id=&#34;redshift-serverless&#34;&gt;Redshift Serverless&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/jp/redshift/redshift-serverless/&#34;&gt;Redshift Serverless&lt;/a&gt; is a new feature that can use Redshift, a petabyte-scale DWH without launching an instance, announced at this year&amp;rsquo;s re:Invent.&#xA;It is available existing features such as &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/c-using-spectrum.html&#34;&gt;Redshift Spectrum&lt;/a&gt;, refer to S3 directly, &lt;a href=&#34;https://docs.aws.amazon.com/ja_jp/redshift/latest/dg/federated-overview.html&#34;&gt;Federated Query&lt;/a&gt; to RDS, and &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/dg/machine_learning.html&#34;&gt;Redshift ML&lt;/a&gt;.&#xA;I&amp;rsquo;m happy with this update as it is costly to keep the instance running for occasional usage such as analytics.&lt;/p&gt;&#xA;&lt;p&gt;The &lt;a href=&#34;https://aws.amazon.com/redshift/pricing&#34;&gt;cost&lt;/a&gt; is charged for at least 1 minute &lt;a href=&#34;https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-serverless.html#serverless-considerations&#34;&gt;RPU&lt;/a&gt; time and storage. RPU is a resource unit that contains 2vCPU and 16GiB memory and scales automatically.&#xA;The rate is &lt;strong&gt;$0.45/RPU hour&lt;/strong&gt; in Oregon, whereas Tokyo is set relatively high at &lt;strong&gt;$0.70/RPU hour&lt;/strong&gt;.&#xA;Since dc2.large (2vCPU, 15 GiB memory) instance is &lt;strong&gt;$0.314/hour&lt;/strong&gt; on-demand in Tokyo, if the resource usage is not over 40% on average due to like infrequent use or irregularity, it seems that the cost can be suppressed.&#xA;If RI is applied to the maximum, the rate becomes &lt;strong&gt;$0.110/hour&lt;/strong&gt; so the threshold drop to 15%, but there is also an operational advantage that you do not have to worry about the scale. In addition, there is no charge for the load amount of Redshift Spectrum.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Treat Spark struct as map to expand to multiple rows with explode</title>
      <link>https://www.sambaiz.net/en/article/384/</link>
      <pubDate>Wed, 13 Oct 2021 02:30:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/384/</guid>
      <description>&lt;p&gt;When you &lt;a href=&#34;https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrameReader.json.html&#34;&gt;read data&lt;/a&gt; without specifying schema in Spark, the schema is automatically determined from the input as follows.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/415/&#34;&gt;Why can Athena v2 fail to query map columns in parquet source tables - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2;&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# {&amp;#34;aaa&amp;#34;:123,&amp;#34;ccc&amp;#34;:[123],&amp;#34;eee&amp;#34;:{&amp;#34;fff&amp;#34;:123},&amp;#34;hhh&amp;#34;:null}&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; spark&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;read&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;json(&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;s3://hogefuga/testjson/&amp;#34;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;df&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;printSchema()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#39;&amp;#39;&amp;#39;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;root&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt; |-- aaa: long (nullable = true)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt; |-- ccc: array (nullable = true)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt; |    |-- element: long (containsNull = true)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt; |-- eee: struct (nullable = true)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt; |    |-- fff: long (nullable = true)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt; |-- hhh: string (nullable = true)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This works well in most cases, but if the field that assumes map is determined as struct, or if the field is determined as string as it contains only null, processings may fail by mismatch of function inputs.&#xA;&lt;a href=&#34;https://spark.apache.org/docs/3.1.1/api/sql/index.html#explode&#34;&gt;explode()&lt;/a&gt; is a function that expands array and map into multiple rows.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark Web UI: Monitor Job Stages, Tasks distribution and SQL plan</title>
      <link>https://www.sambaiz.net/en/article/382/</link>
      <pubDate>Thu, 30 Sep 2021 13:00:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/382/</guid>
      <description>&lt;p&gt;Spark &lt;a href=&#34;https://spark.apache.org/docs/latest/web-ui.html&#34;&gt;Web UI&lt;/a&gt; is a tool for monitoring Jobs and Executors.&lt;/p&gt;&#xA;&lt;p&gt;Get Dockerfile that runs Spark based on maven:3.6-amazoncorretto-8 from &lt;a href=&#34;https://github.com/aws-samples/aws-glue-samples/tree/b76ad0583bdc66e9e5a903f9da3a953c9f6aac4f&#34;&gt;aws-glue-samples&lt;/a&gt;, and &lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-history.html#monitor-spark-ui-history-local&#34;&gt;start&lt;/a&gt; &lt;a href=&#34;https://spark.apache.org/docs/latest/monitoring.html&#34;&gt;History Server&lt;/a&gt; with the path of EventLog output by Glue and authentication information to get it.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;img_container&#34;&gt;&lt;a href=&#34;https://www.sambaiz.net/article/382/images/eventlog-settings.png&#34;&gt;&#xA;    &lt;img style=&#34;max-width: 100%; width: auto; height: auto;&#34; src=&#34;https://www.sambaiz.net/article/382/images/eventlog-settings_hu_75f3daf3dc9d5627.png&#34; width=&#34;385&#34; height=&#34;167&#34; alt=&#34;Output EventLog config&#34;&gt;&#xA;&lt;/a&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ git clone https://github.com/aws-samples/aws-glue-samples.git&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;cd&lt;/span&gt; aws-glue-samples/utilities/Spark_UI/glue-3_0/&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ docker build -t glue/sparkui:latest .&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ docker run -it -e &lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;SPARK_HISTORY_OPTS&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;$SPARK_HISTORY_OPTS&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt; -Dspark.history.fs.logDirectory=s3a://path_to_eventlog &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;-Dspark.hadoop.fs.s3a.access.key=&lt;/span&gt;&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;$AWS_ACCESS_KEY_ID&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt; -Dspark.hadoop.fs.s3a.secret.key=&lt;/span&gt;&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;$AWS_SECRET_ACCESS_KEY&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;&lt;/span&gt; &#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;-p 18080:18080 glue/sparkui:latest &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;/opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, you can access http://localhost:18080 and select Application.&#xA;The timeline of executing Jobs and adding Executors is displayed.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Athena (Presto) and Glue (Spark) can return different values when running the same query</title>
      <link>https://www.sambaiz.net/en/article/370/</link>
      <pubDate>Sat, 03 Jul 2021 23:13:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/370/</guid>
      <description>&lt;p&gt;AWS has multiple managed services that aggregate with SQL-like queries.&#xA;If queries are executed ad-hoc, Presto-based Athena, which can quickly and easily query to tables in Glue&amp;rsquo;s data catalog, is handy, while if heavy queries are executed in batch, Spark-based Glue, which can avoid resource and time limitation, can be better. Therefore, they can be used properly depending on the case.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/392/&#34;&gt;Redshift Serverless and other serverless aggregation services, run query with Glue Data Catalog - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Enable Job Bookmark of AWS Glue to process from the records following ones executed previously</title>
      <link>https://www.sambaiz.net/en/article/333/</link>
      <pubDate>Fri, 16 Apr 2021 20:00:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/333/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html&#34;&gt;Job Bookmark&lt;/a&gt; of AWS Glue is a feature that saves what records are processed, and prevent it from being executed next time.&#xA;Parquet and ORC, which were not supported before 1.0, are now supported.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/203/&#34;&gt;AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>What is Apache Spark, RDD, DataFrame, DataSet, Action and Transformation</title>
      <link>https://www.sambaiz.net/en/article/208/</link>
      <pubDate>Wed, 13 Feb 2019 21:17:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/208/</guid>
      <description>&lt;h2 id=&#34;what-is-spark&#34;&gt;What is Spark?&lt;/h2&gt;&#xA;&lt;p&gt;Spark is high-performance general-purpose distributed processing system.&#xA;It is used with distributed storage such as HDFS and S3, and cluster managers such as Hadoop YARN.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/126/&#34;&gt;HDFS(Hadoop Distributed File System)とは - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/427/&#34;&gt;How Hadoop YARN allocates resources to applications and check how much resources are allocated - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;It can be processed faster than Hadoop&amp;rsquo;s MapReduce by storing the intermediate data in memory.&#xA;There are APIs for Java, Scala, Python, R.&#xA;While python is easy to write, performance suffers due to interaction with JVM.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Launch Hive execution environment with Cloudera Docker Image and execute query to JSON log</title>
      <link>https://www.sambaiz.net/en/article/128/</link>
      <pubDate>Thu, 24 Aug 2017 09:22:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/en/article/128/</guid>
      <description>&lt;h2 id=&#34;what-is-hive&#34;&gt;&lt;a href=&#34;https://cwiki.apache.org/confluence/display/Hive/Home#Home-ApacheHive&#34;&gt;What is Hive&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;Hive is a data warehouse software built on Hadoop, which can access data sources such as HDFS with HiveSQL, an extended SQL.&#xA;Sending a query, the job runs on MapReduce, Spark or Tez.&#xA;It has fault tolerance and is mainly used in batch processing.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/en/article/126/&#34;&gt;What is HDFS(Hadoop Distributed File System) - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://prestodb.io/&#34;&gt;Presto&lt;/a&gt;, which access data sources with SQL likewise, can execute the query faster than Hive by parallelizing tasks and having intermediate data on memory, so it is suitable for an ad-hoc purpose.&#xA;On the other hand, if the intermediate data is large, it &lt;a href=&#34;https://aws.amazon.com/jp/blogs/news/top-10-performance-tuning-tips-for-amazon-athena/&#34;&gt;takes a long time or can be failed&lt;/a&gt;.&#xA;Presto can refer to &lt;strong&gt;Hive metastore&lt;/strong&gt; with &lt;a href=&#34;https://prestodb.io/docs/current/connector/hive.html&#34;&gt;Hive Connector&lt;/a&gt;, so it can share the schema with Hive and query to it.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
