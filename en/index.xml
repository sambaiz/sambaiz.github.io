<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sambaiz-net</title>
    <link>https://www.sambaiz.net/en/</link>
    <description>Recent content on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Sat, 04 Dec 2021 22:38:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/en/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Implement Rabin–Karp algorithm in C&#43;&#43;</title>
      <link>https://www.sambaiz.net/en/article/387/</link>
      <pubDate>Sat, 04 Dec 2021 22:38:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/387/</guid>
      <description>Rabin–Karp algorithm is a string-searching algorithm using rolling hash. Rolling hash can be made with O(1) by removing a first element from the previous hash and adding a next element to it.
class rolling_hash { string str; int window_length = 0; long hash = 0; int head_idx = 0; long tail_pow; const int base = 128; public: rolling_hash(string str, int window_length) { this-&amp;gt;str = str; this-&amp;gt;window_length = window_length; long pow = 1; for (int i = 0; i &amp;lt; window_length; i++) { hash += str[i] * pow; pow *= base; } tail_pow = pow / base; } int get() { return hash; } // a_0 * base^0 + a_1 * base^1 + .</description>
    </item>
    
    <item>
      <title>Columnar format Parquet structure and Read optimization</title>
      <link>https://www.sambaiz.net/en/article/386/</link>
      <pubDate>Fri, 03 Dec 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/386/</guid>
      <description>Parquet is a columnar format mainly used in the Hadoop ecosystem. Compared to row-based formats like CSV, unnecessary data is not read so queries can be executed efficiently.
Structure Data is horizontally partitioned to some Row Groups. A column is splitted into some Column Chunks, and a Column Chunk has some Pages which is an unit of compressing and encoding.
File structure is like following and it contains some Row Groups which have Column Chunks of each columns and metadata.</description>
    </item>
    
    <item>
      <title>struct and class in C&#43;&#43;</title>
      <link>https://www.sambaiz.net/en/article/385/</link>
      <pubDate>Tue, 30 Nov 2021 18:12:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/385/</guid>
      <description>class and struct in C++ are functionally equivalent but class is private by default as opposed to struct being public. class is used for encapsulation and if it has public fields and few methods, it seems that struct is used generally.
#include &amp;lt;iostream&amp;gt;using namespace std; class C { int value; public: C(int value) { this-&amp;gt;value = value; } int func() { return value; }; }; struct S { S(int value) { this-&amp;gt;value = value; } int value; private: int func() { return value; }; }; struct C2: S{ public: C2(int value) : S(value) { this-&amp;gt;value *= value; } int func2() { return value; } }; int main () { (new C(1))-&amp;gt;value; // member &amp;#34;C::value&amp;#34; (declared at line 2) is inaccessible  (new C(1))-&amp;gt;func(); (new S(1))-&amp;gt;value; (new S(1))-&amp;gt;func(); // function &amp;#34;S::func&amp;#34; (declared at line 18) is inaccessible  cout &amp;lt;&amp;lt; C2{9}.</description>
    </item>
    
    <item>
      <title>Treat Spark struct as map to expand to multiple rows with explode</title>
      <link>https://www.sambaiz.net/en/article/384/</link>
      <pubDate>Wed, 13 Oct 2021 02:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/384/</guid>
      <description>When you read data without specifying schema in Spark, the schema is automatically determined from the input as follows.
# {&amp;#34;aaa&amp;#34;:123,&amp;#34;ccc&amp;#34;:[123],&amp;#34;eee&amp;#34;:{&amp;#34;fff&amp;#34;:123},&amp;#34;hhh&amp;#34;:null} df = spark.read.json(&amp;#34;s3://hogefuga/testjson/&amp;#34;) df.printSchema() &amp;#39;&amp;#39;&amp;#39; root |-- aaa: long (nullable = true) |-- ccc: array (nullable = true) | |-- element: long (containsNull = true) |-- eee: struct (nullable = true) | |-- fff: long (nullable = true) |-- hhh: string (nullable = true) &amp;#39;&amp;#39;&amp;#39; This works well in most cases, but if the field that assumes map is determined as struct, or if the field is determined as string as it contains only null, processings may fail by mismatch of function inputs.</description>
    </item>
    
    <item>
      <title>Spark Web UI: Monitor Job Stages, Tasks distribution and SQL plan</title>
      <link>https://www.sambaiz.net/en/article/382/</link>
      <pubDate>Thu, 30 Sep 2021 13:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/382/</guid>
      <description>Spark Web UI is a tool for monitoring Jobs and Executors.
Get Dockerfile that runs Spark based on maven: 3.6-amazoncorretto-8 from aws-glue-samples, and start History Server with the path of EventLog output by Glue and authentication information to get it.
$ git clone https://github.com/aws-samples/aws-glue-samples.git $ cd aws-glue-samples/utilities/Spark_UI/glue-3_0/ $ docker build -t glue/sparkui:latest . $ docker run -it -e SPARK_HISTORY_OPTS=&amp;#34;$SPARK_HISTORY_OPTS-Dspark.history.fs.logDirectory=s3a://path_to_eventlog -Dspark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID-Dspark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY&amp;#34; -p 18080:18080 glue/sparkui:latest &amp;#34;/opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer&amp;#34; Now, you can access http://localhost:18080 and select Application.</description>
    </item>
    
    <item>
      <title>Enable Job Bookmark of AWS Glue to process from the records following ones executed previously</title>
      <link>https://www.sambaiz.net/en/article/333/</link>
      <pubDate>Fri, 16 Apr 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/333/</guid>
      <description>Job Bookmark of AWS Glue is a feature that saves what records are processed, and prevent it from being executed next time. Parquet and ORC, which were not supported before 1.0, are now supported.
AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する - sambaiz-net
Bookmark is available if Job Bookmark is enabled, call DynamicFrame methods with a transaction_ctx, and call job.commit().
For example, following job that counts a table and outputs it doesn&amp;rsquo;t count records previously counted if Bookmark is available, so it outputs not total but difference count from the previous time.</description>
    </item>
    
    <item>
      <title>What is Apache Spark, RDD, DataFrame, DataSet, Action and Transformation</title>
      <link>https://www.sambaiz.net/en/article/208/</link>
      <pubDate>Wed, 13 Feb 2019 21:17:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/208/</guid>
      <description>What is Spark? Spark is high-performance general-purpose distributed processing system. It is used with distributed storage such as HDFS and S3, and cluster managers such as Hadoop YARN. It can be processed faster than Hadoop&amp;rsquo;s MapReduce by storing the intermediate data in memory. There are APIs for Java, Scala, Python, R. While python is easy to write, performance suffers due to interaction with JVM.
HDFS(Hadoop Distributed File System)とは - sambaiz-net</description>
    </item>
    
  </channel>
</rss>
