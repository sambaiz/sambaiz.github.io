<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sambaiz-net</title>
    <link>https://www.sambaiz.net/en/</link>
    <description>Recent content on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Sun, 26 Dec 2021 22:03:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/en/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Redshift Serverless and other serverless aggregation services, run query with Glue Data Catalog</title>
      <link>https://www.sambaiz.net/en/article/392/</link>
      <pubDate>Sun, 26 Dec 2021 22:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/392/</guid>
      <description>Redshift Serverless Redshift Serverless is a new feature which can use Redshift, a petabyte-scale DWH without the instance, announced at this year&amp;rsquo;s re:Invent. It is available existing features such as Redshift Spectrum, refer to S3 directly, Federated Query to RDS, and Redshift ML. I&amp;rsquo;m happy with this update as it is costly to keep the instance running for occasional usage such as analytics.
The cost is charged for at least 1 minute RPU time and storage.</description>
    </item>
    
    <item>
      <title>Generate data using TPC-DS Connector in Athena&#39;s Federated Query</title>
      <link>https://www.sambaiz.net/en/article/391/</link>
      <pubDate>Sat, 25 Dec 2021 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/391/</guid>
      <description>Athena&amp;rsquo;s Federated Query is a feature to execute queries on non-S3 data sources such as DynamoDB and RDS through Lambda function which is a data sources connector.
 This article uses TPC-DS Connector in the AWS official repository. It generates the data of TPC-DS, which is a database benchmark in Decision Support.
Although it is in the official repository, it is a custom connector, so you need to build it yourself.</description>
    </item>
    
    <item>
      <title>Check if there is a cycle in the undirected graph by Union-Find Tree</title>
      <link>https://www.sambaiz.net/en/article/390/</link>
      <pubDate>Sun, 12 Dec 2021 16:39:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/390/</guid>
      <description>Union-Find Tree Union-Find Tree is a data structure having some disjoint sets and can do &amp;ldquo;Union&amp;rdquo; which merges two sets, and &amp;ldquo;Find&amp;rdquo; which checks if two elements are in the same set with amortized O(α(n)) (α(n) is an inverse Ackermann function and smaller than log(n)).
&amp;ldquo;Union&amp;rdquo; connects the tree with the smaller rank to under side so that merged tree will be in equilibrium as much as possible. The rank of one element tree is 0, and when trees of the same rank are merged, merged tree&amp;rsquo;s rank is the original rank + 1.</description>
    </item>
    
    <item>
      <title>Flutter&#39;s Navigator and AuroRoute</title>
      <link>https://www.sambaiz.net/en/article/389/</link>
      <pubDate>Sat, 11 Dec 2021 16:39:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/389/</guid>
      <description>Navigator Flutter&amp;rsquo;s Navigator is a screen transition class by stacking Routes and has APIs such as push() and pop().
import &amp;#39;package:flutter/material.dart&amp;#39;; void main() { runApp(const MyApp()); } class MyApp extends StatelessWidget { const MyApp({Key? key}) : super(key: key); @override Widget build(BuildContext context) { return MaterialApp( title: &amp;#39;Navigation Test&amp;#39;, theme: ThemeData( primarySwatch: Colors.blue, ), home: const Page1(), ); } } class Page1 extends StatelessWidget { const Page1({Key? key}) : super(key: key); @override Widget build(BuildContext context) { return Scaffold( appBar: AppBar( title: const Text(&amp;#34;page1&amp;#34;), ), body: ListView.</description>
    </item>
    
    <item>
      <title>Build iOS/Android/Web App by Flutter</title>
      <link>https://www.sambaiz.net/en/article/388/</link>
      <pubDate>Sun, 05 Dec 2021 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/388/</guid>
      <description>Flutter is a cross-platform framework by Google. In addition to iOS/Android, Web became stable in version 2.0 which is released in March of this year, and Windows/Mac/Linux is beta. Unlike React Native which uses native UI, Flutter uses its own UI. In addition to Material, iOS-style Cupertino is also available, but unless it branches etc., it become the same looks regardless of the platform.
Build the environment Build the environment according to the official Get started.</description>
    </item>
    
    <item>
      <title>Implement Rabin–Karp algorithm in C&#43;&#43;</title>
      <link>https://www.sambaiz.net/en/article/387/</link>
      <pubDate>Sat, 04 Dec 2021 22:38:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/387/</guid>
      <description>Rabin–Karp algorithm is a string-searching algorithm using rolling hash. Rolling hash can be made with O(1) by removing a first element from the previous hash and adding a next element to it.
class rolling_hash { string str; int window_length = 0; long hash = 0; int head_idx = 0; long tail_pow; const int base = 128; public: rolling_hash(string str, int window_length) { this-&amp;gt;str = str; this-&amp;gt;window_length = window_length; long pow = 1; for (int i = 0; i &amp;lt; window_length; i++) { hash += str[i] * pow; pow *= base; } tail_pow = pow / base; } int get() { return hash; } // a_0 * base^0 + a_1 * base^1 + .</description>
    </item>
    
    <item>
      <title>Columnar format Parquet structure and Read optimization</title>
      <link>https://www.sambaiz.net/en/article/386/</link>
      <pubDate>Fri, 03 Dec 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/386/</guid>
      <description>Parquet is a columnar format mainly used in the Hadoop ecosystem. Compared to row-based formats like CSV, unnecessary data is not read so queries can be executed efficiently.
Structure Data is horizontally partitioned to some Row Groups. A column is splitted into some Column Chunks, and a Column Chunk has some Pages which is an unit of compressing and encoding.
File structure is like following and it contains some Row Groups which have Column Chunks of each columns and metadata.</description>
    </item>
    
    <item>
      <title>struct and class in C&#43;&#43;</title>
      <link>https://www.sambaiz.net/en/article/385/</link>
      <pubDate>Tue, 30 Nov 2021 18:12:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/385/</guid>
      <description>class and struct in C++ are functionally equivalent but class is private by default as opposed to struct being public. class is used for encapsulation and if it has public fields and few methods, it seems that struct is used generally.
#include &amp;lt;iostream&amp;gt;using namespace std; class C { int value; public: C(int value) { this-&amp;gt;value = value; } int func() { return value; }; }; struct S { S(int value) { this-&amp;gt;value = value; } int value; private: int func() { return value; }; }; int main () { (new C(1))-&amp;gt;value; // member &amp;#34;C::value&amp;#34; (declared at line 2) is inaccessible  (new C(1))-&amp;gt;func(); (new S(1))-&amp;gt;value; (new S(1))-&amp;gt;func(); // function &amp;#34;S::func&amp;#34; (declared at line 18) is inaccessible } Both can be inherited.</description>
    </item>
    
    <item>
      <title>Treat Spark struct as map to expand to multiple rows with explode</title>
      <link>https://www.sambaiz.net/en/article/384/</link>
      <pubDate>Wed, 13 Oct 2021 02:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/384/</guid>
      <description>When you read data without specifying schema in Spark, the schema is automatically determined from the input as follows.
# {&amp;#34;aaa&amp;#34;:123,&amp;#34;ccc&amp;#34;:[123],&amp;#34;eee&amp;#34;:{&amp;#34;fff&amp;#34;:123},&amp;#34;hhh&amp;#34;:null} df = spark.read.json(&amp;#34;s3://hogefuga/testjson/&amp;#34;) df.printSchema() &amp;#39;&amp;#39;&amp;#39; root |-- aaa: long (nullable = true) |-- ccc: array (nullable = true) | |-- element: long (containsNull = true) |-- eee: struct (nullable = true) | |-- fff: long (nullable = true) |-- hhh: string (nullable = true) &amp;#39;&amp;#39;&amp;#39; This works well in most cases, but if the field that assumes map is determined as struct, or if the field is determined as string as it contains only null, processings may fail by mismatch of function inputs.</description>
    </item>
    
    <item>
      <title>Spark Web UI: Monitor Job Stages, Tasks distribution and SQL plan</title>
      <link>https://www.sambaiz.net/en/article/382/</link>
      <pubDate>Thu, 30 Sep 2021 13:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/382/</guid>
      <description>Spark Web UI is a tool for monitoring Jobs and Executors.
Get Dockerfile that runs Spark based on maven: 3.6-amazoncorretto-8 from aws-glue-samples, and start History Server with the path of EventLog output by Glue and authentication information to get it.
 $ git clone https://github.com/aws-samples/aws-glue-samples.git $ cd aws-glue-samples/utilities/Spark_UI/glue-3_0/ $ docker build -t glue/sparkui:latest . $ docker run -it -e SPARK_HISTORY_OPTS=&amp;#34;$SPARK_HISTORY_OPTS-Dspark.history.fs.logDirectory=s3a://path_to_eventlog -Dspark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID-Dspark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY&amp;#34; -p 18080:18080 glue/sparkui:latest &amp;#34;/opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer&amp;#34; Now, you can access http://localhost:18080 and select Application.</description>
    </item>
    
    <item>
      <title>Enable Job Bookmark of AWS Glue to process from the records following ones executed previously</title>
      <link>https://www.sambaiz.net/en/article/333/</link>
      <pubDate>Fri, 16 Apr 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/333/</guid>
      <description>Job Bookmark of AWS Glue is a feature that saves what records are processed, and prevent it from being executed next time. Parquet and ORC, which were not supported before 1.0, are now supported.
AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する - sambaiz-net
Bookmark is available if Job Bookmark is enabled, call DynamicFrame methods with a transaction_ctx, and call job.commit().
For example, following job that counts a table and outputs it doesn&amp;rsquo;t count records previously counted if Bookmark is available, so it outputs not total but difference count from the previous time.</description>
    </item>
    
    <item>
      <title>Python with structural subtyping by Protocol</title>
      <link>https://www.sambaiz.net/en/article/325/</link>
      <pubDate>Fri, 12 Feb 2021 02:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/325/</guid>
      <description>In Python that has no interface syntax, it can assure that the function can be called by checking the function existance with hasattr(). However, this method needs to insert asserting each time and the error cannot be known until runtime.
class ImplClass(): def foo(self): print(&amp;#34;ok&amp;#34;) class NoImplClass(): pass def call(d): assert hasattr(d, &amp;#39;foo&amp;#39;) d.foo() if __name__ == &amp;#34;__main__&amp;#34;: call(ImplClass()) # =&amp;gt; ok call(NoImplClass()) # =&amp;gt; AssertionError If you describe Type Hints implemented in Python 3.</description>
    </item>
    
    <item>
      <title>What is Apache Spark, RDD, DataFrame, DataSet, Action and Transformation</title>
      <link>https://www.sambaiz.net/en/article/208/</link>
      <pubDate>Wed, 13 Feb 2019 21:17:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/208/</guid>
      <description>What is Spark? Spark is high-performance general-purpose distributed processing system. It is used with distributed storage such as HDFS and S3, and cluster managers such as Hadoop YARN. It can be processed faster than Hadoop&amp;rsquo;s MapReduce by storing the intermediate data in memory. There are APIs for Java, Scala, Python, R. While python is easy to write, performance suffers due to interaction with JVM.
HDFS(Hadoop Distributed File System)とは - sambaiz-net</description>
    </item>
    
    <item>
      <title>Increase the maximum number of file descriptors</title>
      <link>https://www.sambaiz.net/en/article/41/</link>
      <pubDate>Thu, 08 Dec 2016 21:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/en/article/41/</guid>
      <description>What is file descriptors? File desecriptors is an identifier for interacting with an outside of a process. In POSIX, it is an int type, 0 is stdin, 1 is stdout, and 2 is stderr. It is assigned by a system call such as open() to open files and devices, and socket() to create sockets to communicate with other processes.
Maximum number of file descriptors There is a limit on the number of file descriptors a process can use so that one process does not consumpt all resources.</description>
    </item>
    
  </channel>
</rss>
