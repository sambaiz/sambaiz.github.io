<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Infra on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/infra/</link>
    <description>Recent content in Infra on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Wed, 31 Jul 2019 03:22:00 +0900</lastBuildDate>
    
	<atom:link href="https://www.sambaiz.net/tags/infra/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>CDKでECS&#43;Fargate上にDigdagを立てる</title>
      <link>https://www.sambaiz.net/article/234/</link>
      <pubDate>Wed, 31 Jul 2019 03:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/234/</guid>
      <description>AWSでワークフローエンジンDigdagを立てるにあたりスケールを見越してECS+Fargateで動かす。 全体のコードはGitHubにある。
FargateでECSを使う - sambaiz-net
リソースはCDKで作る。最近GAになったので高レベルのクラスを積極的に使っている。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
$ npm run cdk -- --version 1.2.0 (build 6b763b7)  VPC FargateなのでVPCが必要。 テンプレートを書くとSubnetやRouteTable、NATGatewayなど記述量が多くなるところだが、CDKだとこれだけで済む。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
const vpc = new ec2.Vpc(this, &#39;VPC&#39;, { cidr: props.vpcCidr, natGateways: 1, maxAzs: 2, subnetConfiguration: [ { name: &#39;digdag-public&#39;, subnetType: ec2.SubnetType.PUBLIC, }, { name: &#39;digdag-private&#39;, subnetType: ec2.SubnetType.PRIVATE, }, { name: &#39;digdag-db&#39;, subnetType: ec2.SubnetType.ISOLATED, } ] })  DB DigdagはPostgreSQLを使う。
const db = new rds.</description>
    </item>
    
    <item>
      <title>CDK/CircleCI/GitHubでAWSリソース管理リポジトリを作る</title>
      <link>https://www.sambaiz.net/article/223/</link>
      <pubDate>Mon, 20 May 2019 09:23:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/223/</guid>
      <description>AWS CDKでリソースを記述し、PullRequestに対して自動でcdk diffで変更があるものを表示して、mergeしたときにcdk deployする。 全体のコードはGitHubにある。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
CI Userの作成 まずcdkコマンドを実行するためのCI Userを作成する。これはCDK管理外のスタックで、AWSコンソール上から手動で上げる。
AWSのAssumeRole - sambaiz-net
AssumeRoleしかできないCIUserからCIAssumeRoleをassumeすることにした。
AWSTemplateFormatVersion: &#39;2010-09-09&#39; Resources: CIAssumeRole: Type: &#39;AWS::IAM::Role&#39; Properties: RoleName: &#39;CIAssumeRole&#39; ManagedPolicyArns: - &#39;arn:aws:iam::aws:policy/AdministratorAccess&#39; AssumeRolePolicyDocument: Version: &#39;2012-10-17&#39; Statement: - Effect: &#39;Allow&#39; Principal: AWS: - !Ref AWS::AccountId Action: - &#39;sts:AssumeRole&#39; CIGroup: Type: &#39;AWS::IAM::Group&#39; Properties: GroupName: &#39;CI&#39; CIPolicies: Type: &#39;AWS::IAM::Policy&#39; Properties: PolicyName: &#39;CI&#39; PolicyDocument: Statement: - Effect: Allow Action: &#39;sts:AssumeRole&#39; Resource: !GetAtt CIAssumeRole.Arn Groups: - !Ref CIGroup CIUser: Type: &#39;AWS::IAM::User&#39; Properties: UserName: &#39;CI&#39; Groups: - !</description>
    </item>
    
    <item>
      <title>AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする</title>
      <link>https://www.sambaiz.net/article/222/</link>
      <pubDate>Sun, 19 May 2019 01:43:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/222/</guid>
      <description>AWS CDK(Cloud Development Kit)はTypeScriptやJavaなどのコードから CloudFormationのテンプレートを生成して差分を確認しデプロイできる公式のツール。まだdeveloper preview。
$ npm i -g aws-cdk $ cdk --version 0.33.0 (build 50d71bf) $ mkdir cdk-vpc $ cd cdk-vpc $ cdk init app --language=typescript  CloudFormationのリソースと対応するCfnFooや、それを内部で作成する高レベル(L2)のResource ClassFooが実装されている。 ただし、現状CfnFooに対応するResource Classが存在しないものや、複数のリソースを内部で作成するResource Classが存在する。 例えば、ec2.VpcはCfnVPCだけではなく、Public/Private Subnet、NATGatewayまでまとめて一般的な構成で作る。Resource Classはまだ変更が多い。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
型があり補完が効くので通常のテンプレートと比べて書きやすいし、ループしたりすることもできる。
import * as cdk from &#39;@aws-cdk/cdk&#39; import * as ec2 from &#39;@aws-cdk/aws-ec2&#39; interface Export { vpc: ec2.Vpc } export class VPCStack extends cdk.Stack { protected deployEnv: string export: Export constructor(scope: cdk.</description>
    </item>
    
    <item>
      <title>DatadogのAWS integrationとAlertの設定をTerraformで行う</title>
      <link>https://www.sambaiz.net/article/219/</link>
      <pubDate>Sat, 04 May 2019 19:25:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/219/</guid>
      <description>DatadogのAWS integrationとAlertの設定をTerraformで行い、バージョン管理やレビューできるようにする。 全体のコードはGitHubに置いてある。
AWS Integration まずdatadog_integration_awsでAWS integrationの設定を作成してExternalIDを取得し、Policy/Roleを作成する。必要な権限はドキュメントを参照。
resource &amp;quot;datadog_integration_aws&amp;quot; &amp;quot;test&amp;quot; { account_id = &amp;quot;${var.aws_account_id}&amp;quot; role_name = &amp;quot;${var.aws_integration_role_name}&amp;quot; filter_tags = [&amp;quot;datadog:1&amp;quot;] } data &amp;quot;aws_iam_policy_document&amp;quot; &amp;quot;datadog_aws_integration_assume_role&amp;quot; { statement { actions = [&amp;quot;sts:AssumeRole&amp;quot;] principals { type = &amp;quot;AWS&amp;quot; identifiers = [&amp;quot;arn:aws:iam::464622532012:root&amp;quot;] } condition { test = &amp;quot;StringEquals&amp;quot; variable = &amp;quot;sts:ExternalId&amp;quot; values = [ &amp;quot;${datadog_integration_aws.test.external_id}&amp;quot;, ] } } }  Datadog providerにはないSlackなどその他のintegrationは手動で設定する必要がある。 また、Logも集める場合Serverless Application Repositoryから公式のDatadog-Log-Forwarderを入れて AWS IntegrationのところにLambdaのARNを入れるのも手動。
Alert datadog_monitorでAlertを作成する。 今回はLambdaの実行が失敗したときと、WARNという文字列が含まれるログが一定数出力されたときにAlertを飛ばすようにしてみた。 なおinfoやwarnといったログレベルはjsonのlevelフィールドに入れればデフォルトのpipelineでマップされるので通常は文字列比較などする必要はない。
初めに作るときは一度画面で意図通りアラートが飛ぶものを作成し、クエリなどをコピーするのが確実。
resource &amp;quot;datadog_monitor&amp;quot; &amp;quot;lambda-error-alert&amp;quot; { name = &amp;quot;lambda-error-alert&amp;quot; type = &amp;quot;metric alert&amp;quot; message = &amp;quot;Error occurred.</description>
    </item>
    
    <item>
      <title>KubernetesのCustom Resource Definition(CRD)とCustom Controller</title>
      <link>https://www.sambaiz.net/article/182/</link>
      <pubDate>Thu, 09 Aug 2018 23:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/182/</guid>
      <description>K8sではDeploymentを作成したときにReplicaSetも作成されるようにしたり、 Load Balancer Serviceを作成したときにGCPなどその環境に応じたLoad Balancerも作成されるようにしたりするため、Controllerがそれらを監視してAPIを呼んでいる。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
Controllerは単なるAPIを呼ぶアプリケーションなので自分でCustom Controllerを作成してDeploymentとしてデプロイすることもできる。 また、監視する対象もpodsやdeploymentsといった標準のAPIだけではなく、 Custom Resource で拡張したものを使うことができる。
特定のアプリケーションのためのControllerはOperatorとも呼ばれる。
CustomResourceDefinition(CRD) Custom Resourceを定義する。
apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: # REST APIで使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt; group: stable.example.com version: v1 # Namespaced か Cluster scope: Namespaced names: # 複数形 URLに使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;/&amp;lt;plural&amp;gt; plural: crontabs # 単数形 CLIなどで使われる singular: crontab # manifestで使う kind: CronTab shortNames: - ct  $ kubectl create -f crd.yaml $ kubectl get crd NAME AGE crontabs.</description>
    </item>
    
    <item>
      <title>Kubernetes,Helmで負荷試験ツールLocustを立てる</title>
      <link>https://www.sambaiz.net/article/161/</link>
      <pubDate>Sun, 18 Mar 2018 22:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/161/</guid>
      <description>OSSの負荷試験ツールLocustをK8sクラスタに立てる。 K8sならworkerの増減も簡単だし、HelmのChartもあるので立てるのも楽。
LocustはPython製で、以下のようなコードで処理を書くことができる。
@task(10)のように括弧の中に数字を書いて実行される割合を偏らせることもできる。 異なるTaskSetに対応するユーザーを複数作ることもできて、こちらもweightで重みを付けられる。 ユーザー数はあとでWeb上から入力する。
$ mkdir tasks $ cat tasks/tasks.py from locust import HttpLocust, TaskSet, task class ElbTasks(TaskSet): @task def task1(self): with client.get(&amp;quot;/&amp;quot;, catch_response=True) as response: if response.content != &amp;quot;Success&amp;quot;: response.failure(&amp;quot;Got wrong response&amp;quot;) class ElbWarmer(HttpLocust): task_set = ElbTasks min_wait = 1000 max_wait = 3000  stableにChartはあるが、今のところtasksの中を書き換えなくてはいけないようなので、forkしてきてtasksの中を書き換え、helm repo addするためにpackageして、これを参照するindex.yamlを生成した。
$ helm package . $ helm repo index . $ ls locust-0.1.2.tgz index.yaml index.yaml	locust-0.1.2.tgz $ cat index.yaml apiVersion: v1 entries: locust: .</description>
    </item>
    
    <item>
      <title>RBACが有効なGKEでHelmを使う</title>
      <link>https://www.sambaiz.net/article/160/</link>
      <pubDate>Sun, 18 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/160/</guid>
      <description>k8sのパッケージマネージャーHelmを使う - sambaiz-net
$ helm version Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;} Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}  GKEでhelm initしてhelm installしたところ以下のエラーが返ってきた。
Error: release my-locust failed: namespaces &amp;quot;default&amp;quot; is forbidden: User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot get namespaces in the namespace &amp;quot;default&amp;quot;: Unknown user &amp;quot;system:serviceaccount:kube-system:default&amp;quot;  GKEではデフォルトでK8sのRBAC(Role-Based Access Control)が有効になっているため、Tillerインスタンスに権限を与える必要がある。
ということでTiller用にnamespaceを切って、その中では好きにできるRoleと、Tillerが使うServiceAccountを作成し、RoleBindingで紐づける。
kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-manager namespace: tiller-world rules: - apiGroups: [&amp;quot;&amp;quot;, &amp;quot;extensions&amp;quot;, &amp;quot;apps&amp;quot;] resources: [&amp;quot;*&amp;quot;] verbs: [&amp;quot;*&amp;quot;] --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: tiller-world --- kind: RoleBinding apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>TerraformでVPCを管理するmoduleを作る</title>
      <link>https://www.sambaiz.net/article/121/</link>
      <pubDate>Sun, 23 Jul 2017 02:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/121/</guid>
      <description>Terraform
$ brew install terraform $ terraform -v Terraform v0.9.11  Terraformの設定要素 provider IaaS(e.g. AWS)、PaaS(e.g. Heroku)、SaaS(e.g. CloudFlare)など。
AWS Providerはこんな感じ。 ここに直接access_keyやsecret_keyを書くこともできるけど、誤って公開されてしまわないように環境変数か variableで渡す。
provider &amp;quot;aws&amp;quot; { # access_key = &amp;quot;${var.access_key}&amp;quot; # secret_key = &amp;quot;${var.secret_key}&amp;quot; region = &amp;quot;us-east-1&amp;quot; }  $ export AWS_ACCESS_KEY_ID=&amp;quot;anaccesskey&amp;quot; $ export AWS_SECRET_ACCESS_KEY=&amp;quot;asecretkey&amp;quot;  varibale CLIでオーバーライドできるパラメーター。typeにはstringのほかにmapやlistを渡すことができ、 何も渡さないとdefault値のものが、それもなければstringになる。
variable &amp;quot;key&amp;quot; { type = &amp;quot;string&amp;quot; default = &amp;quot;value&amp;quot; description = &amp;quot;description&amp;quot; }  値を渡す方法はTF_VAR_をprefixとする環境変数、-var、-var-fileがある。 また、moduleのinputとして渡されることもある。
$ export TF_VAR_somelist=&#39;[&amp;quot;ami-abc123&amp;quot;, &amp;quot;ami-bcd234&amp;quot;]&#39; $ terraform apply -var foo=bar -var foo=baz $ terraform apply -var-file=foo.</description>
    </item>
    
    <item>
      <title>iftopでネットワークの帯域を見る</title>
      <link>https://www.sambaiz.net/article/60/</link>
      <pubDate>Tue, 07 Feb 2017 20:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/60/</guid>
      <description>$ yum install --enablerepo=epel iftop $ iftop -f &amp;quot;not dst net 10.0.0.0/8&amp;quot;  -i eth0のようにしてインタフェースを指定し、-fでフィルタをかけられる。フィルタの詳細はman pcap-filterで。
 12.5Kb 25.0Kb 37.5Kb 50.0Kb	62.5Kb └─────────────────────────┴──────────────────────────┴──────────────────────────┴──────────────────────────┴────────────────────────── ip-172-31-9-9.ap-northeast-1.compute.internal =&amp;gt; 61-121-217-66.dh-connect.net 1.72Kb 6.57Kb 2.40Kb &amp;lt;= 416b 2.13Kb 702b ... ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── TX: cum: 22.6KB peak: 13.2Kb rates: 1.22Kb 1.27Kb 2.46Kb RX: 6.63KB 5.03Kb 208b 330b 748b TOTAL: 29.2KB 18.2Kb 1.42Kb 1.59Kb 3.19Kb  左から2, 10, 40秒間の平均kbps。TXが送信量、RXが受信量で、cumが総量、peakが最大。
実行中にSでソースのポートをDでディスティネーションのポートが表示される。</description>
    </item>
    
    <item>
      <title>vmstatのメモ</title>
      <link>https://www.sambaiz.net/article/59/</link>
      <pubDate>Mon, 06 Feb 2017 22:45:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/59/</guid>
      <description>$ vmstat 間隔(秒) procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 118588 80388 2516284 0 0 2 77 141 85 1 0 98 0 0  procs  r: 実行待ちプロセス数。CPUの処理が追いついていない。
 b: 割り込み不可能なスリープ中のプロセス数。I/O待ちらしい。
  memory  swpd: バーチャルメモリの使用量。
 free: 空きメモリ量。
 buff: バッファに使われてるメモリ量。
 cache: キャッシュに使われているメモリ量。
  swap  si: 秒あたりのスワップイン量。メモリが足りていない。</description>
    </item>
    
    <item>
      <title>複数EC2インスタンスを立ち上げてvegetaで負荷試験する</title>
      <link>https://www.sambaiz.net/article/43/</link>
      <pubDate>Sun, 18 Dec 2016 20:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/43/</guid>
      <description>vegetaで負荷をかける。
インスタンスを立ち上げるスクリプト コードはここ。 sambaiz/loadtest
まずキーペアを作成し、EC2インスタンスを立ち上げて、全てのインスタンスが使えるようになるまで待つ。
aws ec2 create-key-pair --key-name LoadTestKeyPare --query &#39;KeyMaterial&#39; --output text &amp;gt; LoadTestKeyPare.pem chmod 400 LoadTestKeyPare.pem aws ec2 run-instances --image-id $AMI_ID --count $INSTANCE_NUM --instance-type t2.micro --key-name LoadTestKeyPare --security-group-ids $SECURITY_GROUP_IDS --subnet-id $SUBNET_ID ... aws ec2 wait instance-status-ok --instance-ids $INSTANCE_IDS  このAMIは事前にPackerでつくったもの。vegetaをインストールしてファイルディスクリプタの上限を増やしている。
{ &amp;quot;variables&amp;quot;: { &amp;quot;aws_access_key&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;aws_secret_key&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `aws_access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `aws_secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;ap-northeast-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-0c11b26d&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.micro&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ec2-user&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;loadtest {{timestamp}}&amp;quot; }], &amp;quot;provisioners&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;, &amp;quot;inline&amp;quot;: [ &amp;quot;wget https://github.</description>
    </item>
    
    <item>
      <title>SSHポートフォワーディングとnetstatのメモ</title>
      <link>https://www.sambaiz.net/article/42/</link>
      <pubDate>Sat, 17 Dec 2016 12:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/42/</guid>
      <description> SSHポートフォワーディング ローカルの8080ポートを、example.comを通したexample2.comの80ポートに向ける。
$ ssh hoge@example.com -Nf -L 8080:example2.com:80 $ curl localhost:8080 # =&amp;gt; example2.com:80   -N: リモートでコマンドを実行しない -f: バックグラウンドで実行  netstat ネットワークの状態を確認する。
$ netstat -ant Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN ...   -a: non-listening(TCPではESTABLISHED状態)しているソケットだけではなく、listeningしている情報も出す -n: 数値のアドレスで表示する -t: TCPで制限  </description>
    </item>
    
    <item>
      <title>ファイルディスクリプタの上限を増やす</title>
      <link>https://www.sambaiz.net/article/41/</link>
      <pubDate>Thu, 08 Dec 2016 21:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/41/</guid>
      <description>ファイルディスクリプタとは プロセスの外部とやりとりするための識別子。POSIXではint型で、0がstdin、1がstdout、2がstderrといったもの。 ファイルやデバイスに対するopen()や、 ネットワーク(INETドメインソケット)やホスト内(UNIXドメインソケット)で 通信するためのソケットを生成するsocket()などのシステムコールで生成される。
ファイルディスクリプタの上限 一つのプロセスがリソースを食いつぶさないように 使えるファイルディスクリプタの上限が決まっていて、ulimit -nで確認できる。デフォルトは大体1024。
$ ulimit -n 1024  各プロセスの上限と使っているファイルディスクリプタはこれで確認できる。
$ cat /proc/&amp;lt;プロセスID&amp;gt;/limits ... Max open files 1024 4096 files ... $ ls -l /proc/&amp;lt;プロセスID&amp;gt;/fd  webサーバーのように同時にたくさん通信したりすると上限に達してしまい、Too many open filesになってしまうので増やす必要がある。
/etc/security/limits.confで変更する PAM認証時(ログインするときなど)に適用されるので、サーバーの起動時に立ち上がったデーモンには使えない。
$ cat /etc/pam.d/sshd ... session required pam_limits.so ...  全てのユーザー(*)のプロセスが使える ファイルディスクリプタ(nofile)のsoft(ユーザーが設定できる)とhard(rootが設定できる)上限を共に64000にする。
$ echo &amp;quot;* hard nofile 64000&amp;quot; &amp;gt;&amp;gt; /etc/security/limits.conf $ echo &amp;quot;* soft nofile 64000&amp;quot; &amp;gt;&amp;gt; /etc/security/limits.conf $ ulimit -n 64000  ulimit -nで変更する シェルと、起動したプロセスで有効。</description>
    </item>
    
    <item>
      <title>OpenVPNサーバーPritunlをDockerで動かす</title>
      <link>https://www.sambaiz.net/article/39/</link>
      <pubDate>Fri, 02 Dec 2016 21:05:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/39/</guid>
      <description>PritunlでVPNサーバーを立てる。
Dockerfileはこんな感じ。
https://hub.docker.com/r/sambaiz/pritunl/
FROM mongo:3.4 # https://docs.pritunl.com/docs/installation RUN echo &#39;deb http://repo.pritunl.com/stable/apt jessie main&#39; &amp;gt; /etc/apt/sources.list.d/pritunl.list &amp;amp;&amp;amp; \ apt-key adv --keyserver hkp://keyserver.ubuntu.com --recv 7568D9BB55FF9E5287D586017AE645C0CF8E292A &amp;amp;&amp;amp; \ apt-get --assume-yes update &amp;amp;&amp;amp; \ apt-get --assume-yes upgrade &amp;amp;&amp;amp; \ apt-get --assume-yes install pritunl iptables EXPOSE 80 443 12345/udp CMD mongod --fork --logpath /data/db/mongod.log &amp;amp;&amp;amp; echo &#39;Setup Key:&#39; `pritunl setup-key` &amp;amp;&amp;amp; pritunl start  $ docker run -itd -p 80:80 -p 443:443 -p 12345:12345/udp --privileged sambaiz/pritunl $ docker logs &amp;lt;id&amp;gt; .</description>
    </item>
    
    <item>
      <title>PackerでAMIを作る</title>
      <link>https://www.sambaiz.net/article/24/</link>
      <pubDate>Tue, 18 Oct 2016 22:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/24/</guid>
      <description>https://www.packer.io/
いろんなプラットフォームのイメージを作ることができるツール。 これでfluentdのログサーバーのAMIを作る。
$ brew install packer # mac $ packer -v 0.10.1  設定ファイルはこんな感じ。variablesの値は{{user ... }}のところで使われる。 buildersに作るイメージの情報を書いて、provisionersで環境を作る。
provisionersにはchefやansibleなども指定できるが、 継ぎ足し継ぎ足しで秘伝のタレ化したAMIも最初は、コマンドいくつか実行するだけなのでとりあえず手作業で作った、後でなんとかするなんてものもあったりして、 そういうものは無理にchefなどで始めず、手軽にshellでpacker buildするといいと思う。 手作業よりも楽だしソースが別にあるので使われていないAMIを消すのも簡単。
fileではpermissionがないところに置くことができないので、一旦置いてshellで移動する。
{ &amp;quot;variables&amp;quot;: { &amp;quot;aws_access_key&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;aws_secret_key&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `aws_access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `aws_secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;ap-northeast-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-1a15c77b&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.small&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ec2-user&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;fluentd-logserver {{timestamp}}&amp;quot; }], &amp;quot;provisioners&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;file&amp;quot;, &amp;quot;source&amp;quot;: &amp;quot;td-agent.conf&amp;quot;, &amp;quot;destination&amp;quot;: &amp;quot;/home/ec2-user/td-agent.conf&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;, &amp;quot;inline&amp;quot;: [ &amp;quot;curl -L https://toolbelt.</description>
    </item>
    
  </channel>
</rss>