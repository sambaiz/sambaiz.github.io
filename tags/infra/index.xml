<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>infra on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/infra/</link>
    <description>Recent content in infra on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Fri, 08 Apr 2022 12:11:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/infra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CloudWatch Logsを介さずにLambdaのテレメトリを行うnewrelic-lambda-extensionとその仕組み</title>
      <link>https://www.sambaiz.net/article/401/</link>
      <pubDate>Fri, 08 Apr 2022 12:11:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/401/</guid>
      <description>New RelicにLambdaのログを転送するには、CloudWatch Logsに出力したものをサブスクライブして送るLambda function aws-log-ingestionを用いる従来の方法のほかに、Lambda layer newrelic-lambda-extensionを用いる方法があって、トレースログなどをCloudWatc Logsに出力することなく送れるのでコストを最小限に抑えられる。
インストール newrelic-lambda integrations install するとLayerが参照するAPI KeyのSecretのStackなどがデプロイされる。
$ pip3 install newrelic-lambda-cli $ newrelic-lambda integrations install \  --nr-account-id &amp;lt;account id&amp;gt; \  --nr-api-key &amp;lt;api key&amp;gt; \  --linked-account-name &amp;lt;linked account name&amp;gt; \  --enable-license-key-secret \  --aws-profile &amp;lt;aws_profile_name&amp;gt; --aws-region &amp;lt;aws_region&amp;gt; Validating New Relic credentials Retrieving integration license key Creating the AWS role for the New Relic AWS Lambda Integration Waiting for stack creation to complete.</description>
    </item>
    
    <item>
      <title>New Relicでインフラやアプリケーションをモニタリングする</title>
      <link>https://www.sambaiz.net/article/399/</link>
      <pubDate>Wed, 30 Mar 2022 19:11:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/399/</guid>
      <description>New RelicはインフラやアプリケーションをモニタリングするSaaSで、同種のサービスとしてはDatadogがある。 2020年に製品や課金体系が変わり、転送量とユーザー数に対して課金されるようになったようだ。 ホストや機能に対して課金されるDatadogと比べると、少人数で多くのインスタンスを管理している場合に有利ということになる。また、Datadogの課金体系はやや複雑で頻繁に台数が増減する環境では請求がどの程度になるか読みづらいところがあるが、こちらは分かりやすい。新しい機能を使っても単価が上がることもないのでいろいろと試しやすいのも良いと思う。
一方、大人数で管理していたり、捌くリクエストが膨大な場合APMの転送量がかさむことでDatadogより高くなるケースがある。 多くの場合はユーザー課金の割合が大きくなるので、Full platform userの数を減らして半額のCore userや無料のBasic userにすればコストが抑えられるが、 DashboardやAlertについては全てのユーザーが使える一方APMの画面が見られるのはFull platform userのみだったりするのが悩ましい。 転送量についてはDrop dataすることで減らすことができるが現状画面からは設定できずNerdGraphを用いる必要がある。次はTerraformで設定を行う例。
New RelicのGraphQL API、NerdGraphでリソースを取得する - sambaiz-net
resource &amp;#34;newrelic_nrql_drop_rule&amp;#34; &amp;#34;heavy_path&amp;#34; { account_id = ***** description = &amp;#34;Drop transactions data&amp;#34; action = &amp;#34;drop_data&amp;#34; nrql = &amp;#34;SELECT * FROM Transaction WHERE `request.uri` = &amp;#39;/heavy_path&amp;#39;&amp;#34; } トラブルシューティングは比較的難しく感じていて、なぜかメトリクスが送られないといったことがあったり、古いドキュメントを参照してしまったりしたがライブラリやドキュメントはOSSになっており、いざとなれば実装を読んだり修正のPRを出せば良いと考えている。ただ明示されていないことがあったり、されていても見つけられないことがあったりするので、特に初めて使う場合はユーザー単価が上がってしまうがサポートが付くPro以上のプランにすると安心かもしれない。去年買収したCodeStreamを含め、新機能の開発は活発に行われている印象があるので、トラブルシューティングについても改善されると良いなと思っている。
NewRelic CodeStreamでコードの質問やデバッグを効率的に行えるようにする - sambaiz-net
AWS連携 New RelicのアカウントからAssumeRoleするReadOnlyAccessとbudgets:ViewBudgetを付与したRoleを作る。
AWSのAssumeRole - sambaiz-net
{ &amp;#34;Statement&amp;#34;: [ { &amp;#34;Action&amp;#34;: [ &amp;#34;budgets:ViewBudget&amp;#34; ], &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;*&amp;#34; } ], &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34; } メトリクスの取得は従来の定期的にAPIを呼んでポーリングする方法と、FirehoseにストリーミングするCloudWatch Metric Steamsを用いる方法がサポートされていて、後者が推奨されている。用意されているCloudFormationのテンプレートにLicense Keyを渡してスタックを作成すると次のリソースが作成され、New Relicの画面上に各種リソースのメトリクスが表示されるようになる。</description>
    </item>
    
    <item>
      <title>Ansibleでnginxを入れてLoad Balancingさせる</title>
      <link>https://www.sambaiz.net/article/239/</link>
      <pubDate>Tue, 17 Sep 2019 09:42:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/239/</guid>
      <description>EC2でUbuntu Server 18.04 LTS (ami-07d0cf3af28718ef8) の t3.medium (2vCPU, 4GiB) インスタンスを3台立ち上げた。この内1台をLB用とし、2台のAppサーバーに負荷分散させる。
https://github.com/sambaiz/ansible-nginx-lb-example
nginx.conf LBとAppのnginx.conf。upstreamはデフォルトでラウンドロビンする。
$ cat conf/lb/nginx.conf ... http { ... upstream app-server { server ip-172-31-94-208.ec2.internal; server ip-172-31-88-90.ec2.internal; } server { listen 80; server_name .compute-1.amazonaws.com; access_log /var/log/nginx/app-server.log main; location / { proxy_pass http://app-server; } } } $ cat conf/app/nginx.conf ... http { ... server { listen 80; server_name .compute-1.amazonaws.com; location / { proxy_pass http://127.0.0.1:8080; } } } Ansibleの実行 まずInventoryを書いて疎通確認する。
$ pip install --user ansible $ cat hosts [lb] ec2-3-227-2-54.</description>
    </item>
    
    <item>
      <title>User NamespaceでrootになってNetwork Namespaceを作りvethとNATで外と通信する</title>
      <link>https://www.sambaiz.net/article/237/</link>
      <pubDate>Fri, 06 Sep 2019 02:20:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/237/</guid>
      <description>LinuxのNamespaceはuidやpid、networkなどを分離できる機能で、Dockerなどのコンテナ技術で使われている。
# Amazon Linux 2 (ami-0ff21806645c5e492) $ uname -r 4.14.138-114.102.amzn2.x86_64 User NamespaceでRootになる rootでないと正常終了しないコードを書いた。
$ cat /tmp/root_only.sh #!/bin/sh if [ &amp;#34;$(id -u)&amp;#34; != &amp;#34;0&amp;#34; ]; then echo &amp;#34;you are not root...&amp;#34; exit 1 fi echo &amp;#34;you are root!&amp;#34; 実際ec2-userではexit 1になる。
$ id -u 1000 $ sh /tmp/root_only.sh; echo $? you are not root... 1 User Namespaceを作る。rootでないとそれ以外のNamespaceは作れない。
$ unshare --net unshare: unshare failed: Operation not permitted $ unshare --user $ id uid=65534(nfsnobody) gid=65534(nfsnobody) groups=65534(nfsnobody) $ echo $$ 3537 他のshellを開き、Namespaceの外側からuid_mapを書き込む。 外側の1000から始まるuidを、このNamespaceの0から始まるuidに範囲1でマッピングする。</description>
    </item>
    
    <item>
      <title>SystemdのService</title>
      <link>https://www.sambaiz.net/article/236/</link>
      <pubDate>Fri, 30 Aug 2019 00:16:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/236/</guid>
      <description>SystemdはLinuxで動くServiceの管理などを行うデーモン。initの後継でchkconfig/servicceの代わりにsystemctlコマンドを使う。
$ systemctl start name.service $ systemctl stop name.service $ systemctl status name.service $ systemctl enable name.service $ systemctl disable name.service Serviceの一覧を見る。
$ systemctl list-units --type service --all UNIT LOAD ACTIVE SUB DESCRIPTION accounts-daemon.service loaded active running Accounts Service acpid.service loaded active running ACPI event daemon apparmor.service loaded active exited LSB: AppArmor initialization apport.service loaded active exited LSB: automatic crash report generation $ systemctl list-unit-files --type service UNIT FILE STATE accounts-daemon.</description>
    </item>
    
    <item>
      <title>PR上でCDKのレビューやデプロイを行うツールcdkbotを作った</title>
      <link>https://www.sambaiz.net/article/235/</link>
      <pubDate>Thu, 29 Aug 2019 22:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/235/</guid>
      <description>sambaiz/cdkbot
PRのコメントで/diffや/deployと打つとcdk diffやcdk deployが走る。 diffを見てレビューし、良ければ/deployでデプロイし完了するとmergeされる。
以前CircleCIでmerge時にdeployされる仕組みを作った。
CDK/CircleCI/GitHubでAWSリソース管理リポジトリを作る - sambaiz-net
ただ、この仕組みだと CFnの実行時エラーのためにデプロイできない状態のものがmasterブランチにmergeされてしまい、その修正のために何回も試行錯誤のPRを出すことになったり、 Stack間の依存がある場合リソースを削除するとcdk deployによって依存解決された順序だと失敗してしまうという問題があった。 cdkbotでは必要ならデプロイするStackを選べて、完了してからmergeすることでこれらの問題を解決した。 また、AWS外のCIにとても強い権限を与えていたがそれも必要なくなった。
単純にブランチの状態でデプロイしてしまうと古い状態に巻き戻ってしまう可能性があるので、内部でbaseブランチをmergeしていたり、 ラベルによってそのPRがデプロイ可能かどうかを制御していたりする。 最低限デプロイできるようになってから、この辺りの仕組みを整えるまでに存外に時間がかかった。
Serverless Application Repositoryに公開してあるので簡単にインストールできる。
AWS SAMでLambdaの関数をデプロイしServerless Application Repositoryに公開する - sambaiz-net
 追記 (2019-10-26): ap-northeast-1に対応していないのと、ECSのリソースを作成できないため、Serverless Application Repositoryに公開するのはやめた。makeでインストールできる。 Lambda環境でできない処理をECSで実行する - sambaiz-net
 外部コマンド gitやnpmといった外部コマンドを実行する必要があるが、標準では入っていないのでLambda Layerで入れている。
Lambda上でnpm installできるLayerを作った - sambaiz-net
Go moduleのキャッシュ Dockerコンテナ内でテストを実行しているが、毎回go moduleの解決が走ることで時間はかかるし、テザリングの容量に大打撃を受けたので、 ローカルのキャッシュをコピーするようにした。
test: docker build -t cdkbot-npmbin ./npm-lambda-layer docker build -t cdkbot-test -f ./test/Dockerfile . docker rm -f cdkbot-test || true docker run -itd --name cdkbot-test cdkbot-test /bin/sh docker cp .</description>
    </item>
    
    <item>
      <title>CDKでECS&#43;Fargate上にDigdagを立ててCognito認証を挟む</title>
      <link>https://www.sambaiz.net/article/234/</link>
      <pubDate>Wed, 31 Jul 2019 03:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/234/</guid>
      <description>AWSでワークフローエンジンDigdagを立てるにあたりスケールを見越してECS+Fargateで動かす。 全体のコードはGitHubにある。
FargateでECSを使う - sambaiz-net
リソースはCDKで作る。最近GAになったので高レベルのクラスを積極的に使っている。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
$ npm run cdk -- --version 1.2.0 (build 6b763b7) VPC FargateなのでVPCが必要。 テンプレートを書くとSubnetやRouteTable、NATGatewayなど記述量が多くなるところだが、CDKだとこれだけで済む。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
const vpc = new ec2.Vpc(this, &amp;#39;VPC&amp;#39;, { cidr: props.vpcCidr, natGateways: 1, maxAzs: 2, subnetConfiguration: [ { name: &amp;#39;digdag-public&amp;#39;, subnetType: ec2.SubnetType.PUBLIC, }, { name: &amp;#39;digdag-private&amp;#39;, subnetType: ec2.SubnetType.PRIVATE, }, { name: &amp;#39;digdag-db&amp;#39;, subnetType: ec2.SubnetType.ISOLATED, } ] }) DB DigdagはPostgreSQLを使う。
const db = new rds.DatabaseCluster(this, &amp;#39;DBCluster&amp;#39;, { engine: rds.</description>
    </item>
    
    <item>
      <title>CDK/CircleCI/GitHubでAWSリソース管理リポジトリを作る</title>
      <link>https://www.sambaiz.net/article/223/</link>
      <pubDate>Mon, 20 May 2019 09:23:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/223/</guid>
      <description>AWS CDKでリソースを記述し、PullRequestに対して自動でcdk diffで変更があるものを表示して、mergeしたときにcdk deployする。 全体のコードはGitHubにある。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
 追記 (2019-08-29): このフローで起こったいくつかの問題を解決するため新しいツールを作った。 PR上でCDKのレビューやデプロイを行うツールcdkbotを作った - sambaiz-net
 CI Userの作成 まずcdkコマンドを実行するためのCI Userを作成する。これはCDK管理外のスタックで、AWSコンソール上から手動で上げる。
AWSのAssumeRole - sambaiz-net
AssumeRoleしかできないCIUserからCIAssumeRoleをassumeすることにした。
AWSTemplateFormatVersion: &amp;#39;2010-09-09&amp;#39; Resources: CIAssumeRole: Type: &amp;#39;AWS::IAM::Role&amp;#39; Properties: RoleName: &amp;#39;CIAssumeRole&amp;#39; ManagedPolicyArns: - &amp;#39;arn:aws:iam::aws:policy/AdministratorAccess&amp;#39; AssumeRolePolicyDocument: Version: &amp;#39;2012-10-17&amp;#39; Statement: - Effect: &amp;#39;Allow&amp;#39; Principal: AWS: - !Ref AWS::AccountId Action: - &amp;#39;sts:AssumeRole&amp;#39; CIGroup: Type: &amp;#39;AWS::IAM::Group&amp;#39; Properties: GroupName: &amp;#39;CI&amp;#39; CIPolicies: Type: &amp;#39;AWS::IAM::Policy&amp;#39; Properties: PolicyName: &amp;#39;CI&amp;#39; PolicyDocument: Statement: - Effect: Allow Action: &amp;#39;sts:AssumeRole&amp;#39; Resource: !</description>
    </item>
    
    <item>
      <title>AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする</title>
      <link>https://www.sambaiz.net/article/222/</link>
      <pubDate>Sun, 19 May 2019 01:43:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/222/</guid>
      <description>AWS CDK(Cloud Development Kit)はTypeScriptやJavaなどのコードから CloudFormationのテンプレートを生成して差分を確認しデプロイできる公式のツール。まだdeveloper preview。
$ npm i -g aws-cdk $ cdk --version 0.33.0 (build 50d71bf) $ mkdir cdk-vpc $ cd cdk-vpc $ cdk init app --language=typescript CloudFormationのリソースと対応するCfnFooや、それを内部で作成する高レベル(L2)のResource ClassFooが実装されている。 ただし、現状CfnFooに対応するResource Classが存在しないものや、複数のリソースを内部で作成するResource Classが存在する。 例えば、ec2.VpcはCfnVPCだけではなく、Public/Private Subnet、NATGatewayまでまとめて一般的な構成で作る。Resource Classはまだ変更が多い。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
型があり補完が効くので通常のテンプレートと比べて書きやすいし、ループしたりすることもできる。
import * as cdk from &amp;#39;@aws-cdk/cdk&amp;#39; import * as ec2 from &amp;#39;@aws-cdk/aws-ec2&amp;#39; interface Export { vpc: ec2.Vpc } export class VPCStack extends cdk.Stack { protected deployEnv: string export: Export constructor(scope: cdk.</description>
    </item>
    
    <item>
      <title>DatadogのAWS integrationとAlertの設定をTerraformで行う</title>
      <link>https://www.sambaiz.net/article/219/</link>
      <pubDate>Sat, 04 May 2019 19:25:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/219/</guid>
      <description>DatadogのAWS integrationとAlertの設定をTerraformで行い、バージョン管理やレビューできるようにする。 全体のコードはGitHubに置いてある。
AWS Integration まずdatadog_integration_awsでAWS integrationの設定を作成してExternalIDを取得し、Policy/Roleを作成する。必要な権限はドキュメントを参照。
resource &amp;#34;datadog_integration_aws&amp;#34; &amp;#34;test&amp;#34; { account_id = &amp;#34;${var.aws_account_id}&amp;#34; role_name = &amp;#34;${var.aws_integration_role_name}&amp;#34; filter_tags = [&amp;#34;datadog:1&amp;#34;] } data &amp;#34;aws_iam_policy_document&amp;#34; &amp;#34;datadog_aws_integration_assume_role&amp;#34; { statement { actions = [&amp;#34;sts:AssumeRole&amp;#34;] principals { type = &amp;#34;AWS&amp;#34; identifiers = [&amp;#34;arn:aws:iam::464622532012:root&amp;#34;] } condition { test = &amp;#34;StringEquals&amp;#34; variable = &amp;#34;sts:ExternalId&amp;#34; values = [ &amp;#34;${datadog_integration_aws.test.external_id}&amp;#34;, ] } } } Datadog providerにはないSlackなどその他のintegrationは手動で設定する必要がある。 また、ログを集める場合Serverless Application Repositoryから公式のDatadog-Log-Forwarderを入れて AWS IntegrationのところにLambdaのARNを入れるのも手動。
 追記 (2020-12-07): 今はDatadog Forwaderとなり、Serverless Application Repositoryからではなく、直接CloudFormationのスタックを上げるようになっているのでaws_cloudformation_stackでデプロイできる。AWS IntegrationのRoleに必要なPolicyを与えてCollect LogsタブのLambda Cloudwatch Logsにチェックを入れると、Optionally limit resource collectionを設定しているならそのtagを持つ、全てのFunctionのStreamに自動でForwarderへのSubscription Filterが作成され転送が始まる。チェックを外すと削除されるが、この際もtagを見ているようで、条件を変更するとSubscription Filterが一部残ることがあった。ログだけではなくlambdaからのカスタムメトリクスの中継や、estimated_costなどの拡張メトリクスの送信も行う。</description>
    </item>
    
    <item>
      <title>KubernetesのCustom Resource Definition(CRD)とCustom Controller</title>
      <link>https://www.sambaiz.net/article/182/</link>
      <pubDate>Thu, 09 Aug 2018 23:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/182/</guid>
      <description>K8sではDeploymentを作成したときにReplicaSetも作成されるようにしたり、 Load Balancer Serviceを作成したときにGCPなどその環境に応じたLoad Balancerも作成されるようにしたりするため、Controllerがそれらを監視してAPIを呼んでいる。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
Controllerは単なるAPIを呼ぶアプリケーションなので自分でCustom Controllerを作成してDeploymentとしてデプロイすることもできる。 また、監視する対象もpodsやdeploymentsといった標準のAPIだけではなく、 Custom Resource で拡張したものを使うことができる。
特定のアプリケーションのためのControllerはOperatorとも呼ばれる。
CustomResourceDefinition(CRD) Custom Resourceを定義する。
apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: # REST APIで使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt; group: stable.example.com version: v1 # Namespaced か Cluster scope: Namespaced names: # 複数形 URLに使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;/&amp;lt;plural&amp;gt; plural: crontabs # 単数形 CLIなどで使われる singular: crontab # manifestで使う kind: CronTab shortNames: - ct $ kubectl create -f crd.yaml $ kubectl get crd NAME AGE crontabs.</description>
    </item>
    
    <item>
      <title>KubernetesにHelmでLocustによる分散負荷試験環境を立てる</title>
      <link>https://www.sambaiz.net/article/161/</link>
      <pubDate>Sun, 18 Mar 2018 22:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/161/</guid>
      <description>OSSの負荷試験ツールLocustをK8sクラスタに立てる。 K8sならworkerの増減も簡単だし、HelmのChartもあるので立てるのも楽。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
LocustはPython製で、以下のようなコードで処理を書くことができる。
@task(10)のように括弧の中に数字を書いて実行される割合を偏らせることもできる。 異なるTaskSetに対応するユーザーを複数作ることもできて、こちらもweightで重みを付けられる。 ユーザー数はあとでWeb上から入力する。
$ mkdir tasks $ cat tasks/tasks.py from locust import HttpLocust, TaskSet, task class ElbTasks(TaskSet): @task def task1(self): with self.client.get(&amp;#34;/&amp;#34;, catch_response=True) as response: if response.content != &amp;#34;Success&amp;#34;: response.failure(&amp;#34;Got wrong response&amp;#34;) class ElbWarmer(HttpLocust): task_set = ElbTasks min_wait = 1000 max_wait = 3000 stableにChartはあるが、今のところtasksの中を書き換えなくてはいけないようなので、forkしてきてtasksの中を書き換え、package して、helm repo index でこれを参照するindex.yamlを生成した。
 追記(2020-03-11): 今はConfigmapを自分で作成し --set worker.config.configmapName=*** することでforkしなくてもよくなった kubectl create configmap locust-worker-configs --from-file tasks/tasks.py
 $ helm package .</description>
    </item>
    
    <item>
      <title>RBACが有効なGKEでHelmを使う</title>
      <link>https://www.sambaiz.net/article/160/</link>
      <pubDate>Sun, 18 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/160/</guid>
      <description>k8sのパッケージマネージャーHelmを使う - sambaiz-net
$ helm version Client: &amp;amp;version.Version{SemVer:&amp;#34;v2.8.2&amp;#34;, GitCommit:&amp;#34;a80231648a1473929271764b920a8e346f6de844&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} Server: &amp;amp;version.Version{SemVer:&amp;#34;v2.8.2&amp;#34;, GitCommit:&amp;#34;a80231648a1473929271764b920a8e346f6de844&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} GKEでhelm initしてhelm installしたところ以下のエラーが返ってきた。
Error: release my-locust failed: namespaces &amp;#34;default&amp;#34; is forbidden: User &amp;#34;system:serviceaccount:kube-system:default&amp;#34; cannot get namespaces in the namespace &amp;#34;default&amp;#34;: Unknown user &amp;#34;system:serviceaccount:kube-system:default&amp;#34; GKEではデフォルトでK8sのRBAC(Role-Based Access Control)が有効になっているため、Tillerインスタンスに権限を与える必要がある。
ということでTiller用にnamespaceを切って、その中では好きにできるRoleと、Tillerが使うServiceAccountを作成し、RoleBindingで紐づける。
kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-manager namespace: tiller-world rules: - apiGroups: [&amp;#34;&amp;#34;, &amp;#34;extensions&amp;#34;, &amp;#34;apps&amp;#34;] resources: [&amp;#34;*&amp;#34;] verbs: [&amp;#34;*&amp;#34;] --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: tiller-world --- kind: RoleBinding apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>TerraformでVPCを管理するmoduleを作る</title>
      <link>https://www.sambaiz.net/article/121/</link>
      <pubDate>Sun, 23 Jul 2017 02:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/121/</guid>
      <description>Terraform
$ brew install terraform $ terraform -v Terraform v0.9.11 Terraformの設定要素 provider IaaS(e.g. AWS)、PaaS(e.g. Heroku)、SaaS(e.g. CloudFlare)など。
AWS Providerはこんな感じ。 ここに直接access_keyやsecret_keyを書くこともできるが、誤って公開されてしまわないように環境変数か variableで渡す。
provider &amp;#34;aws&amp;#34; {# access_key = &amp;#34;${var.access_key}&amp;#34; # secret_key = &amp;#34;${var.secret_key}&amp;#34;  region = &amp;#34;us-east-1&amp;#34; } $ export AWS_ACCESS_KEY_ID=&amp;#34;anaccesskey&amp;#34; $ export AWS_SECRET_ACCESS_KEY=&amp;#34;asecretkey&amp;#34; varibale CLIでオーバーライドできるパラメーター。typeにはstringのほかにmapやlistを渡すことができ、 何も渡さないとdefault値のものが、それもなければstringになる。
variable &amp;#34;key&amp;#34; { type = &amp;#34;string&amp;#34; default = &amp;#34;value&amp;#34; description = &amp;#34;description&amp;#34; } 値を渡す方法はTF_VAR_をprefixとする環境変数、-var、-var-fileがある。 また、moduleのinputとして渡されることもある。
$ export TF_VAR_somelist=&amp;#39;[&amp;#34;ami-abc123&amp;#34;, &amp;#34;ami-bcd234&amp;#34;]&amp;#39; $ terraform apply -var foo=bar -var foo=baz $ terraform apply -var-file=foo.</description>
    </item>
    
    <item>
      <title>iftopでネットワークの帯域を見る</title>
      <link>https://www.sambaiz.net/article/60/</link>
      <pubDate>Tue, 07 Feb 2017 20:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/60/</guid>
      <description>$ yum install --enablerepo=epel iftop $ iftop -f &amp;#34;not dst net 10.0.0.0/8&amp;#34; -i eth0のようにしてインタフェースを指定し、-fでフィルタをかけられる。フィルタの詳細はman pcap-filterで。
12.5Kb 25.0Kb 37.5Kb 50.0Kb	62.5Kb └─────────────────────────┴──────────────────────────┴──────────────────────────┴──────────────────────────┴────────────────────────── ip-172-31-9-9.ap-northeast-1.compute.internal =&amp;gt; 61-121-217-66.dh-connect.net 1.72Kb 6.57Kb 2.40Kb &amp;lt;= 416b 2.13Kb 702b ... ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── TX: cum: 22.6KB peak: 13.2Kb rates: 1.22Kb 1.27Kb 2.46Kb RX: 6.63KB 5.03Kb 208b 330b 748b TOTAL: 29.2KB 18.2Kb 1.42Kb 1.59Kb 3.19Kb 左から2, 10, 40秒間の平均kbps。TXが送信量、RXが受信量で、cumが総量、peakが最大。
実行中にSでソースのポートをDでディスティネーションのポートが表示される。</description>
    </item>
    
    <item>
      <title>vmstatのメモ</title>
      <link>https://www.sambaiz.net/article/59/</link>
      <pubDate>Mon, 06 Feb 2017 22:45:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/59/</guid>
      <description>$ vmstat 間隔(秒) procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 118588 80388 2516284 0 0 2 77 141 85 1 0 98 0 0 procs   r: 実行待ちプロセス数。CPUの処理が追いついていない。
  b: 割り込み不可能なスリープ中のプロセス数。I/O待ちらしい。
  memory   swpd: バーチャルメモリの使用量。
  free: 空きメモリ量。
  buff: バッファに使われてるメモリ量。
  cache: キャッシュに使われているメモリ量。</description>
    </item>
    
    <item>
      <title>複数EC2インスタンスを立ち上げてvegetaで負荷試験する</title>
      <link>https://www.sambaiz.net/article/43/</link>
      <pubDate>Sun, 18 Dec 2016 20:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/43/</guid>
      <description>vegetaで負荷をかける。
インスタンスを立ち上げるスクリプト コードはここ。 sambaiz/loadtest
まずキーペアを作成し、EC2インスタンスを立ち上げて、全てのインスタンスが使えるようになるまで待つ。
aws ec2 create-key-pair --key-name LoadTestKeyPare --query &amp;#39;KeyMaterial&amp;#39; --output text &amp;gt; LoadTestKeyPare.pem chmod 400 LoadTestKeyPare.pem aws ec2 run-instances --image-id $AMI_ID --count $INSTANCE_NUM --instance-type t2.micro --key-name LoadTestKeyPare --security-group-ids $SECURITY_GROUP_IDS --subnet-id $SUBNET_ID ... aws ec2 wait instance-status-ok --instance-ids $INSTANCE_IDS このAMIは事前にPackerでつくったもの。vegetaをインストールしてファイルディスクリプタの上限を増やしている。
{ &amp;#34;variables&amp;#34;: { &amp;#34;aws_access_key&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;aws_secret_key&amp;#34;: &amp;#34;&amp;#34; }, &amp;#34;builders&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;amazon-ebs&amp;#34;, &amp;#34;access_key&amp;#34;: &amp;#34;{{user `aws_access_key`}}&amp;#34;, &amp;#34;secret_key&amp;#34;: &amp;#34;{{user `aws_secret_key`}}&amp;#34;, &amp;#34;region&amp;#34;: &amp;#34;ap-northeast-1&amp;#34;, &amp;#34;source_ami&amp;#34;: &amp;#34;ami-0c11b26d&amp;#34;, &amp;#34;instance_type&amp;#34;: &amp;#34;t2.micro&amp;#34;, &amp;#34;ssh_username&amp;#34;: &amp;#34;ec2-user&amp;#34;, &amp;#34;ami_name&amp;#34;: &amp;#34;loadtest {{timestamp}}&amp;#34; }], &amp;#34;provisioners&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;shell&amp;#34;, &amp;#34;inline&amp;#34;: [ &amp;#34;wget https://github.</description>
    </item>
    
    <item>
      <title>SSHポートフォワーディングとnetstatのメモ</title>
      <link>https://www.sambaiz.net/article/42/</link>
      <pubDate>Sat, 17 Dec 2016 12:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/42/</guid>
      <description>SSHポートフォワーディング ローカルの8080ポートを、example.comを通したexample2.comの80ポートに向ける。
$ ssh hoge@example.com -Nf -L 8080:example2.com:80 $ curl localhost:8080 # =&amp;gt; example2.com:80  -N: リモートでコマンドを実行しない -f: バックグラウンドで実行  netstat ネットワークの状態を確認する。
$ netstat -ant Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN ...  -a: non-listening(TCPではESTABLISHED状態)しているソケットだけではなく、listeningしている情報も出す -n: 数値のアドレスで表示する -t: TCPで制限  </description>
    </item>
    
    <item>
      <title>ファイルディスクリプタの上限を増やす</title>
      <link>https://www.sambaiz.net/article/41/</link>
      <pubDate>Thu, 08 Dec 2016 21:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/41/</guid>
      <description>ファイルディスクリプタとは プロセスの外部とやりとりするための識別子。POSIXではint型で、0がstdin、1がstdout、2がstderrとなっている。 ファイルやデバイスに対するopen()や、 ネットワーク(INETドメインソケット)やホスト内(UNIXドメインソケット)で 通信するためのソケットを生成するsocket()などのシステムコールで割り当てられる。
ファイルディスクリプタの上限 一つのプロセスがリソースを食いつぶさないように使えるファイルディスクリプタの上限が決まっていて、現在のプロセスの値はulimit -nで確認できる。
$ ulimit -n 1024 各プロセスの上限と、使っているファイルディスクリプタは次のようにして確認できる。
$ cat /proc/&amp;lt;プロセスID&amp;gt;/limits ... Max open files 1024 4096 files ... $ ls -l /proc/&amp;lt;プロセスID&amp;gt;/fd webサーバーのような同時に大量の通信をするプロセスではこの上限に達してしまい、Too many open filesになってしまうことがあるので増やす必要がある。 上限を変更する方法として次の方法がある。
/etc/security/limits.confで変更する ログインの際などに行われるPAM認証時に適用されるので、サーバーの起動時に立ち上がったデーモンには適用されない。
$ cat /etc/pam.d/sshd ... session required pam_limits.so ... 全てのユーザー(*)のプロセスが使えるファイルディスクリプタの、ユーザーが設定できるsoftとrootが設定できるhard limitを共に64000にする。
$ echo &amp;#34;* hard nofile 64000&amp;#34; &amp;gt;&amp;gt; /etc/security/limits.conf $ echo &amp;#34;* soft nofile 64000&amp;#34; &amp;gt;&amp;gt; /etc/security/limits.conf $ ulimit -n 64000 ulimit -nで変更する シェルと、そこから起動したプロセスでのみ有効。
$ ulimit -n 64000 (dockerの場合) run時の&amp;ndash;ulimitで変更する $ docker run -itd --ulimit nofile=11111 ubuntu $ docker exec -it &amp;lt;id&amp;gt; /bin/bash -c &amp;#34;ulimit -n&amp;#34; 11111 参考 /etc/security/limits.</description>
    </item>
    
    <item>
      <title>OpenVPNサーバーPritunlをDockerで動かす</title>
      <link>https://www.sambaiz.net/article/39/</link>
      <pubDate>Fri, 02 Dec 2016 21:05:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/39/</guid>
      <description>PritunlでVPNサーバーを立てる。
Dockerfileはこんな感じ。
https://hub.docker.com/r/sambaiz/pritunl/
FROMmongo:3.4 # https://docs.pritunl.com/docs/installation RUN echo &amp;#39;deb http://repo.pritunl.com/stable/apt jessie main&amp;#39; &amp;gt; /etc/apt/sources.list.d/pritunl.list &amp;amp;&amp;amp; \  apt-key adv --keyserver hkp://keyserver.ubuntu.com --recv 7568D9BB55FF9E5287D586017AE645C0CF8E292A &amp;amp;&amp;amp; \  apt-get --assume-yes update &amp;amp;&amp;amp; \  apt-get --assume-yes upgrade &amp;amp;&amp;amp; \  apt-get --assume-yes install pritunl iptables EXPOSE80 443 12345/udp CMD mongod --fork --logpath /data/db/mongod.log &amp;amp;&amp;amp; echo &amp;#39;Setup Key:&amp;#39; `pritunl setup-key` &amp;amp;&amp;amp; pritunl start $ docker run -itd -p 80:80 -p 443:443 -p 12345:12345/udp --privileged sambaiz/pritunl $ docker logs &amp;lt;id&amp;gt; .</description>
    </item>
    
    <item>
      <title>PackerでAMIを作る</title>
      <link>https://www.sambaiz.net/article/24/</link>
      <pubDate>Tue, 18 Oct 2016 22:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/24/</guid>
      <description>https://www.packer.io/
いろんなプラットフォームのイメージを作ることができるツール。 これでfluentdのログサーバーのAMIを作る。
$ brew install packer # mac $ packer -v 0.10.1 設定ファイルはこんな感じ。variablesの値は{{user ... }}のところで使われる。 buildersに作るイメージの情報を書いて、provisionersで環境を作る。
provisionersにはchefやansibleなども指定できるが、 継ぎ足し継ぎ足しで秘伝のタレ化したAMIも最初は、コマンドいくつか実行するだけなのでとりあえず手作業で作った、後でなんとかするなんてものもあったりして、 そういうものは無理にchefなどで始めず、手軽にshellでpacker buildするといいと思う。 手作業よりも楽だしソースが別にあるので使われていないAMIを消すのも簡単。
fileではpermissionがないところに置くことができないので、一旦置いてshellで移動する。
{ &amp;#34;variables&amp;#34;: { &amp;#34;aws_access_key&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;aws_secret_key&amp;#34;: &amp;#34;&amp;#34; }, &amp;#34;builders&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;amazon-ebs&amp;#34;, &amp;#34;access_key&amp;#34;: &amp;#34;{{user `aws_access_key`}}&amp;#34;, &amp;#34;secret_key&amp;#34;: &amp;#34;{{user `aws_secret_key`}}&amp;#34;, &amp;#34;region&amp;#34;: &amp;#34;ap-northeast-1&amp;#34;, &amp;#34;source_ami&amp;#34;: &amp;#34;ami-1a15c77b&amp;#34;, &amp;#34;instance_type&amp;#34;: &amp;#34;t2.small&amp;#34;, &amp;#34;ssh_username&amp;#34;: &amp;#34;ec2-user&amp;#34;, &amp;#34;ami_name&amp;#34;: &amp;#34;fluentd-logserver {{timestamp}}&amp;#34; }], &amp;#34;provisioners&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;file&amp;#34;, &amp;#34;source&amp;#34;: &amp;#34;td-agent.conf&amp;#34;, &amp;#34;destination&amp;#34;: &amp;#34;/home/ec2-user/td-agent.conf&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;shell&amp;#34;, &amp;#34;inline&amp;#34;: [ &amp;#34;curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent2.sh | sh&amp;#34;, &amp;#34;sudo mv /home/ec2-user/td-agent.</description>
    </item>
    
  </channel>
</rss>
