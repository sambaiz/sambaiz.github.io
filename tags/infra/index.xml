<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>infra on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/infra/</link>
    <description>Recent content in infra on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Tue, 17 Sep 2019 09:42:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/infra/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ansibleでnginxを入れてLoad Balancingさせる</title>
      <link>https://www.sambaiz.net/article/239/</link>
      <pubDate>Tue, 17 Sep 2019 09:42:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/239/</guid>
      <description>EC2でUbuntu Server 18.04 LTS (ami-07d0cf3af28718ef8) の t3.medium (2vCPU, 4GiB) インスタンスを3台立ち上げた。この内1台をLB用とし、2台のAppサーバーに負荷分散させる。
https://github.com/sambaiz/ansible-nginx-lb-example
nginx.conf LBとAppのnginx.conf。upstreamはデフォルトでラウンドロビンする。
$ cat conf/lb/nginx.conf ... http { ... upstream app-server { server ip-172-31-94-208.ec2.internal; server ip-172-31-88-90.ec2.internal; } server { listen 80; server_name .compute-1.amazonaws.com; access_log /var/log/nginx/app-server.log main; location / { proxy_pass http://app-server; } } } $ cat conf/app/nginx.conf ... http { ... server { listen 80; server_name .compute-1.amazonaws.com; location / { proxy_pass http://127.0.0.1:8080; } } } Ansibleの実行 まずInventoryを書いて疎通確認する。
$ pip install --user ansible $ cat hosts [lb] ec2-3-227-2-54.</description>
    </item>
    
    <item>
      <title>User NamespaceでrootになってNetwork Namespaceを作りvethとNATで外と通信する</title>
      <link>https://www.sambaiz.net/article/237/</link>
      <pubDate>Fri, 06 Sep 2019 02:20:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/237/</guid>
      <description>LinuxのNamespaceはuidやpid、networkなどを分離できる機能で、Dockerなどのコンテナ技術で使われている。
# Amazon Linux 2 (ami-0ff21806645c5e492) $ uname -r 4.14.138-114.102.amzn2.x86_64 User NamespaceでRootになる rootでないと正常終了しないコードを書いた。
$ cat /tmp/root_only.sh #!/bin/sh if [ &amp;quot;$(id -u)&amp;quot; != &amp;quot;0&amp;quot; ]; then echo &amp;quot;you are not root...&amp;quot; exit 1 fi echo &amp;quot;you are root!&amp;quot; 実際ec2-userではexit 1になる。
$ id -u 1000 $ sh /tmp/root_only.sh; echo $? you are not root... 1 User Namespaceを作る。rootでないとそれ以外のNamespaceは作れない。
$ unshare --net unshare: unshare failed: Operation not permitted $ unshare --user $ id uid=65534(nfsnobody) gid=65534(nfsnobody) groups=65534(nfsnobody) $ echo $$ 3537 他のshellを開き、Namespaceの外側からuid_mapを書き込む。 外側の1000から始まるuidを、このNamespaceの0から始まるuidに範囲1でマッピングする。</description>
    </item>
    
    <item>
      <title>SystemdのService</title>
      <link>https://www.sambaiz.net/article/236/</link>
      <pubDate>Fri, 30 Aug 2019 00:16:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/236/</guid>
      <description>SystemdはLinuxで動くServiceの管理などを行うデーモン。initの後継でchkconfig/servicceの代わりにsystemctlコマンドを使う。
$ systemctl start name.service $ systemctl stop name.service $ systemctl status name.service $ systemctl enable name.service $ systemctl disable name.service Serviceの一覧を見る。
$ systemctl list-units --type service --all UNIT LOAD ACTIVE SUB DESCRIPTION accounts-daemon.service loaded active running Accounts Service acpid.service loaded active running ACPI event daemon apparmor.service loaded active exited LSB: AppArmor initialization apport.service loaded active exited LSB: automatic crash report generation $ systemctl list-unit-files --type service UNIT FILE STATE accounts-daemon.</description>
    </item>
    
    <item>
      <title>PR上でCDKのレビューやデプロイを行うツールcdkbotを作った</title>
      <link>https://www.sambaiz.net/article/235/</link>
      <pubDate>Thu, 29 Aug 2019 22:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/235/</guid>
      <description>sambaiz/cdkbot
PRのコメントで/diffや/deployと打つとcdk diffやcdk deployが走る。 diffを見てレビューし、良ければ/deployでデプロイし完了するとmergeされる。
以前CircleCIでmerge時にdeployされる仕組みを作った。
CDK/CircleCI/GitHubでAWSリソース管理リポジトリを作る - sambaiz-net
ただ、この仕組みだと CFnの実行時エラーのためにデプロイできない状態のものがmasterブランチにmergeされてしまい、その修正のために何回も試行錯誤のPRを出すことになったり、 Stack間の依存がある場合リソースを削除するとcdk deployによって依存解決された順序だと失敗してしまうという問題があった。 cdkbotでは必要ならデプロイするStackを選べて、完了してからmergeすることでこれらの問題を解決した。 また、AWS外のCIにとても強い権限を与えていたがそれも必要なくなった。
単純にブランチの状態でデプロイしてしまうと古い状態に巻き戻ってしまう可能性があるので、内部でbaseブランチをmergeしていたり、 ラベルによってそのPRがデプロイ可能かどうかを制御していたりする。 最低限デプロイできるようになってから、この辺りの仕組みを整えるまでに存外に時間がかかった。
Serverless Application Repositoryに公開してあるので簡単にインストールできる。
AWS SAMでLambdaの関数をデプロイしServerless Application Repositoryに公開する - sambaiz-net
 追記 (2019-10-26): ap-northeast-1に対応していないのと、ECSのリソースを作成できないため、Serverless Application Repositoryに公開するのはやめた。makeでインストールできる。 Lambda環境でできない処理をECSで実行する - sambaiz-net
 外部コマンド gitやnpmといった外部コマンドを実行する必要があるが、標準では入っていないのでLambda Layerで入れている。
Lambda上でnpm installできるLayerを作った - sambaiz-net
Go moduleのキャッシュ Dockerコンテナ内でテストを実行しているが、毎回go moduleの解決が走ることで時間はかかるし、テザリングの容量に大打撃を受けたので、 ローカルのキャッシュをコピーするようにした。
test: docker build -t cdkbot-npmbin ./npm-lambda-layer docker build -t cdkbot-test -f ./test/Dockerfile . docker rm -f cdkbot-test || true docker run -itd --name cdkbot-test cdkbot-test /bin/sh docker cp .</description>
    </item>
    
    <item>
      <title>CDKでECS&#43;Fargate上にDigdagを立ててCognito認証を挟む</title>
      <link>https://www.sambaiz.net/article/234/</link>
      <pubDate>Wed, 31 Jul 2019 03:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/234/</guid>
      <description>AWSでワークフローエンジンDigdagを立てるにあたりスケールを見越してECS+Fargateで動かす。 全体のコードはGitHubにある。
FargateでECSを使う - sambaiz-net
リソースはCDKで作る。最近GAになったので高レベルのクラスを積極的に使っている。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
$ npm run cdk -- --version 1.2.0 (build 6b763b7) VPC FargateなのでVPCが必要。 テンプレートを書くとSubnetやRouteTable、NATGatewayなど記述量が多くなるところだが、CDKだとこれだけで済む。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
const vpc = new ec2.Vpc(this, &#39;VPC&#39;, { cidr: props.vpcCidr, natGateways: 1, maxAzs: 2, subnetConfiguration: [ { name: &#39;digdag-public&#39;, subnetType: ec2.SubnetType.PUBLIC, }, { name: &#39;digdag-private&#39;, subnetType: ec2.SubnetType.PRIVATE, }, { name: &#39;digdag-db&#39;, subnetType: ec2.SubnetType.ISOLATED, } ] }) DB DigdagはPostgreSQLを使う。
const db = new rds.DatabaseCluster(this, &#39;DBCluster&#39;, { engine: rds.</description>
    </item>
    
    <item>
      <title>CDK/CircleCI/GitHubでAWSリソース管理リポジトリを作る</title>
      <link>https://www.sambaiz.net/article/223/</link>
      <pubDate>Mon, 20 May 2019 09:23:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/223/</guid>
      <description>AWS CDKでリソースを記述し、PullRequestに対して自動でcdk diffで変更があるものを表示して、mergeしたときにcdk deployする。 全体のコードはGitHubにある。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
 追記 (2019-08-29): このフローで起こったいくつかの問題を解決するため新しいツールを作った。 PR上でCDKのレビューやデプロイを行うツールcdkbotを作った - sambaiz-net
 CI Userの作成 まずcdkコマンドを実行するためのCI Userを作成する。これはCDK管理外のスタックで、AWSコンソール上から手動で上げる。
AWSのAssumeRole - sambaiz-net
AssumeRoleしかできないCIUserからCIAssumeRoleをassumeすることにした。
AWSTemplateFormatVersion: &#39;2010-09-09&#39; Resources: CIAssumeRole: Type: &#39;AWS::IAM::Role&#39; Properties: RoleName: &#39;CIAssumeRole&#39; ManagedPolicyArns: - &#39;arn:aws:iam::aws:policy/AdministratorAccess&#39; AssumeRolePolicyDocument: Version: &#39;2012-10-17&#39; Statement: - Effect: &#39;Allow&#39; Principal: AWS: - !Ref AWS::AccountId Action: - &#39;sts:AssumeRole&#39; CIGroup: Type: &#39;AWS::IAM::Group&#39; Properties: GroupName: &#39;CI&#39; CIPolicies: Type: &#39;AWS::IAM::Policy&#39; Properties: PolicyName: &#39;CI&#39; PolicyDocument: Statement: - Effect: Allow Action: &#39;sts:AssumeRole&#39; Resource: !</description>
    </item>
    
    <item>
      <title>AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする</title>
      <link>https://www.sambaiz.net/article/222/</link>
      <pubDate>Sun, 19 May 2019 01:43:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/222/</guid>
      <description>AWS CDK(Cloud Development Kit)はTypeScriptやJavaなどのコードから CloudFormationのテンプレートを生成して差分を確認しデプロイできる公式のツール。まだdeveloper preview。
$ npm i -g aws-cdk $ cdk --version 0.33.0 (build 50d71bf) $ mkdir cdk-vpc $ cd cdk-vpc $ cdk init app --language=typescript CloudFormationのリソースと対応するCfnFooや、それを内部で作成する高レベル(L2)のResource ClassFooが実装されている。 ただし、現状CfnFooに対応するResource Classが存在しないものや、複数のリソースを内部で作成するResource Classが存在する。 例えば、ec2.VpcはCfnVPCだけではなく、Public/Private Subnet、NATGatewayまでまとめて一般的な構成で作る。Resource Classはまだ変更が多い。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
型があり補完が効くので通常のテンプレートと比べて書きやすいし、ループしたりすることもできる。
import * as cdk from &#39;@aws-cdk/cdk&#39; import * as ec2 from &#39;@aws-cdk/aws-ec2&#39; interface Export { vpc: ec2.Vpc } export class VPCStack extends cdk.Stack { protected deployEnv: string export: Export constructor(scope: cdk.</description>
    </item>
    
    <item>
      <title>DatadogのAWS integrationとAlertの設定をTerraformで行う</title>
      <link>https://www.sambaiz.net/article/219/</link>
      <pubDate>Sat, 04 May 2019 19:25:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/219/</guid>
      <description>DatadogのAWS integrationとAlertの設定をTerraformで行い、バージョン管理やレビューできるようにする。 全体のコードはGitHubに置いてある。
AWS Integration まずdatadog_integration_awsでAWS integrationの設定を作成してExternalIDを取得し、Policy/Roleを作成する。必要な権限はドキュメントを参照。
resource &amp;quot;datadog_integration_aws&amp;quot; &amp;quot;test&amp;quot; { account_id = &amp;quot;${var.aws_account_id}&amp;quot; role_name = &amp;quot;${var.aws_integration_role_name}&amp;quot; filter_tags = [&amp;quot;datadog:1&amp;quot;] } data &amp;quot;aws_iam_policy_document&amp;quot; &amp;quot;datadog_aws_integration_assume_role&amp;quot; { statement { actions = [&amp;quot;sts:AssumeRole&amp;quot;] principals { type = &amp;quot;AWS&amp;quot; identifiers = [&amp;quot;arn:aws:iam::464622532012:root&amp;quot;] } condition { test = &amp;quot;StringEquals&amp;quot; variable = &amp;quot;sts:ExternalId&amp;quot; values = [ &amp;quot;${datadog_integration_aws.test.external_id}&amp;quot;, ] } } } Datadog providerにはないSlackなどその他のintegrationは手動で設定する必要がある。 また、Logも集める場合Serverless Application Repositoryから公式のDatadog-Log-Forwarderを入れて AWS IntegrationのところにLambdaのARNを入れるのも手動。
Alert datadog_monitorでAlertを作成する。 今回はLambdaの実行が失敗したときと、WARNという文字列が含まれるログが一定数出力されたときにAlertを飛ばすようにしてみた。 なおinfoやwarnといったログレベルはjsonのlevelフィールドに入れればデフォルトのpipelineでマップされるので通常は文字列比較などする必要はない。
初めに作るときは一度画面で意図通りアラートが飛ぶものを作成し、クエリなどをコピーするのが確実。
resource &amp;quot;datadog_monitor&amp;quot; &amp;quot;lambda-error-alert&amp;quot; { name = &amp;quot;lambda-error-alert&amp;quot; type = &amp;quot;metric alert&amp;quot; message = &amp;quot;Error occurred.</description>
    </item>
    
    <item>
      <title>KubernetesのCustom Resource Definition(CRD)とCustom Controller</title>
      <link>https://www.sambaiz.net/article/182/</link>
      <pubDate>Thu, 09 Aug 2018 23:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/182/</guid>
      <description>K8sではDeploymentを作成したときにReplicaSetも作成されるようにしたり、 Load Balancer Serviceを作成したときにGCPなどその環境に応じたLoad Balancerも作成されるようにしたりするため、Controllerがそれらを監視してAPIを呼んでいる。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
Controllerは単なるAPIを呼ぶアプリケーションなので自分でCustom Controllerを作成してDeploymentとしてデプロイすることもできる。 また、監視する対象もpodsやdeploymentsといった標準のAPIだけではなく、 Custom Resource で拡張したものを使うことができる。
特定のアプリケーションのためのControllerはOperatorとも呼ばれる。
CustomResourceDefinition(CRD) Custom Resourceを定義する。
apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: # REST APIで使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt; group: stable.example.com version: v1 # Namespaced か Cluster scope: Namespaced names: # 複数形 URLに使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;/&amp;lt;plural&amp;gt; plural: crontabs # 単数形 CLIなどで使われる singular: crontab # manifestで使う kind: CronTab shortNames: - ct $ kubectl create -f crd.yaml $ kubectl get crd NAME AGE crontabs.</description>
    </item>
    
    <item>
      <title>KubernetesにHelmでLocustによる分散負荷試験環境を立てる</title>
      <link>https://www.sambaiz.net/article/161/</link>
      <pubDate>Sun, 18 Mar 2018 22:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/161/</guid>
      <description>OSSの負荷試験ツールLocustをK8sクラスタに立てる。 K8sならworkerの増減も簡単だし、HelmのChartもあるので立てるのも楽。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
LocustはPython製で、以下のようなコードで処理を書くことができる。
@task(10)のように括弧の中に数字を書いて実行される割合を偏らせることもできる。 異なるTaskSetに対応するユーザーを複数作ることもできて、こちらもweightで重みを付けられる。 ユーザー数はあとでWeb上から入力する。
$ mkdir tasks $ cat tasks/tasks.py from locust import HttpLocust, TaskSet, task class ElbTasks(TaskSet): @task def task1(self): with self.client.get(&amp;quot;/&amp;quot;, catch_response=True) as response: if response.content != &amp;quot;Success&amp;quot;: response.failure(&amp;quot;Got wrong response&amp;quot;) class ElbWarmer(HttpLocust): task_set = ElbTasks min_wait = 1000 max_wait = 3000 stableにChartはあるが、今のところtasksの中を書き換えなくてはいけないようなので、forkしてきてtasksの中を書き換え、package して、helm repo index でこれを参照するindex.yamlを生成した。
 追記(2020-03-11): 今はConfigmapを自分で作成し --set worker.config.configmapName=*** することでforkしなくてもよくなった kubectl create configmap locust-worker-configs --from-file tasks/tasks.py
 $ helm package .</description>
    </item>
    
    <item>
      <title>RBACが有効なGKEでHelmを使う</title>
      <link>https://www.sambaiz.net/article/160/</link>
      <pubDate>Sun, 18 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/160/</guid>
      <description>k8sのパッケージマネージャーHelmを使う - sambaiz-net
$ helm version Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;} Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;} GKEでhelm initしてhelm installしたところ以下のエラーが返ってきた。
Error: release my-locust failed: namespaces &amp;quot;default&amp;quot; is forbidden: User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot get namespaces in the namespace &amp;quot;default&amp;quot;: Unknown user &amp;quot;system:serviceaccount:kube-system:default&amp;quot; GKEではデフォルトでK8sのRBAC(Role-Based Access Control)が有効になっているため、Tillerインスタンスに権限を与える必要がある。
ということでTiller用にnamespaceを切って、その中では好きにできるRoleと、Tillerが使うServiceAccountを作成し、RoleBindingで紐づける。
kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-manager namespace: tiller-world rules: - apiGroups: [&amp;quot;&amp;quot;, &amp;quot;extensions&amp;quot;, &amp;quot;apps&amp;quot;] resources: [&amp;quot;*&amp;quot;] verbs: [&amp;quot;*&amp;quot;] --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: tiller-world --- kind: RoleBinding apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>TerraformでVPCを管理するmoduleを作る</title>
      <link>https://www.sambaiz.net/article/121/</link>
      <pubDate>Sun, 23 Jul 2017 02:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/121/</guid>
      <description>Terraform
$ brew install terraform $ terraform -v Terraform v0.9.11 Terraformの設定要素 provider IaaS(e.g. AWS)、PaaS(e.g. Heroku)、SaaS(e.g. CloudFlare)など。
AWS Providerはこんな感じ。 ここに直接access_keyやsecret_keyを書くこともできるけど、誤って公開されてしまわないように環境変数か variableで渡す。
provider &amp;quot;aws&amp;quot; { # access_key = &amp;quot;${var.access_key}&amp;quot; # secret_key = &amp;quot;${var.secret_key}&amp;quot; region = &amp;quot;us-east-1&amp;quot; } $ export AWS_ACCESS_KEY_ID=&amp;quot;anaccesskey&amp;quot; $ export AWS_SECRET_ACCESS_KEY=&amp;quot;asecretkey&amp;quot; varibale CLIでオーバーライドできるパラメーター。typeにはstringのほかにmapやlistを渡すことができ、 何も渡さないとdefault値のものが、それもなければstringになる。
variable &amp;quot;key&amp;quot; { type = &amp;quot;string&amp;quot; default = &amp;quot;value&amp;quot; description = &amp;quot;description&amp;quot; } 値を渡す方法はTF_VAR_をprefixとする環境変数、-var、-var-fileがある。 また、moduleのinputとして渡されることもある。
$ export TF_VAR_somelist=&#39;[&amp;quot;ami-abc123&amp;quot;, &amp;quot;ami-bcd234&amp;quot;]&#39; $ terraform apply -var foo=bar -var foo=baz $ terraform apply -var-file=foo.</description>
    </item>
    
    <item>
      <title>iftopでネットワークの帯域を見る</title>
      <link>https://www.sambaiz.net/article/60/</link>
      <pubDate>Tue, 07 Feb 2017 20:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/60/</guid>
      <description>$ yum install --enablerepo=epel iftop $ iftop -f &amp;quot;not dst net 10.0.0.0/8&amp;quot; -i eth0のようにしてインタフェースを指定し、-fでフィルタをかけられる。フィルタの詳細はman pcap-filterで。
 12.5Kb 25.0Kb 37.5Kb 50.0Kb	62.5Kb └─────────────────────────┴──────────────────────────┴──────────────────────────┴──────────────────────────┴────────────────────────── ip-172-31-9-9.ap-northeast-1.compute.internal =&amp;gt; 61-121-217-66.dh-connect.net 1.72Kb 6.57Kb 2.40Kb &amp;lt;= 416b 2.13Kb 702b ... ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── TX: cum: 22.6KB peak: 13.2Kb rates: 1.22Kb 1.27Kb 2.46Kb RX: 6.63KB 5.03Kb 208b 330b 748b TOTAL: 29.2KB 18.2Kb 1.42Kb 1.59Kb 3.19Kb 左から2, 10, 40秒間の平均kbps。TXが送信量、RXが受信量で、cumが総量、peakが最大。
実行中にSでソースのポートをDでディスティネーションのポートが表示される。</description>
    </item>
    
    <item>
      <title>vmstatのメモ</title>
      <link>https://www.sambaiz.net/article/59/</link>
      <pubDate>Mon, 06 Feb 2017 22:45:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/59/</guid>
      <description>$ vmstat 間隔(秒) procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu----- r b swpd free buff cache si so bi bo in cs us sy id wa st 0 0 0 118588 80388 2516284 0 0 2 77 141 85 1 0 98 0 0 procs   r: 実行待ちプロセス数。CPUの処理が追いついていない。
  b: 割り込み不可能なスリープ中のプロセス数。I/O待ちらしい。
  memory   swpd: バーチャルメモリの使用量。
  free: 空きメモリ量。
  buff: バッファに使われてるメモリ量。
  cache: キャッシュに使われているメモリ量。</description>
    </item>
    
    <item>
      <title>複数EC2インスタンスを立ち上げてvegetaで負荷試験する</title>
      <link>https://www.sambaiz.net/article/43/</link>
      <pubDate>Sun, 18 Dec 2016 20:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/43/</guid>
      <description>vegetaで負荷をかける。
インスタンスを立ち上げるスクリプト コードはここ。 sambaiz/loadtest
まずキーペアを作成し、EC2インスタンスを立ち上げて、全てのインスタンスが使えるようになるまで待つ。
aws ec2 create-key-pair --key-name LoadTestKeyPare --query &#39;KeyMaterial&#39; --output text &amp;gt; LoadTestKeyPare.pem chmod 400 LoadTestKeyPare.pem aws ec2 run-instances --image-id $AMI_ID --count $INSTANCE_NUM --instance-type t2.micro --key-name LoadTestKeyPare --security-group-ids $SECURITY_GROUP_IDS --subnet-id $SUBNET_ID ... aws ec2 wait instance-status-ok --instance-ids $INSTANCE_IDS このAMIは事前にPackerでつくったもの。vegetaをインストールしてファイルディスクリプタの上限を増やしている。
{ &amp;quot;variables&amp;quot;: { &amp;quot;aws_access_key&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;aws_secret_key&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `aws_access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `aws_secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;ap-northeast-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-0c11b26d&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.micro&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ec2-user&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;loadtest {{timestamp}}&amp;quot; }], &amp;quot;provisioners&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;, &amp;quot;inline&amp;quot;: [ &amp;quot;wget https://github.</description>
    </item>
    
    <item>
      <title>SSHポートフォワーディングとnetstatのメモ</title>
      <link>https://www.sambaiz.net/article/42/</link>
      <pubDate>Sat, 17 Dec 2016 12:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/42/</guid>
      <description>SSHポートフォワーディング ローカルの8080ポートを、example.comを通したexample2.comの80ポートに向ける。
$ ssh hoge@example.com -Nf -L 8080:example2.com:80 $ curl localhost:8080 # =&amp;gt; example2.com:80  -N: リモートでコマンドを実行しない -f: バックグラウンドで実行  netstat ネットワークの状態を確認する。
$ netstat -ant Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN ...  -a: non-listening(TCPではESTABLISHED状態)しているソケットだけではなく、listeningしている情報も出す -n: 数値のアドレスで表示する -t: TCPで制限  </description>
    </item>
    
    <item>
      <title>ファイルディスクリプタの上限を増やす</title>
      <link>https://www.sambaiz.net/article/41/</link>
      <pubDate>Thu, 08 Dec 2016 21:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/41/</guid>
      <description>ファイルディスクリプタとは プロセスの外部とやりとりするための識別子。POSIXではint型で、0がstdin、1がstdout、2がstderrといったもの。 ファイルやデバイスに対するopen()や、 ネットワーク(INETドメインソケット)やホスト内(UNIXドメインソケット)で 通信するためのソケットを生成するsocket()などのシステムコールで生成される。
ファイルディスクリプタの上限 一つのプロセスがリソースを食いつぶさないように 使えるファイルディスクリプタの上限が決まっていて、ulimit -nで確認できる。デフォルトは大体1024。
$ ulimit -n 1024 各プロセスの上限と使っているファイルディスクリプタはこれで確認できる。
$ cat /proc/&amp;lt;プロセスID&amp;gt;/limits ... Max open files 1024 4096 files ... $ ls -l /proc/&amp;lt;プロセスID&amp;gt;/fd webサーバーのように同時にたくさん通信したりすると上限に達してしまい、Too many open filesになってしまうので増やす必要がある。
/etc/security/limits.confで変更する PAM認証時(ログインするときなど)に適用されるので、サーバーの起動時に立ち上がったデーモンには使えない。
$ cat /etc/pam.d/sshd ... session required pam_limits.so ... 全てのユーザー(*)のプロセスが使える ファイルディスクリプタ(nofile)のsoft(ユーザーが設定できる)とhard(rootが設定できる)上限を共に64000にする。
$ echo &amp;quot;* hard nofile 64000&amp;quot; &amp;gt;&amp;gt; /etc/security/limits.conf $ echo &amp;quot;* soft nofile 64000&amp;quot; &amp;gt;&amp;gt; /etc/security/limits.conf $ ulimit -n 64000 ulimit -nで変更する シェルと、起動したプロセスで有効。
$ ulimit -n 64000 dockerコンテナでは run時にulimitオプションで --ulimit nofile=11111のように指定することもできる。</description>
    </item>
    
    <item>
      <title>OpenVPNサーバーPritunlをDockerで動かす</title>
      <link>https://www.sambaiz.net/article/39/</link>
      <pubDate>Fri, 02 Dec 2016 21:05:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/39/</guid>
      <description>PritunlでVPNサーバーを立てる。
Dockerfileはこんな感じ。
https://hub.docker.com/r/sambaiz/pritunl/
FROM mongo:3.4 # https://docs.pritunl.com/docs/installation RUN echo &#39;deb http://repo.pritunl.com/stable/apt jessie main&#39; &amp;gt; /etc/apt/sources.list.d/pritunl.list &amp;amp;&amp;amp; \ apt-key adv --keyserver hkp://keyserver.ubuntu.com --recv 7568D9BB55FF9E5287D586017AE645C0CF8E292A &amp;amp;&amp;amp; \ apt-get --assume-yes update &amp;amp;&amp;amp; \ apt-get --assume-yes upgrade &amp;amp;&amp;amp; \ apt-get --assume-yes install pritunl iptables EXPOSE 80 443 12345/udp CMD mongod --fork --logpath /data/db/mongod.log &amp;amp;&amp;amp; echo &#39;Setup Key:&#39; `pritunl setup-key` &amp;amp;&amp;amp; pritunl start $ docker run -itd -p 80:80 -p 443:443 -p 12345:12345/udp --privileged sambaiz/pritunl $ docker logs &amp;lt;id&amp;gt; .</description>
    </item>
    
    <item>
      <title>PackerでAMIを作る</title>
      <link>https://www.sambaiz.net/article/24/</link>
      <pubDate>Tue, 18 Oct 2016 22:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/24/</guid>
      <description>https://www.packer.io/
いろんなプラットフォームのイメージを作ることができるツール。 これでfluentdのログサーバーのAMIを作る。
$ brew install packer # mac $ packer -v 0.10.1 設定ファイルはこんな感じ。variablesの値は{{user ... }}のところで使われる。 buildersに作るイメージの情報を書いて、provisionersで環境を作る。
provisionersにはchefやansibleなども指定できるが、 継ぎ足し継ぎ足しで秘伝のタレ化したAMIも最初は、コマンドいくつか実行するだけなのでとりあえず手作業で作った、後でなんとかするなんてものもあったりして、 そういうものは無理にchefなどで始めず、手軽にshellでpacker buildするといいと思う。 手作業よりも楽だしソースが別にあるので使われていないAMIを消すのも簡単。
fileではpermissionがないところに置くことができないので、一旦置いてshellで移動する。
{ &amp;quot;variables&amp;quot;: { &amp;quot;aws_access_key&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;aws_secret_key&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `aws_access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `aws_secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;ap-northeast-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-1a15c77b&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.small&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ec2-user&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;fluentd-logserver {{timestamp}}&amp;quot; }], &amp;quot;provisioners&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;file&amp;quot;, &amp;quot;source&amp;quot;: &amp;quot;td-agent.conf&amp;quot;, &amp;quot;destination&amp;quot;: &amp;quot;/home/ec2-user/td-agent.conf&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;, &amp;quot;inline&amp;quot;: [ &amp;quot;curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent2.sh | sh&amp;quot;, &amp;quot;sudo mv /home/ec2-user/td-agent.</description>
    </item>
    
  </channel>
</rss>
