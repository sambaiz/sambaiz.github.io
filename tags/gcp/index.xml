<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/gcp/index.xml</link>
    <language>ja</language>
    <author>sambaiz</author>
    <rights>(C) 2018</rights>
    <updated>0001-01-01 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Kubernetes,Helmで負荷試験ツールLocustを立てる</title>
          <link>https://www.sambaiz.net/article/161/</link>
          <pubDate>Sun, 18 Mar 2018 22:35:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/161/</guid>
          <description>&lt;p&gt;OSSの負荷試験ツール&lt;a href=&#34;https://locust.io/&#34;&gt;Locust&lt;/a&gt;をK8sクラスタに立てる。
K8sならworkerの増減も簡単だし、HelmのChartもあるので立てるのも楽。&lt;/p&gt;

&lt;p&gt;LocustはPython製で、以下のような&lt;a href=&#34;https://docs.locust.io/en/latest/writing-a-locustfile.html&#34;&gt;コード&lt;/a&gt;で処理を書くことができる。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;@task(10)&lt;/code&gt;のように括弧の中に数字を書いて実行される割合を偏らせることもできる。
ユーザー数はあとでWeb上から入力する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ mkdir tasks
$ cat tasks/tasks.py
from locust import HttpLocust, TaskSet, task

class ElbTasks(TaskSet):
  @task
  def task1(self):
    with client.get(&amp;quot;/&amp;quot;, catch_response=True) as response:
    if response.content != &amp;quot;Success&amp;quot;:
        response.failure(&amp;quot;Got wrong response&amp;quot;)

class ElbWarmer(HttpLocust):
  task_set = ElbTasks
  min_wait = 1000
  max_wait = 3000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;stableに&lt;a href=&#34;https://github.com/kubernetes/charts/tree/master/stable/locust&#34;&gt;Chart&lt;/a&gt;はあるが、今のところtasksの中を書き換えなくてはいけないようなので、forkしてきてtasksの中を書き換え、&lt;code&gt;helm repo add&lt;/code&gt;するために&lt;code&gt;package&lt;/code&gt;して、これを参照する&lt;code&gt;index.yaml&lt;/code&gt;を生成した。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm package .
$ helm repo index .
$ ls locust-0.1.2.tgz index.yaml 
index.yaml		locust-0.1.2.tgz
$ cat index.yaml
apiVersion: v1
entries:
  locust:
    ...
    urls:
    - locust-0.1.2.tgz
    version: 0.1.2
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ helm repo add my-locust &#39;https://raw.githubusercontent.com/sambaiz/charts/my-locust/stable/locust&#39;
$ helm repo list
NAME     	URL                                                                     
stable   	https://kubernetes-charts.storage.googleapis.com
local    	http://127.0.0.1:8879/charts    
my-locust	https://raw.githubusercontent.com/sambaiz/charts/my-locust/stable/locust
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GKEのようなRBACが有効な環境でHelmを使う場合、TillerにServiceAccountを設定しておく。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/160/&#34;&gt;RBACが有効なGKEでHelmを使う - sambaiz-net&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;addしたrepoからinstallする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install -n my-locust --set master.config.target-host=**** my-locust/locust 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;masterとworker(デフォルトで2つ)が立った。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl get pods
NAME                                READY     STATUS    RESTARTS   AGE
my-locust-master-7f7d57f5dc-k966s   1/1       Running   0          1m
my-locust-worker-66cc964748-7ff5w   1/1       Running   0          1m
my-locust-worker-66cc964748-wx5hf   1/1       Running   0          1m
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;port-forwardしてweb-uiにアクセスしてみる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl port-forward $POD_NAME 8089
$ open localhost:8089
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/161.png&#34; alt=&#34;User数と生成速度の設定&#34; /&gt;&lt;/p&gt;

&lt;p&gt;実行開始するとRPSなどがリアルタイムで表示される。
1000Users、50spawned/sで始めてみたところ、200Userにも到達せずにリクエスト自体が止まってしまった。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/161-2.png&#34; alt=&#34;実行結果&#34; /&gt;&lt;/p&gt;

&lt;p&gt;少なく再設定してもUsersはそのままでよく分からない。
一旦立ち上げ直して100,10で実行してみたら100Userまで行くことは確認できた。&lt;/p&gt;

&lt;p&gt;再び止まるまで上げてみたところ、途中でOOMになっていることが分かった。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl describe pod my-locust-worker-****
Containers:
  locust:
    ...
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    137
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;CPU(割り当て100&lt;a href=&#34;https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#meaning-of-cpu&#34;&gt;millicores&lt;/a&gt;)もメモリ(割り当て128mi)もそれなりに使われていて、単純にreplica数を増やす必要がある。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ watch kubectl top node
NAME                                CPU(cores)   MEMORY(bytes)
my-locust-master-7f7d57f5dc-h9mvt   1m           19Mi
my-locust-worker-66cc964748-9dp8n   72m          112Mi
my-locust-worker-66cc964748-h94v9   61m          114Mi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ということでworkerを2から5に増やしてみたところUser数を増やしても止まらなくなった。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale deployment my-locust-worker --replicas=5
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>RBACが有効なGKEでHelmを使う</title>
          <link>https://www.sambaiz.net/article/160/</link>
          <pubDate>Sun, 18 Mar 2018 01:04:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/160/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/122/&#34;&gt;k8sのパッケージマネージャーHelmを使う - sambaiz-net&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm version
Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GKEで&lt;code&gt;helm init&lt;/code&gt;して&lt;code&gt;helm install&lt;/code&gt;したところ以下のエラーが返ってきた。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error: release my-locust failed: namespaces &amp;quot;default&amp;quot; is forbidden: User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot get namespaces in the namespace &amp;quot;default&amp;quot;: Unknown user &amp;quot;system:serviceaccount:kube-system:default&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;GKEではデフォルトでK8sのRBAC(Role-Based Access Control)が&lt;a href=&#34;https://cloud.google.com/kubernetes-engine/docs/role-based-access-control&#34;&gt;有効になっている&lt;/a&gt;ため、&lt;a href=&#34;https://github.com/kubernetes/helm/blob/master/docs/rbac.md&#34;&gt;Tillerインスタンスに権限を与える&lt;/a&gt;必要がある。&lt;/p&gt;

&lt;p&gt;ということでTiller用にnamespaceを切って、その中では好きにできる&lt;a href=&#34;https://kubernetes.io/docs/admin/authorization/rbac/#role-and-clusterrole&#34;&gt;Role&lt;/a&gt;と、Tillerが使う&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/&#34;&gt;ServiceAccount&lt;/a&gt;を作成し、RoleBindingで紐づける。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tiller-manager
  namespace: tiller-world
rules:
- apiGroups: [&amp;quot;&amp;quot;, &amp;quot;extensions&amp;quot;, &amp;quot;apps&amp;quot;]
  resources: [&amp;quot;*&amp;quot;]
  verbs: [&amp;quot;*&amp;quot;]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: tiller
  namespace: tiller-world
---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tiller-binding
  namespace: tiller-world
subjects:
- kind: ServiceAccount
  name: tiller
  namespace: tiller-world
roleRef:
  kind: Role
  name: tiller-manager
  apiGroup: rbac.authorization.k8s.io
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Roleを追加するため、自分自身にsuper-user相当の&lt;code&gt;cluster-admin&lt;/code&gt;roleをbindする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=&amp;lt;email&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create namespace tiller-world
$ kubectl create -f tiller-rbac.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;helm init&lt;/code&gt;時に作ったServiceAccountを渡す。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm init --tiller-namespace tiller-world --service-account tiller
$ kubectl get deployment -n tiller-world
NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
tiller-deploy   1         1         1            1           1m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Prometheusをinstallしてみる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ helm install --name my-prometheus stable/prometheus --tiller-namespace tiller-world --namespace tiller-world
$ kubectl get pods -n tiller-world
NAME                                                           READY     STATUS    RESTARTS   AGE
my-prometheus-prometheus-alertmanager-85f75879db-9r9z2         2/2       Running   0          1m
my-prometheus-prometheus-kube-state-metrics-65dfc57897-95ts7   1/1       Running   0          1m
my-prometheus-prometheus-server-7c98fb8ffb-s7jpq               2/2       Running   0          1m
tiller-deploy-57dbcb6bbc-6srlt                                 1/1       Running   0          2m
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る</title>
          <link>https://www.sambaiz.net/article/159/</link>
          <pubDate>Tue, 13 Mar 2018 01:04:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/159/</guid>
          <description>

&lt;p&gt;Logging AgentをNodeレベルの&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&#34;&gt;DaemonSet&lt;/a&gt;として動かすのではなく、Podの中に&lt;a href=&#34;https://kubernetes.io/docs/concepts/cluster-administration/logging/#using-a-sidecar-container-with-the-logging-agent&#34;&gt;Sidecar Container&lt;/a&gt;として動かす。その分リソースは食うけど、独自の設定で動かせる。&lt;/p&gt;

&lt;h2 id=&#34;アプリケーション&#34;&gt;アプリケーション&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/sambaiz/go-logging-sample&#34;&gt;https://github.com/sambaiz/go-logging-sample&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Goで定期的にログを出すサンプルコードを書いたのでこれを使う。
&lt;a href=&#34;https://github.com/spf13/viper&#34;&gt;viper&lt;/a&gt;で設定を持ち、
&lt;a href=&#34;https://github.com/uber-go/zap&#34;&gt;zap&lt;/a&gt;でログを出力する。
あとSIGINTを拾ってSync()してGraceful Shutdownするようにしている。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/104/&#34;&gt;Golangの高速なロガーzapとlumberjackでログを出力してrotateさせる - sambaiz-net&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://docs.docker.com/develop/develop-images/multistage-build/#name-your-build-stages&#34;&gt;multistage-build&lt;/a&gt;でビルドして、GKEで動かすのでContainer Registryに上げる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t go-logging-sample .
$ docker tag go-logging-sample gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample:v1 
$ gcloud docker -- push gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;fluentdの設定&#34;&gt;Fluentdの設定&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/kaizenplatform/fluent-plugin-bigquery&#34;&gt;fluent-plugin-bigquery&lt;/a&gt;プラグインを使う。&lt;/p&gt;

&lt;p&gt;projectとdataset、パーティションの&lt;a href=&#34;https://cloud.google.com/bigquery/docs/creating-partitioned-tables&#34;&gt;日付分割テーブル&lt;/a&gt;に入れる場合は、auto_create_table&lt;a href=&#34;https://github.com/kaizenplatform/fluent-plugin-bigquery#date-partitioned-table-support&#34;&gt;できない&lt;/a&gt;のでtableも作成しておく。&lt;/p&gt;

&lt;p&gt;fluentdの設定は&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/&#34;&gt;ConfigMap&lt;/a&gt;で持つ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluent.conf: |
    &amp;lt;source&amp;gt;
      @type tail
      format json
      path /var/log/app.log
      pos_file /var/log/app.log.pos
      tag bigquery 
    &amp;lt;/source&amp;gt;

    &amp;lt;match bigquery&amp;gt;
      @type bigquery

      method load

      &amp;lt;buffer time&amp;gt;
        @type file
        path /var/log/bigquery.*.buffer
        timekey 1d
        flush_at_shutdown true
      &amp;lt;/buffer&amp;gt;

      auth_method	compute_engine

      project &amp;lt;project-name&amp;gt;
      dataset &amp;lt;dataset-name&amp;gt;
      table &amp;lt;table-name&amp;gt;$%Y%m%d
      fetch_schema true 
      ignore_unknown_values true	  
    &amp;lt;/match&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;プラグイン入りのfluentdイメージもビルドして上げる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM fluent/fluentd:v1.1-onbuild

RUN apk add --update --virtual .build-deps \
        sudo build-base ruby-dev \

 # cutomize following instruction as you wish
 &amp;amp;&amp;amp; sudo gem install \
        bigdecimal fluent-plugin-bigquery:1.2.0 \

 &amp;amp;&amp;amp; sudo gem sources --clear-all \
 &amp;amp;&amp;amp; apk del .build-deps \
 &amp;amp;&amp;amp; rm -rf /var/cache/apk/* \
           /home/fluent/.gem/ruby/*/cache/*.gem

EXPOSE 24284
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ touch fluent.conf
$ mkdir plugins
$ docker build -t fluentd-bigquery .
$ docker tag fluentd-bigquery gcr.io/&amp;lt;project_id&amp;gt;/fluentd-bigquery:v1 
$ gcloud docker -- push gcr.io/&amp;lt;project_id&amp;gt;/fluentd-bigquery
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;podの設定&#34;&gt;Podの設定&lt;/h2&gt;

&lt;p&gt;ConfigMapの&lt;a href=&#34;https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#add-configmap-data-to-a-volume&#34;&gt;Volume&lt;/a&gt;と、ログが書かれるVolumeを定義し、
これを各コンテナでマウントする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
  name: something-collector
spec:
  containers:
  - name: collector
    image: gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample:v1
    env:
    - name: SAMPLE_LOG_PATH
      value: /var/log/app.log
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: fluentd
    image: gcr.io/&amp;lt;project_id&amp;gt;/fluentd-bigquery:v1
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: config-volume
      mountPath: /fluentd/etc
  volumes:
  - name: varlog
    emptyDir: {}
  - name: config-volume
    configMap:
      name: fluentd-config
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;動かす&#34;&gt;動かす&lt;/h2&gt;

&lt;p&gt;BigQueryの権限が付いたGKEのクラスタにデプロイ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gcloud container clusters get-credentials &amp;lt;cluster-name&amp;gt; --zone &amp;lt;zone&amp;gt; --project &amp;lt;project-name&amp;gt;
$ kubectl config current-context
gke_***

$ kubectl create -f fluentd-configmap.yaml
$ kubectl create -f something-collector-pod.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;立ち上がらないときは&lt;code&gt;-c&lt;/code&gt;でコンテナのnameを指定してログを見る。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl logs something-collector -c fluentd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OptionのUse Legacy SQLのチェックを外して&lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/standard-sql/migrating-from-legacy-sql&#34;&gt;標準SQL&lt;/a&gt;でBigQueryに入っていることを確認した。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT
  *
FROM
  `&amp;lt;dataset-name&amp;gt;.&amp;lt;table-name&amp;gt;` 
WHERE
  _PARTITIONTIME BETWEEN 
  TIMESTAMP_TRUNC(TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 * 24 HOUR),DAY)
  AND 
  TIMESTAMP_TRUNC(CURRENT_TIMESTAMP(),DAY);
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>fluentdのmonitor_agentのデータをGoでGoogle Stackdriverに送って監視する</title>
          <link>https://www.sambaiz.net/article/66/</link>
          <pubDate>Sun, 19 Feb 2017 23:55:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/66/</guid>
          <description>

&lt;h2 id=&#34;fluentdのmonitor-agent-http-docs-fluentd-org-v0-12-articles-monitoring&#34;&gt;&lt;a href=&#34;http://docs.fluentd.org/v0.12/articles/monitoring&#34;&gt;fluentdのmonitor_agent&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;メトリクスをjsonで返すAPIを提供する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;source&amp;gt;
  @type monitor_agent
  bind 0.0.0.0
  port 24220
&amp;lt;/source&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ curl localhost:24220/api/plugins.json | jq
{
  &amp;quot;plugins&amp;quot;: [
    {
      &amp;quot;plugin_id&amp;quot;: &amp;quot;object:3f8590d8c250&amp;quot;,
      &amp;quot;plugin_category&amp;quot;: &amp;quot;input&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;forward&amp;quot;,
      &amp;quot;config&amp;quot;: {
        &amp;quot;@type&amp;quot;: &amp;quot;forward&amp;quot;,
        &amp;quot;port&amp;quot;: &amp;quot;24222&amp;quot;,
        &amp;quot;bind&amp;quot;: &amp;quot;0.0.0.0&amp;quot;
      },
      &amp;quot;output_plugin&amp;quot;: false,
      &amp;quot;retry_count&amp;quot;: null
    },
    {
      &amp;quot;plugin_id&amp;quot;: &amp;quot;object:3f8590d894c4&amp;quot;,
      &amp;quot;plugin_category&amp;quot;: &amp;quot;input&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;monitor_agent&amp;quot;,
      &amp;quot;config&amp;quot;: {
        &amp;quot;@type&amp;quot;: &amp;quot;monitor_agent&amp;quot;,
        &amp;quot;bind&amp;quot;: &amp;quot;0.0.0.0&amp;quot;,
        &amp;quot;port&amp;quot;: &amp;quot;24220&amp;quot;
      },
      &amp;quot;output_plugin&amp;quot;: false,
      &amp;quot;retry_count&amp;quot;: null
    },
    {
      &amp;quot;plugin_id&amp;quot;: &amp;quot;object:3f8590dc1f2c&amp;quot;,
      &amp;quot;plugin_category&amp;quot;: &amp;quot;output&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;file&amp;quot;,
      &amp;quot;config&amp;quot;: {
        &amp;quot;@type&amp;quot;: &amp;quot;file&amp;quot;,
        &amp;quot;path&amp;quot;: &amp;quot;/var/log/td-agent/hoge.log&amp;quot;,
        &amp;quot;buffer_path&amp;quot;: &amp;quot;/var/log/td-agent/hoge.log.*&amp;quot;
      },
      &amp;quot;output_plugin&amp;quot;: true,
      &amp;quot;buffer_queue_length&amp;quot;: 0,
      &amp;quot;buffer_total_queued_size&amp;quot;: 0,
      &amp;quot;retry_count&amp;quot;: 0
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これをもとにStackdriverで異常を検知できるようにする。&lt;/p&gt;

&lt;h2 id=&#34;google-stackdriver-https-cloud-google-com-stackdriver&#34;&gt;&lt;a href=&#34;https://cloud.google.com/stackdriver/&#34;&gt;Google Stackdriver&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;GoogleがStackdriverを買収して改造したもの。GCPだけではなくAWSのリソースも監視できる。
まだBeta。&lt;/p&gt;

&lt;h2 id=&#34;ec2インスタンスを監視する-https-cloud-google-com-monitoring-quickstart-aws-configure-sd-acct&#34;&gt;&lt;a href=&#34;https://cloud.google.com/monitoring/quickstart-aws#configure-sd-acct&#34;&gt;EC2インスタンスを監視する&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;GCPのメニューのSTACKDRIVER -&amp;gt; モニタリングで、プロジェクトを指定してStackdriverアカウントを作成する。&lt;/p&gt;

&lt;p&gt;今回はEC2で動いているfluentdを監視するので指示に従ってクロスアカウントアクセスのロールを作成、
Role ARNを入力してAWSアカウントと接続すると、
StackdriverのResouces-&amp;gt;InstancesでCPUの使用率などは確認できるが、
EC2にAgentを入れると詳細な情報を取得できる。&lt;/p&gt;

&lt;p&gt;GCPのメニューのサービスアカウントから接続したAWSアカウントを選択し、
Project-&amp;gt;編集者とLogging-&amp;gt;ログ書き込みロールのサービスアカウントを作成する。
新しい秘密鍵の提供にチェックを入れて、JSONのキーをダウンロードする。
これをEC2の&lt;code&gt;/etc/google/auth/application_default_credentials.json&lt;/code&gt;に置いて
&lt;code&gt;chown root:root&lt;/code&gt;、&lt;code&gt;chmod 400&lt;/code&gt;する。&lt;/p&gt;

&lt;p&gt;Monitoring AgentとLogging Agentをインストールし、
&lt;code&gt;stackdriver-collectd&lt;/code&gt;と&lt;code&gt;google-fluentd&lt;/code&gt;のプロセスがあれば正常。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -O https://repo.stackdriver.com/stack-install.sh
sudo bash stack-install.sh --write-gcm

curl -sSO https://dl.google.com/cloudagents/install-logging-agent.sh
sudo bash install-logging-agent.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;メモリの使用量やTCPコネクション数などがとれていることを確認する。
Googleのドキュメントには見つからなかったけど、
旧Stackdriverと同様、&lt;code&gt;stackdriver_monitor: false&lt;/code&gt;のタグを付けると
&lt;a href=&#34;https://support.stackdriver.com/customer/portal/articles/1491785-collecting-data-from-specific-resources-only&#34;&gt;監視対象から外れる&lt;/a&gt;
っぽい。&lt;/p&gt;

&lt;h2 id=&#34;カスタムメトリクスを送る-https-cloud-google-com-monitoring-custom-metrics&#34;&gt;&lt;a href=&#34;https://cloud.google.com/monitoring/custom-metrics/&#34;&gt;カスタムメトリクスを送る&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;MetricDescriptorを作成し、これにTimeSeriesデータを書き込んでいく。&lt;/p&gt;

&lt;h3 id=&#34;metricdescriptorの作成-https-cloud-google-com-monitoring-custom-metrics-creating-metrics-monitoring-create-metric-protocol&#34;&gt;&lt;a href=&#34;https://cloud.google.com/monitoring/custom-metrics/creating-metrics#monitoring-create-metric-protocol&#34;&gt;MetricDescriptorの作成&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.metricDescriptors&#34;&gt;MetricDescriptor&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;typeは&lt;code&gt;custom.googleapis.com/&lt;/code&gt;
から&lt;a href=&#34;https://cloud.google.com/monitoring/custom-metrics/creating-metrics#custom_metric_names&#34;&gt;始める&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/projects.metricDescriptors#MetricKind&#34;&gt;metricKind&lt;/a&gt;
にはGAUGEのほかに変化量をとるDELTA、累積するCUMULATIVEを指定できる。&lt;/p&gt;

&lt;p&gt;labelはフィルタリングのためのもの。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;name&amp;quot;: &amp;quot;&amp;quot;,
  &amp;quot;description&amp;quot;: &amp;quot;fluentd buffer_queue_length&amp;quot;,
  &amp;quot;displayName&amp;quot;: &amp;quot;fluentd-buffer_queue_length&amp;quot;,
  &amp;quot;type&amp;quot;: &amp;quot;custom.googleapis.com/fluentd/buffer_queue_length&amp;quot;,
  &amp;quot;metricKind&amp;quot;: &amp;quot;GAUGE&amp;quot;,
  &amp;quot;valueType&amp;quot;: &amp;quot;INT64&amp;quot;,
  &amp;quot;labels&amp;quot;: [
    {
      &amp;quot;key&amp;quot;: &amp;quot;plugin_type&amp;quot;,
      &amp;quot;valueType&amp;quot;: &amp;quot;STRING&amp;quot;,
      &amp;quot;description&amp;quot;: &amp;quot;The type of the plugin&amp;quot;
    },
  ],
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これをGoで登録する。&lt;/p&gt;

&lt;p&gt;gcpのほうのprojectでProject-&amp;gt;編集者のサービスアカウントを作成してパスを
環境変数&lt;code&gt;GOOGLE_APPLICATION_CREDENTIALS&lt;/code&gt;に入れて
&lt;a href=&#34;https://developers.google.com/identity/protocols/application-default-credentials&#34;&gt;Default Credential&lt;/a&gt;
にする。&lt;/p&gt;

&lt;p&gt;必要なパッケージをgo get。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go get google.golang.org/api/monitoring/v3
$ go get golang.org/x/oauth2/google
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;func main() {
	ctx := context.Background()
	httpClient, err := google.DefaultClient(ctx, monitoring.CloudPlatformScope)
	if err != nil {
		panic(err)
	}
	client, err := monitoring.New(httpClient)
	if err != nil {
		panic(err)
	}

	var (
		// The project on which to execute the request. The format is `&amp;quot;projects/{project_id_or_number}&amp;quot;`.
		name = &amp;quot;projects/*****&amp;quot;

		requestBody = &amp;amp;monitoring.MetricDescriptor{
			Description: &amp;quot;fluentd buffer_queue_length&amp;quot;,
			DisplayName: &amp;quot;fluentd-buffer_queue_length&amp;quot;,
			Type:        &amp;quot;custom.googleapis.com/fluentd/buffer_queue_length&amp;quot;,
			MetricKind:  &amp;quot;GAUGE&amp;quot;,
			ValueType:   &amp;quot;INT64&amp;quot;,
			Labels: []*monitoring.LabelDescriptor{
				&amp;amp;monitoring.LabelDescriptor{
					Key:         &amp;quot;plugin_type&amp;quot;,
					ValueType:   &amp;quot;STRING&amp;quot;,
					Description: &amp;quot;The type of the plugin&amp;quot;,
				},
			},
		}
	)

	response, err := client.Projects.MetricDescriptors.Create(name, requestBody).Context(ctx).Do()
	if err != nil {
		panic(err)
	}

	fmt.Println(&amp;quot;done&amp;quot;)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;登録されたことをlistで確認する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;response, err := client.Projects.MetricDescriptors.List(name).Context(ctx).Do()
if err != nil {
  panic(err)
}

for _, v := range response.MetricDescriptors {
  fmt.Println(v.DisplayName)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;API Request Count
Agent Memory Usage
Stream Space Used
...
fluentd-buffer_queue_length
...
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;timeseriesの書き込み-https-cloud-google-com-monitoring-custom-metrics-creating-metrics-monitoring-write-timeseries-protocol&#34;&gt;&lt;a href=&#34;https://cloud.google.com/monitoring/custom-metrics/creating-metrics#monitoring-write-timeseries-protocol&#34;&gt;TimeSeriesの書き込み&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/monitoring/api/ref_v3/rest/v3/TimeSeries&#34;&gt;TimeSeries&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;metricのtypeはMetricDescriptorのtypeと対応する。
pointsのendTimeはRFC3339のUTC文字列で渡す。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
 &amp;quot;timeSeries&amp;quot;: [
  {
   &amp;quot;metric&amp;quot;: {
    &amp;quot;type&amp;quot;: &amp;quot;custom.googleapis.com/fluentd/buffer_queue_length&amp;quot;,
    &amp;quot;labels&amp;quot;: {
     &amp;quot;plugin_type&amp;quot;: &amp;quot;file&amp;quot;
    }
   },
   &amp;quot;resource&amp;quot;: {
    &amp;quot;type&amp;quot;: &amp;quot;aws_ec2_instance&amp;quot;,
    &amp;quot;labels&amp;quot;: {
     &amp;quot;project_id&amp;quot;: &amp;quot;*****&amp;quot;,
     &amp;quot;instance_id&amp;quot;: &amp;quot;*****&amp;quot;,
     &amp;quot;region&amp;quot;: &amp;quot;aws:ap-northeast-1&amp;quot;,
     &amp;quot;aws_account&amp;quot;: &amp;quot;*****&amp;quot;
    }
   },
   &amp;quot;points&amp;quot;: [
    {
     &amp;quot;interval&amp;quot;: {
      &amp;quot;endTime&amp;quot;: &amp;quot;2016-06-01T10:00:00-04:00&amp;quot;
     },
     &amp;quot;value&amp;quot;: {
      &amp;quot;int64Value&amp;quot;: 0
     }
    }
   ]
  }
 ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;resourceのtypeは
&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/monitoredResourceDescriptors/list#MonitoredResourceDescriptor&#34;&gt;MonitoredResourceDescriptor&lt;/a&gt;
と対応していて、
&lt;a href=&#34;https://cloud.google.com/logging/docs/reference/v2/rest/v2/monitoredResourceDescriptors/list&#34;&gt;list&lt;/a&gt;で確認できる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
 &amp;quot;resourceDescriptors&amp;quot;: [
   {
   &amp;quot;type&amp;quot;: &amp;quot;aws_ec2_instance&amp;quot;,
   &amp;quot;displayName&amp;quot;: &amp;quot;Amazon EC2 Instance&amp;quot;,
   &amp;quot;description&amp;quot;: &amp;quot;A VM instance in Amazon EC2.&amp;quot;,
   &amp;quot;labels&amp;quot;: [
    {
     &amp;quot;key&amp;quot;: &amp;quot;project_id&amp;quot;,
     &amp;quot;description&amp;quot;: &amp;quot;The identifier of the GCP project under which data is stored for the AWS account specified in the aws_account label (e.g., my-project).&amp;quot;
    },
    {
     &amp;quot;key&amp;quot;: &amp;quot;instance_id&amp;quot;,
     &amp;quot;description&amp;quot;: &amp;quot;The VM instance identifier assigned by AWS.&amp;quot;
    },
    {
     &amp;quot;key&amp;quot;: &amp;quot;region&amp;quot;,
     &amp;quot;description&amp;quot;: &amp;quot;The AWS region in which the VM is running. Supported AWS region values are listed by service at http://docs.aws.amazon.com/general/latest/gr/rande.html. The value supplied for this label must be prefixed with &#39;aws:&#39; (for example, &#39;aws:us-east-1&#39; is a valid value while &#39;us-east-1&#39; is not).&amp;quot;
    },
    {
     &amp;quot;key&amp;quot;: &amp;quot;aws_account&amp;quot;,
     &amp;quot;description&amp;quot;: &amp;quot;The AWS account number under which the VM is running.&amp;quot;
    }
   ]
  },
  ...
 ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;書くコード。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func writeFluentdBufferQueueLength() error {
	ctx := context.Background()
	httpClient, err := google.DefaultClient(ctx, monitoring.CloudPlatformScope)
	if err != nil {
		return err
	}
	client, err := monitoring.New(httpClient)
	if err != nil {
		return err
	}

	now := time.Now().UTC().Format(time.RFC3339)

	resource := &amp;amp;monitoring.MonitoredResource{
		Type: &amp;quot;aws_ec2_instance&amp;quot;,
		Labels: map[string]string{
			&amp;quot;project_id&amp;quot;:  &amp;quot;*****&amp;quot;,
			&amp;quot;instance_id&amp;quot;: &amp;quot;*****&amp;quot;,
			&amp;quot;region&amp;quot;:      &amp;quot;aws:ap-northeast-1&amp;quot;,
			&amp;quot;aws_account&amp;quot;: &amp;quot;*****&amp;quot;,
		},
	}

	metrics, err := fetchFluentdMetrics()
	if err != nil {
		return err
	}

	timeSeries := []*monitoring.TimeSeries{}

	for _, v := range metrics.Plugins {
		if v.OutputPlugin {

			fmt.Printf(&amp;quot;send %s\n&amp;quot;, v.Type)

			timeSeries = append(
				timeSeries,
				&amp;amp;monitoring.TimeSeries{
					Metric: &amp;amp;monitoring.Metric{
						Type: &amp;quot;custom.googleapis.com/fluentd/buffer_queue_length&amp;quot;,
						Labels: map[string]string{
							&amp;quot;plugin_type&amp;quot;: v.Type,
						},
					},
					Resource: resource,
					Points: []*monitoring.Point{
						&amp;amp;monitoring.Point{
							Interval: &amp;amp;monitoring.TimeInterval{
								EndTime: now,
							},
							Value: &amp;amp;monitoring.TypedValue{
								Int64Value: int64p(v.BufferQueueLength),
							},
						},
					},
				},
			)
		}
	}

	var (
		// The project on which to execute the request. The format is `&amp;quot;projects/{project_id_or_number}&amp;quot;`.
		name = &amp;quot;projects/try-stackdriver-159110&amp;quot;

		requestBody = &amp;amp;monitoring.CreateTimeSeriesRequest{
			TimeSeries: timeSeries,
		}
	)

	_, err = client.Projects.TimeSeries.Create(name, requestBody).Context(ctx).Do()
	if err != nil {
		return err
	}

	fmt.Println(&amp;quot;done&amp;quot;)

	return nil
}

const fluentdMonitorEndpoint = &amp;quot;http://localhost:24220/api/plugins.json&amp;quot;

type fluentdMetrics struct {
	Plugins []fluentdMetricsPlugin `json:&amp;quot;plugins&amp;quot;`
}
type fluentdMetricsPlugin struct {
	Type              string `json:&amp;quot;type&amp;quot;`
	OutputPlugin      bool   `json:&amp;quot;output_plugin&amp;quot;`
	BufferQueueLength int64  `json:&amp;quot;buffer_queue_length&amp;quot;`
}

// monitor_agentからfluentdのメトリクスを取得する
func fetchFluentdMetrics() (*fluentdMetrics, error) {

	resp, err := http.Get(fluentdMonitorEndpoint)
	if err != nil {
		return nil, err
	}

	body, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}

	var ret fluentdMetrics

	if err := json.Unmarshal(body, &amp;amp;ret); err != nil {
		return nil, err
	}

	return &amp;amp;ret, nil
}

// int64 -&amp;gt; *int64
func int64p(n int64) *int64 {
	return &amp;amp;n
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを&lt;a href=&#34;https://github.com/jasonlvhit/gocron&#34;&gt;gocron&lt;/a&gt;などで定期的に実行させる。&lt;/p&gt;

&lt;p&gt;読むコード。確認用。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func readFluentdBufferQueueLength() error {
	ctx := context.Background()
	httpClient, err := google.DefaultClient(ctx, monitoring.CloudPlatformScope)
	if err != nil {
		return err
	}
	client, err := monitoring.New(httpClient)
	if err != nil {
		return err
	}

	var (
		// The project on which to execute the request. The format is `&amp;quot;projects/{project_id_or_number}&amp;quot;`.
		name = &amp;quot;projects/*****&amp;quot;
	)

	start := time.Now().Add(time.Hour * -3).UTC().Format(time.RFC3339)
	now := time.Now().UTC().Format(time.RFC3339)

	filter := &amp;quot;metric.type = \&amp;quot;custom.googleapis.com/fluentd/buffer_queue_length\&amp;quot;&amp;quot;
	resp, err := client.Projects.TimeSeries.List(name).
		IntervalStartTime(start).
		IntervalEndTime(now).
		Filter(filter).Context(ctx).Do()
	if err != nil {
		return err
	}

	for _, v := range resp.TimeSeries {
		fmt.Println(v.Metric.Type)
		for _, p := range v.Points {
			fmt.Println(*(p.Value.Int64Value))
		}
	}

	return nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ちゃんと届いていれば
Resource-&amp;gt;Metrics Explorerでもcustom/fluentd/buffer_queue_lengthを確認できる。&lt;/p&gt;

&lt;p&gt;これでAlertを設定できるようになった。TargetのResource TypeはCustom Metrics。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/66.png&#34; alt=&#34;Alertの設定&#34; /&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>gcloudのアカウント切り替えとkubectlのcontext変更</title>
          <link>https://www.sambaiz.net/article/28/</link>
          <pubDate>Tue, 25 Oct 2016 20:29:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/28/</guid>
          <description>

&lt;p&gt;いつも迷うのでまとめた。&lt;/p&gt;

&lt;h2 id=&#34;gcloudのアカウント一覧と切り替え&#34;&gt;gcloudのアカウント一覧と切り替え&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ gcloud auth list
$ gcloud config set account `ACCOUNT`
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;kubectlのcontext変更&#34;&gt;kubectlのcontext変更&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl config current-context
$ kubectl config view # contexts
$ kubectl config use-context minikube
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>GKEで複数コンテナのアプリケーションを動かす</title>
          <link>https://www.sambaiz.net/article/18/</link>
          <pubDate>Fri, 26 Aug 2016 21:57:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/18/</guid>
          <description>&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/17&#34;&gt;前回&lt;/a&gt;は単一コンテナのアプリケーションを動かしたが、今回はコンテナ間でやり取りが発生するものを動かす。
流れとしては、クライアントからのリクエストを&lt;code&gt;GATEWAY&lt;/code&gt;サーバーで受け取り、&lt;code&gt;SERVICE&lt;/code&gt;サーバーにリクエストし、その結果を返すまで。&lt;/p&gt;

&lt;p&gt;プログラムは以下の通り、環境変数&lt;code&gt;TYPE&lt;/code&gt;の値によって挙動を変えていて、同じイメージを使い回す。コードは&lt;a href=&#34;https://github.com/sambaiz/gke-multi-container-app&#34;&gt;ここ&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var http = require(&#39;http&#39;);
var handleRequest = function(request, response) {
  if(process.env.TYPE == &amp;quot;GATEWAY&amp;quot;){
    console.log(&#39;Passed.&#39;);
    var options = {
      host: &#39;service&#39;,
      port: 8080,
      method: &#39;GET&#39;
    };
    var req = http.request(options, function(res) {
      data = &amp;quot;&amp;quot;
      res.on(&#39;data&#39;, function (chunk) {
        data+=chunk;
      });

      res.on(&#39;end&#39;, () =&amp;gt; {
        response.writeHead(200);
        response.end(data);
      });
    });
    req.on(&#39;error&#39;, function(e) {
      response.writeHead(500)
      response.end(e.message);
    });
    req.end();
  }else{
    console.log(&#39;Received.&#39;);
    response.writeHead(200);
    response.end(&#39;ok&#39;);
  }
};
var www = http.createServer(handleRequest);
www.listen(8080);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これをContainer RegistryにPushする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ export PROJECT_ID=&amp;quot;gcp-test-141011&amp;quot;
$ docker build -t gcr.io/$PROJECT_ID/multitest:v1 .
$ gcloud docker push gcr.io/$PROJECT_ID/multitest:v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ローカルで試すためにminikubeを起動。コンテキストがminikubeに設定される。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ minikube start
$ kubectl config current-context
minikube
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;minikubeでContainer Registryから取得できるようにsecretを作成する。
ローカルでビルドしたimageを使うこともできる。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/151/&#34;&gt;ローカルでビルドしたimageをminikubeで使う - sambaiz-net&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create secret docker-registry myregistrykey --docker-server=https://gcr.io --docker-username=oauth2accesstoken --docker-password=&amp;quot;$(gcloud auth print-access-token)&amp;quot; --docker-email=***
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;まず、以下の&lt;code&gt;service_deployment.yaml&lt;/code&gt;でDeploymentを作成する。
envのところで環境変数&lt;code&gt;TYPE&lt;/code&gt;を&lt;code&gt;SERVICE&lt;/code&gt;に設定した。
また、先ほど作成したsecretを&lt;code&gt;imagePullSecrets&lt;/code&gt;で指定し、
ローカルのminikube環境でContainer Registryから取得できるようにしている。
同様に&lt;code&gt;gateway_deployment.yaml&lt;/code&gt;でも作成する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: service
  labels:
    app: service
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: service
    spec:
      containers:
      - name: service
        image: gcr.io/gcp-test-141011/multitest:v1
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: TYPE
          value: SERVICE
        ports:
        - containerPort: 8080
      imagePullSecrets:
      - name: myregistrykey    
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f service_deployment.yaml
$ kubectl create -f gateway_deployment.yaml
$ kubectl get deployment
NAME      DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
gateway   1         1         1            1           8m
service   1         1         1            1           28m
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;次に、これを内部から呼べるようにするためのServiceを作成する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: service
  labels:
    app: service
spec:
  ports:
  - port: 8080
  selector:
    app: service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;GATEWAY&lt;/code&gt;の方は外に開くので、EXTERNAL IPが割り当てられるようにする。
ただ、minikubeでは&lt;code&gt;type: LoadBalancer&lt;/code&gt;に対応していないので代わりに&lt;code&gt;type: NodePort&lt;/code&gt;を指定する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
  name: gateway
  labels:
    app: gateway
spec:
  # type: LoadBalancer
  type: NodePort
  ports:
  - port: 8080
    nodePort: 30002
  selector:
    app: gateway
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl create -f service_service.yaml
$ kubectl create -f gateway_service.yaml
$ kubectl get services
NAME            CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE
gateway         10.0.0.176   &amp;lt;nodes&amp;gt;       8080/TCP   1h
service         10.0.0.111   &amp;lt;none&amp;gt;        8080/TCP   2d
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;GATEWAY&lt;/code&gt;サーバーを越えて&lt;code&gt;SERVICE&lt;/code&gt;サーバーまで到達したことが確認できた。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl 192.168.99.100:30002
ok

$ kubectl get pods
NAME                       READY     STATUS    RESTARTS   AGE
gateway-3043515563-a0tb5   1/1       Running   0          38m
service-2727435432-9glvb   1/1       Running   0          38m

$ kubectl logs service-2727435432-9glvb
Received.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;一通り動いたのでこんなシェルスクリプトを書いた。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash

context=`kubectl config current-context`
echo &amp;quot;${context}で実行します。よろしいですか[Y/N]&amp;quot;
read ANSWER
case $ANSWER in
    &amp;quot;Y&amp;quot; ) :;;
    * ) exit;;
esac
if [ $context = &amp;quot;minikube&amp;quot; ] ; then
  kubectl create -f yaml/gateway_service_minikube.yaml
  kubectl create -f yaml/service_deployment_minikube.yaml
  kubectl create -f yaml/gateway_deployment_minikube.yaml
else
  kubectl create -f yaml/gateway_service.yaml
  kubectl create -f yaml/service_deployment.yaml
  kubectl create -f yaml/gateway_deployment.yaml
fi
kubectl create -f yaml/service_service.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを実行するとこうなる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gcloud container clusters create multi-container-app
$ kubectl config current-context
gke_gcp-test-141011_asia-east1-b_multi-container-app

$ sh create.sh
gke_gcp-test-141011_asia-east1-b_multi-container-appで実行します。よろしいですか[Y/N]
Y
service &amp;quot;gateway&amp;quot; created
deployment &amp;quot;service&amp;quot; created
deployment &amp;quot;gateway&amp;quot; created
service &amp;quot;service&amp;quot; created

$ kubectl get service
NAME         CLUSTER-IP      EXTERNAL-IP       PORT(S)    AGE
gateway      10.83.254.5     104.199.206.147   8080/TCP   4m
service      10.83.255.118   &amp;lt;none&amp;gt;            8080/TCP   4m

$ curl 104.199.206.147:8080
ok
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Google Container Engine(GKE)で単一コンテナのアプリケーションを動かす</title>
          <link>https://www.sambaiz.net/article/17/</link>
          <pubDate>Sun, 21 Aug 2016 23:37:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/17/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;http://kubernetes.io/docs/hellonode/&#34;&gt;Kubernetes - Hello World Walkthrough&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;cloudsdkとkubectlのインストール&#34;&gt;CloudSDKとkubectlのインストール&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/sdk/#Quick_Start&#34;&gt;Cloud SDKをインストール&lt;/a&gt;して&lt;code&gt;gloud&lt;/code&gt;コマンドを使えるようにする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ gcloud --version
Google Cloud SDK 122.0.0
$ gcloud components install kubectl
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;google-container-registryにpush&#34;&gt;Google Container RegistryにPush&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ export PROJECT_ID=&amp;quot;******&amp;quot;
$ docker build -t gcr.io/$PROJECT_ID/test:v1 .
$ gcloud docker push gcr.io/$PROJECT_ID/test:v1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;プロジェクトの課金を有効にしていないとこんなエラーメッセージが出る。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;denied: Unable to create the repository, please check that you have access to do so.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;clusterの作成&#34;&gt;Clusterの作成&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ gcloud config set core/project $PROJECT_ID
$ gcloud config set compute/zone asia-east1-b
$ gcloud container clusters create test-cluster
$ gcloud config set container/cluster test-cluster
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Container Engine APIが有効になっていない場合はこうなる。
一度コンソールからContainer Engineを選ぶと、サービスの準備が始まって有効になる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ERROR: (gcloud.container.clusters.create) ResponseError: code=503, message=Project **** is not fully initialized with the default service accounts. Please try again later.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;pod-とそのdeployment-の作成&#34;&gt;Pod(とそのDeployment)の作成&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl run test-node --image=gcr.io/$PROJECT_ID/test:v1 --port=8080
$ kubectl get deployments
NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
test-node   1         1         1            0           12s

$ kubectl get pods
NAME                         READY     STATUS              RESTARTS   AGE
test-node-1016577872-h7yiz   0/1       ContainerCreating   0          18s
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;serviceを作成してクラスタの外からアクセスできるようにする&#34;&gt;Serviceを作成してクラスタの外からアクセスできるようにする&lt;/h2&gt;

&lt;p&gt;デフォルトではクラスタ内からしかアクセスできない。
&lt;code&gt;--type=&amp;quot;LoadBalancer&amp;quot;&lt;/code&gt;のServiceを作成するとEXTERNAL_IPが割り当てられ、外からアクセスできるようになる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl expose deployment test-node --type=&amp;quot;LoadBalancer&amp;quot;
$ kubectl get services test-node
NAME        CLUSTER-IP     EXTERNAL-IP       PORT(S)    AGE
test-node   10.43.247.66   104.199.158.131   8080/TCP   1m
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;スケーリング&#34;&gt;スケーリング&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl scale deployment test-node --replicas=4
$ kubectl get deployment
NAME        DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
test-node   4         4         4            2           11m

$ kubectl get pods
NAME                         READY     STATUS              RESTARTS   AGE
test-node-1016577872-fso09   0/1       ContainerCreating   0          12s
test-node-1016577872-h7yiz   1/1       Running             0          11m
test-node-1016577872-sbdvl   1/1       Running             0          12s
test-node-1016577872-z9ji3   0/1       ContainerCreating   0          12s
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;更新&#34;&gt;更新&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t gcr.io/$PROJECT_ID/test:v2 .
$ gcloud docker push gcr.io/$PROJECT_ID/test:v2
$ kubectl set image deployment/test-node test-node=gcr.io/$PROJECT_ID/test:v2
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;削除&#34;&gt;削除&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ kubectl delete service,deployment test-node
$ gcloud container clusters delete test-cluster
$ gcloud config unset container/cluster
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    

  </channel>
</rss>
