<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gcp on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/gcp/</link>
    <description>Recent content in gcp on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Tue, 13 Jul 2021 21:02:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/gcp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GlueのカスタムコネクタでBigQueryに接続する</title>
      <link>https://www.sambaiz.net/article/372/</link>
      <pubDate>Tue, 13 Jul 2021 21:02:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/372/</guid>
      <description>GlueはConnectorによって様々なデータソースに対応していて、RDSやMongoDBなど標準で提供されているもの以外にも カスタムコネクタを用いることで接続できる。今回はMarketplaceで提供されているBigQueryのカスタムコネクタを用いてBigQueryのテーブルの内容をS3に出力するJobを作成する。
GlueのETL JobをGUIで構築したりモニタリングできるサービス、Glue StudioからMarketplaceに飛んで AWS Glue Connector for Google BigQueryをSubscribeする。
BigQuery ConnectorをactivateしConnectionを作成する。
Studioでない方のGlueのConnectionからも確認できる。
JobのRoleには 117940112483.dkr.ecr.us-east-1.amazonaws.com/818e4ebf-997f-4d87-beb3-e0196e500474/cg-1025003233/bigquery-spark-connector:2.12.0-latest をpullするための GetAuthorizationToken と GCPのcredentialをSecretsManagerから持ってくるための GetSecretValue が必要。 SecretsManagerにそのままcredentialのjsonを入れて実行したところ次のエラーが出た。正しくはcredentialsというキーにbase64エンコードしたjsonの値を入れる。
IllegalArgumentException: &amp;#39;A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.&amp;#39; また、Apache Spark SQL connector for Google BigQueryが Storage Read APIを呼ぶので bigquery.readsessions.* の権限が必要。
JobのConnection Options に parentProject としてGCPのProjectID、table としてBQのテーブル名を入れると次のようなスクリプトが生成され、 実行するとS3にBQのデータが保存される。DataFrameに変換してクエリを実行することもできる。DynamicFrameは現状overwriteできないので次のスクリプトは冪等性を持たない。</description>
    </item>
    
    <item>
      <title>ウェブアプリとしてデプロイしたGASをブラウザからAPIとして呼ぶ際のCORSエラー</title>
      <link>https://www.sambaiz.net/article/319/</link>
      <pubDate>Wed, 23 Dec 2020 08:41:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/319/</guid>
      <description>GASでAPIを公開する方法として、scripts.run APIで実行できる実行可能APIと、ウェブアプリがある。 前者は認証が走り実行者の権限で動くが、後者は認証を行わずデプロイユーザーの権限で動かすこともできるのでパブリックなAPIとして使うことができる。 当然、不正な操作が行われないように注意する必要があり、GASのQuotaやhard limitも気にする必要がある。無料で運用することができるが、レイテンシやエラーハンドリング、監視などの上で本番環境で動かすイメージはあまり湧いていない。
ウェブアプリではdoGet(e)とdoPost(e)を実装することでそれぞれのメソッドのリクエストをハンドリングできる。これ以外のメソッドには対応していない。
function doGet(e) { return ContentService.createTextOutput(JSON.stringify(e.parameter)) } function doPost(e) { return ContentService.createTextOutput(e.postData.contents) } claspでデプロイする際は一度画面上でウェブアプリとして公開した後、そのDeplymentsを更新するとそのまま反映できる。ただしDeploymentsに紐づくVersionに限りがあるので一旦undeployしている。
claspでGoogle Apps Scriptをローカルで開発しデプロイする - sambaiz-net
$ clasp push &amp;amp;&amp;amp; clasp undeploy --all &amp;amp;&amp;amp; clasp deploy -V 1 ウェブアプリのURLにリクエストを送ると https://script.googleusercontent.com/macros/echo?user_content_key=**** にリダイレクトするので、curlでリクエストする際は-Lフラグを付ける必要がある。 また、POSTする際は-X POSTを付けるとステータスコードに関わらずリダイレクト先にPOSTでリクエストし、Not Foundとなってしまうので付けない。
$ curl -L https://script.google.com/macros/s/*****/exec?aaa=bbb {&amp;#34;aaa&amp;#34;:&amp;#34;bbb&amp;#34;} $ $ curl -d &amp;#39;aaaa&amp;#39; -L https://script.google.com/macros/s/*****/exec &amp;#34;aaaa&amp;#34; GETとPOSTしか対応してないということは、OPTIONSメソッドによるCORSのpreflightリクエストも送れないということになる。 したがって、preflightリクエストが送られないように、余分なHeaderを付けずに Content-Typeをapplication/x-www-form-urlencoded、multipart/form-data、text/plainのいずれかで送る必要がある。 Request.modeをno-corsにすると送られるHeaderが制限されてエラーにはならなくなるが、レスポンスをスクリプトから取ることができない。
$ cat test.html &amp;lt;script&amp;gt; (async () =&amp;gt; { const resp = await fetch(&amp;#39;https://script.</description>
    </item>
    
    <item>
      <title>claspでGoogle Apps Scriptをローカルで開発しデプロイする</title>
      <link>https://www.sambaiz.net/article/315/</link>
      <pubDate>Sun, 29 Nov 2020 14:42:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/315/</guid>
      <description>claspはGASをローカルで開発するための公式のツール。
$ npm install -g @google/clasp $ clasp --version 2.3.0 設定でGASのAPIを有効にしてログインし、必要な権限を許可する。
$ clasp login $ cat ~/.clasprc.json {&amp;#34;token&amp;#34;:{&amp;#34;access_token&amp;#34;:&amp;#34;... clasp createでプロジェクトを作成する。typeを指定するとドキュメントが作られ、スクリプトがそれに紐づく。 紐づいているスクリプトからはSpreadsheetApp.getActiveSpreadsheet()のようにIDを指定することなくドキュメントを参照できる。
$ clasp create --type sheets --title &amp;#34;clasp test&amp;#34; Created new Google Sheet: https://drive.google.com/open?id=***** Created new Google Sheets Add-on script: https://script.google.com/d/*****/edit Warning: files in subfolder are not accounted for unless you set a &amp;#39;.claspignore&amp;#39; file. Cloned 1 file. └─ appsscript.json $ tree -a . ├── .clasp.json └── appsscript.json $ cat .</description>
    </item>
    
    <item>
      <title>GoのSheets API v4クライアントでSpreadsheetを読み書きする</title>
      <link>https://www.sambaiz.net/article/312/</link>
      <pubDate>Mon, 23 Nov 2020 22:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/312/</guid>
      <description>まずGCPのコンソールでプロジェクトを作成するか選んでGoogle Sheets APIを有効にする。 料金はかからないが、デフォルトのQuotaが、100秒ごとに100リクエスト/ユーザー、500リクエスト/プロジェクトとなっている。
OAuth 2.0で得たユーザーの権限でAPIを呼ぶこともできるが、今回はサービスアカウントを用いる。ロールは付与する必要はなく、対象のSheetの共有先に追加すればよい。 JWTの署名に用いる秘密鍵をダウンロードしておく。
OpenID ConnectのIDトークンの内容と検証 - sambaiz-net
全体のコードはGitHubにある。
クライアントの準備 サービスアカウントでアクセストークンを取得するにはJWTを認可サーバーに送る必要があるが、JWTをアクセストークンの代わりに用いることができるのでそうしている。
SpreadsheetIDはURLから得られる。
package main import ( &amp;#34;context&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;io/ioutil&amp;#34; &amp;#34;os&amp;#34; &amp;#34;golang.org/x/oauth2/google&amp;#34; &amp;#34;google.golang.org/api/sheets/v4&amp;#34; ) type SheetClient struct { srv *sheets.Service spreadsheetID string } func NewSheetClient(ctx context.Context, spreadsheetID string) (*SheetClient, error) { b, err := ioutil.ReadFile(&amp;#34;secret.json&amp;#34;) if err != nil { return nil, err } // read &amp;amp; write permission 	jwt, err := google.JWTConfigFromJSON(b, &amp;#34;https://www.googleapis.com/auth/spreadsheets&amp;#34;) if err != nil { return nil, err } srv, err := sheets.</description>
    </item>
    
    <item>
      <title>IstioをHelmでインストールしてRoutingとTelemetryを行いJaeger/Kialiで確認する</title>
      <link>https://www.sambaiz.net/article/185/</link>
      <pubDate>Sun, 02 Sep 2018 23:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/185/</guid>
      <description>IstioはEnvoyというProxyをSidecarとしてPodに入れてトラフィックを通すことでマイクロサービスのRoutingやTelemetryをサービスのコードに手を入れずに行うことができるサービスメッシュ。 もともとEnvoy自体は単体で、コネクションを張りっぱなしのgRPC(HTTP/2)をK8sのServiceのL4ロードバランサーでは分散できない問題の解決方法の一つとして 各PodのIPの一覧を返すHeadless Serviceと使われていたが、各Manifestに入れたりConfigMapを編集したりする必要があり少し面倒だった。 Istioにするとそれらが省けて、さらに賢いRoutingやモニタリングの仕組みまで付いてくるのでとても便利だ。
インストール IstioをダウンロードしてきてHelmでインストールする。Istioには様々なコンポーネントが含まれているが、パラメータでインストールするものを選択することができる。
KubernetesのパッケージマネージャーHelmを使う - sambaiz-net
今回はデフォルトではfalseになっているGrafana/Jaeger/Kialiをtrueにしてほぼ全て入るようにしている。
RBACが有効な場合はServiceAccountを作ってcluster-adminあるいは必要なRoleをBindしておく。
RBACが有効なGKEでHelmを使う - sambaiz-net
$ curl -L https://git.io/getLatestIstio | sh - $ cd istio-1.0.1/ # helm init --service-account tiller $ helm install install/kubernetes/helm/istio --name istio --namespace istio-system --set grafana.enabled=true --set grafana.persist=true --set grafana.storageClassName=standard --set tracing.enabled=true --set kiali.enabled=true $ kubectl get pod -n istio-system NAME READY STATUS RESTARTS AGE grafana-598678cbb-bglbq 1/1 Running 0 3m istio-citadel-6f9887d776-tvdg8 1/1 Running 0 3m istio-egressgateway-84d78d84bf-zpxrq 1/1 Running 0 3m istio-galley-556f5558f5-hk2r8 1/1 Running 0 3m istio-ingressgateway-78cccbddbb-gh2xl 1/1 Running 0 3m istio-pilot-799845f56d-l777d 2/2 Running 0 3m istio-policy-7666fcd574-nbx8s 2/2 Running 0 3m istio-sidecar-injector-7b6589c9-m7x77 1/1 Running 0 3m istio-statsd-prom-bridge-55965ff9c8-s6dmj 1/1 Running 0 3m istio-telemetry-844c8d6bff-9trcf 2/2 Running 0 3m istio-tracing-77f9f94b98-g7v6f 1/1 Running 0 3m kiali-bdf7fdc78-9lpd4 1/1 Running 0 3m prometheus-7456f56c96-drhlq 1/1 Running 0 3m default namespaceにラベルを貼って自動でEnvoyが各PodにInjectionされるようにする。</description>
    </item>
    
    <item>
      <title>CircleCI 2.0でDocker imageをbuildしてタグを付けてContainer Registryに上げる</title>
      <link>https://www.sambaiz.net/article/183/</link>
      <pubDate>Wed, 22 Aug 2018 23:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/183/</guid>
      <description>(追記: 2019-04-13) 2.1からのOrbを使うと自分でjobを書かなくてもよくなる CircleCI 2.1からのOrbでdocker buildしてECRにpushし、Slackに通知させる - sambaiz-net
 masterにpushしたときと、リリースタグを切ったときにビルドされるようにする。
version: 2 jobs: build: docker: - image: google/cloud-sdk environment: GCP_PROJECT: &amp;lt;project_name&amp;gt; IMAGE_NAME: &amp;lt;image_name&amp;gt; steps: - checkout - setup_remote_docker: version: 18.05.0-ce - run: name: gcloud auth command: |echo $GCLOUD_SERVICE_KEY | base64 --decode &amp;gt; ${HOME}/gcloud-service-key.json gcloud auth activate-service-account --key-file ${HOME}/gcloud-service-key.json gcloud --quiet auth configure-docker - run: name: docker build &amp;amp; push command: |docker build -t asia.gcr.io/${GCP_PROJECT}/${IMAGE_NAME}:${CIRCLE_BUILD_NUM} . docker tag asia.gcr.io/${GCP_PROJECT}/${IMAGE_NAME}:${CIRCLE_BUILD_NUM} asia.gcr.io/${GCP_PROJECT}/${IMAGE_NAME}:latest if [ -n &amp;#34;${CIRCLE_TAG}&amp;#34; ]; then docker tag asia.</description>
    </item>
    
    <item>
      <title>KubernetesのCustom Resource Definition(CRD)とCustom Controller</title>
      <link>https://www.sambaiz.net/article/182/</link>
      <pubDate>Thu, 09 Aug 2018 23:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/182/</guid>
      <description>K8sではDeploymentを作成したときにReplicaSetも作成されるようにしたり、 Load Balancer Serviceを作成したときにGCPなどその環境に応じたLoad Balancerも作成されるようにしたりするため、Controllerがそれらを監視してAPIを呼んでいる。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
Controllerは単なるAPIを呼ぶアプリケーションなので自分でCustom Controllerを作成してDeploymentとしてデプロイすることもできる。 また、監視する対象もpodsやdeploymentsといった標準のAPIだけではなく、 Custom Resource で拡張したものを使うことができる。
特定のアプリケーションのためのControllerはOperatorとも呼ばれる。
CustomResourceDefinition(CRD) Custom Resourceを定義する。
apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: # REST APIで使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt; group: stable.example.com version: v1 # Namespaced か Cluster scope: Namespaced names: # 複数形 URLに使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;/&amp;lt;plural&amp;gt; plural: crontabs # 単数形 CLIなどで使われる singular: crontab # manifestで使う kind: CronTab shortNames: - ct $ kubectl create -f crd.yaml $ kubectl get crd NAME AGE crontabs.</description>
    </item>
    
    <item>
      <title>KubernetesのNetworkPolicy Resource</title>
      <link>https://www.sambaiz.net/article/181/</link>
      <pubDate>Mon, 30 Jul 2018 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/181/</guid>
      <description>Network Policies - Kubernetes
PodのトラフィックをラベルやIPアドレスで許可するためのResource。AWSのセキュリティグループやGCPのファイアウォールルールのようなもの。 GKEでは今のところデフォルトでオフになっているので--enable-network-policyを付けてクラスタを作成する必要がある。
以前作成したmulti podのアプリケーションで挙動を確認する。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
$ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE clusterip-app ClusterIP 10.23.247.54 &amp;lt;none&amp;gt; 80/TCP 48m loadbalancer-app LoadBalancer 10.23.244.137 35.224.130.196 80:31508/TCP 48m nodeport-app NodePort 10.23.246.215 &amp;lt;none&amp;gt; 80:32181/TCP 48m ... $ curl -d &amp;#39;{&amp;#34;url&amp;#34;: &amp;#34;http://nodeport-app&amp;#34;}&amp;#39; http://35.224.130.196/ 200 作成するNetworkPolicyは以下の二つで、いずれも対象はapp: nodeport-appのラベルが付いたPod。 一つ目は対象Podへのリクエストを一旦全て拒否する。 二つ目はnodeport-access: &amp;quot;true&amp;quot;のラベルが付いたPodから対象Podへの8080ポートのリクエストを許可するもの。 今回は設定しないがegressも設定できる。
$ cat networkPolicies.yaml --- # Default deny all ingress traffic apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: matchLabels: app: nodeport-app --- apiVersion: networking.</description>
    </item>
    
    <item>
      <title>GCPのCloud Pub/Sub</title>
      <link>https://www.sambaiz.net/article/180/</link>
      <pubDate>Thu, 26 Jul 2018 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/180/</guid>
      <description>スケーラビリティに優れるメッセージングミドルウェア。 データはPullするだけではなくhttpsのエンドポイントにPushすることもでき、Cloud Dataflowを通してBigQueryやCloud MLに繋げることもできる。GAEのTaskQueueのように遅延させる機能は今のところない。
GAEのTaskQueue - sambaiz-net
料金はPublish/Pull/Pushしたデータ容量による。1TB送ると$60くらい。
Goのクライアントライブラリで動かしてみる。 まずTopicを作成して50件Publishした後、Subsriptionを作成して、再び50件Publishする。 Publishできるデータは10MB未満。
topic, err := client.CreateTopic(ctx, topicName) if err != nil { panic(err) } var wg sync.WaitGroup for i := 0; i &amp;lt; 50; i++ { wg.Add(1) go func() { if _, err := topic.Publish(ctx, &amp;amp;pubsub.Message{Data: []byte(strconv.Itoa(i))}).Get(ctx); err != nil { log.Fatalf(&amp;#34;Publish error: %s&amp;#34;, err.Error()) } else { log.Printf(&amp;#34;Publish successful: %d&amp;#34;, i) } wg.Done() }() wg.Wait() } log.Printf(&amp;#34;Create Subscription&amp;#34;) sub := createSubscription(ctx, client, topic, subscriptionName) for i := 50; i &amp;lt; 100; i++ { wg.</description>
    </item>
    
    <item>
      <title>GAEのTaskQueue</title>
      <link>https://www.sambaiz.net/article/178/</link>
      <pubDate>Sun, 15 Jul 2018 16:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/178/</guid>
      <description>GCPのマネージドなQueueサービスとしてGAEのTaskQueueがあることを教えてもらったので動かしてみる。 PushQueueとPullQueueがあって、それぞれおおよそAWSのSNSとSQSに相当する。PushQueueの場合はHTTPのリクエストとしてGAEのサービスに投げられる。PullQueueはCloud Tasks APIを使えばGAE外からも使えるらしいがまだalpha。
設定ファイルqueue.yamlはこんな感じ。bucket_sizeは最大同時実行数で空いていたらrateで埋められていく。
queue: - name: default rate: 10/m bucket_size: 5 retry_parameters: min_backoff_seconds: 10 max_backoff_seconds: 300 bucket_sizeの最大は500なのでこれ以上の性能が必要な場合は複数のQueueに分けるか Cloud Pub/Subを使うことになる。ただし、At-Least-Onceなのでレコードが重複しても問題ないように作る必要がある。SQSも同じ。
GCPのCloud Pub/Sub - sambaiz-net
アプリケーション /にアクセスすると2つのTaskをdefaultのTaskQueueにDelay25秒でPOSTする。 Taskによるリクエストは/workerで受け、30%の確率で500エラーを返すようにしている。
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;io/ioutil&amp;#34; &amp;#34;math/rand&amp;#34; &amp;#34;net/http&amp;#34; &amp;#34;net/url&amp;#34; &amp;#34;strconv&amp;#34; &amp;#34;time&amp;#34; &amp;#34;google.golang.org/appengine&amp;#34; &amp;#34;google.golang.org/appengine/log&amp;#34; &amp;#34;google.golang.org/appengine/taskqueue&amp;#34; ) func main() { http.HandleFunc(&amp;#34;/&amp;#34;, handler) http.HandleFunc(&amp;#34;/worker&amp;#34;, handlerQueue) appengine.Main() } func handler(w http.ResponseWriter, r *http.Request) { ctx := appengine.NewContext(r) // POST body: name=a%26&amp;amp;value=20 	t := taskqueue.NewPOSTTask(&amp;#34;/worker&amp;#34;, map[string][]string{&amp;#34;name&amp;#34;: {&amp;#34;a&amp;amp;&amp;#34;}, &amp;#34;time&amp;#34;: {strconv.</description>
    </item>
    
    <item>
      <title>GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress</title>
      <link>https://www.sambaiz.net/article/173/</link>
      <pubDate>Sat, 23 Jun 2018 15:02:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/173/</guid>
      <description>疎通確認用アプリケーション GETでは200を返し、POSTではURLにGETリクエストを送ってステータスコードを返す。
package main import ( &amp;#34;encoding/json&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;io/ioutil&amp;#34; &amp;#34;net/http&amp;#34; ) type PostBody struct { URL string `json:&amp;#34;url&amp;#34;` } func handler(w http.ResponseWriter, r *http.Request) { if r.Method == http.MethodGet { fmt.Fprintln(w, &amp;#34;ok&amp;#34;) } else if r.Method == http.MethodPost { data, err := ioutil.ReadAll(r.Body) if err != nil { w.WriteHeader(http.StatusInternalServerError) fmt.Fprintln(w, err.Error()) return } p := PostBody{} if err := json.Unmarshal(data, &amp;amp;p); err != nil { w.WriteHeader(http.StatusBadRequest) fmt.Fprintln(w, err.Error()) return } resp, err := http.</description>
    </item>
    
    <item>
      <title>TerraformでGKEクラスタとBigQueryを立てる</title>
      <link>https://www.sambaiz.net/article/165/</link>
      <pubDate>Tue, 29 May 2018 02:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/165/</guid>
      <description>GKEクラスタからBigQueryを読み書きすることを想定している。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る - sambaiz-net
GKE  google_container_cluster  oauth_scopeにbigqueryを付けている。
resource &amp;#34;google_container_cluster&amp;#34; &amp;#34;sample&amp;#34; { name = &amp;#34;${var.cluster_name}&amp;#34; description = &amp;#34;sample k8s cluster&amp;#34; zone = &amp;#34;${var.gcp_zone}&amp;#34; initial_node_count = &amp;#34;${var.initial_node_count}&amp;#34; master_auth { username = &amp;#34;${var.master_username}&amp;#34; password = &amp;#34;${var.master_password}&amp;#34; } node_config { machine_type = &amp;#34;${var.node_machine_type}&amp;#34; disk_size_gb = &amp;#34;${var.node_disk_size}&amp;#34; oauth_scopes = [ &amp;#34;https://www.googleapis.com/auth/compute&amp;#34;, &amp;#34;https://www.googleapis.com/auth/devstorage.read_only&amp;#34;, &amp;#34;https://www.googleapis.com/auth/logging.write&amp;#34;, &amp;#34;https://www.googleapis.com/auth/monitoring&amp;#34;, &amp;#34;https://www.googleapis.com/auth/bigquery&amp;#34;, ] } } variable &amp;#34;env&amp;#34; { description = &amp;#34;system env&amp;#34; } variable &amp;#34;gcp_zone&amp;#34; { description = &amp;#34;GCP zone, e.</description>
    </item>
    
    <item>
      <title>Macでの開発環境構築メモ</title>
      <link>https://www.sambaiz.net/article/163/</link>
      <pubDate>Sat, 14 Apr 2018 14:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/163/</guid>
      <description>新しいMBPを買ったので開発環境の構築でやったことを残しておく
設定  アクセシビリティから3本指スクロールを有効にする ホットコーナーの左上にLaunchPad、右上にデスクトップを割り当てている 画面をなるべく広く使うためにDockは左に置いて自動的に隠す  bash_profile パッケージマネージャ以外で持ってきたバイナリは${HOME}/binに置くことにする。
touch ~/.bash_profile mkdir ${HOME}/bin echo &amp;#34;export PATH=\$PATH:${HOME}/bin&amp;#34; &amp;gt;&amp;gt; ~/.bash_profile HomeBrew &amp;amp; Cask /usr/bin/ruby -e &amp;#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;#34; brew tap caskroom/cask 一般的なアプリケーション/コマンドのインストール XcodeとUnityとLINEは手動で入れる。
brew cask install google-chrome kap visual-studio-code slack kindle brew install jq gibo mysql wget Git git config --global user.name sambaiz git config --global user.email godgourd@gmail.com Docker &amp;amp; K8s brew cask install docker virtualbox minikube brew install docker kubernetes-helm fish bash前提で書かれたスクリプトも多いので、デフォルトシェルにはしない。</description>
    </item>
    
    <item>
      <title>KubernetesにHelmでLocustによる分散負荷試験環境を立てる</title>
      <link>https://www.sambaiz.net/article/161/</link>
      <pubDate>Sun, 18 Mar 2018 22:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/161/</guid>
      <description>OSSの負荷試験ツールLocustをK8sクラスタに立てる。 K8sならworkerの増減も簡単だし、HelmのChartもあるので立てるのも楽。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
LocustはPython製で、以下のようなコードで処理を書くことができる。
@task(10)のように括弧の中に数字を書いて実行される割合を偏らせることもできる。 異なるTaskSetに対応するユーザーを複数作ることもできて、こちらもweightで重みを付けられる。 ユーザー数はあとでWeb上から入力する。
$ mkdir tasks $ cat tasks/tasks.py from locust import HttpLocust, TaskSet, task class ElbTasks(TaskSet): @task def task1(self): with self.client.get(&amp;#34;/&amp;#34;, catch_response=True) as response: if response.content != &amp;#34;Success&amp;#34;: response.failure(&amp;#34;Got wrong response&amp;#34;) class ElbWarmer(HttpLocust): task_set = ElbTasks min_wait = 1000 max_wait = 3000 stableにChartはあるが、今のところtasksの中を書き換えなくてはいけないようなので、forkしてきてtasksの中を書き換え、package して、helm repo index でこれを参照するindex.yamlを生成した。
 追記(2020-03-11): 今はConfigmapを自分で作成し --set worker.config.configmapName=*** することでforkしなくてもよくなった kubectl create configmap locust-worker-configs --from-file tasks/tasks.py
 $ helm package .</description>
    </item>
    
    <item>
      <title>RBACが有効なGKEでHelmを使う</title>
      <link>https://www.sambaiz.net/article/160/</link>
      <pubDate>Sun, 18 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/160/</guid>
      <description>k8sのパッケージマネージャーHelmを使う - sambaiz-net
$ helm version Client: &amp;amp;version.Version{SemVer:&amp;#34;v2.8.2&amp;#34;, GitCommit:&amp;#34;a80231648a1473929271764b920a8e346f6de844&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} Server: &amp;amp;version.Version{SemVer:&amp;#34;v2.8.2&amp;#34;, GitCommit:&amp;#34;a80231648a1473929271764b920a8e346f6de844&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} GKEでhelm initしてhelm installしたところ以下のエラーが返ってきた。
Error: release my-locust failed: namespaces &amp;#34;default&amp;#34; is forbidden: User &amp;#34;system:serviceaccount:kube-system:default&amp;#34; cannot get namespaces in the namespace &amp;#34;default&amp;#34;: Unknown user &amp;#34;system:serviceaccount:kube-system:default&amp;#34; GKEではデフォルトでK8sのRBAC(Role-Based Access Control)が有効になっているため、Tillerインスタンスに権限を与える必要がある。
ということでTiller用にnamespaceを切って、その中では好きにできるRoleと、Tillerが使うServiceAccountを作成し、RoleBindingで紐づける。
kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-manager namespace: tiller-world rules: - apiGroups: [&amp;#34;&amp;#34;, &amp;#34;extensions&amp;#34;, &amp;#34;apps&amp;#34;] resources: [&amp;#34;*&amp;#34;] verbs: [&amp;#34;*&amp;#34;] --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: tiller-world --- kind: RoleBinding apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る</title>
      <link>https://www.sambaiz.net/article/159/</link>
      <pubDate>Tue, 13 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/159/</guid>
      <description>Logging AgentをNodeレベルのDaemonSetとして動かすのではなく、Podの中にSidecar Containerとして動かす。その分リソースは食うけど、独立した設定で動かせる。
アプリケーション https://github.com/sambaiz/go-logging-sample
Goで定期的にログを出すサンプルコードを書いたのでこれを使う。 viperで設定を持ち、 zapでログを出力する。 あとSIGINTを拾ってSync()してGraceful Shutdownするようにしている。
Golangの高速なロガーzapとlumberjackでログを出力してrotateさせる - sambaiz-net
multistage-buildして、GKEで動かすのでContainer Registryに上げる。
$ docker build -t go-logging-sample . $ docker tag go-logging-sample gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample:v1 $ gcloud docker -- push gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample Fluentdの設定 fluent-plugin-bigqueryプラグインを使う。
projectとdataset、パーティションの日付分割テーブルに入れる場合は、auto_create_tableできないのでtableも作成しておく。
fluentdの設定はConfigMapで持つ。
apiVersion: v1 kind: ConfigMap metadata: name: fluentd-config data: fluent.conf: |&amp;lt;source&amp;gt; @type tail format json path /var/log/app.log pos_file /var/log/app.log.pos tag bigquery &amp;lt;/source&amp;gt; &amp;lt;match bigquery&amp;gt; @type bigquery method load &amp;lt;buffer time&amp;gt; @type file path /var/log/bigquery.*.buffer timekey 1d flush_at_shutdown true &amp;lt;/buffer&amp;gt; auth_method	compute_engine project &amp;lt;project-name&amp;gt; dataset &amp;lt;dataset-name&amp;gt; table &amp;lt;table-name&amp;gt;$%Y%m%d fetch_schema true ignore_unknown_values true	&amp;lt;/match&amp;gt; プラグイン入りのfluentdイメージもビルドして上げる。</description>
    </item>
    
    <item>
      <title>fluentdのmonitor_agentのデータをGoでGoogle Stackdriverに送って監視する</title>
      <link>https://www.sambaiz.net/article/66/</link>
      <pubDate>Sun, 19 Feb 2017 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/66/</guid>
      <description>fluentdのmonitor_agent メトリクスをjsonで返すAPIを提供する。
&amp;lt;source&amp;gt; @type monitor_agent bind 0.0.0.0 port 24220 &amp;lt;/source&amp;gt; $ curl localhost:24220/api/plugins.json | jq { &amp;#34;plugins&amp;#34;: [ { &amp;#34;plugin_id&amp;#34;: &amp;#34;object:3f8590d8c250&amp;#34;, &amp;#34;plugin_category&amp;#34;: &amp;#34;input&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;forward&amp;#34;, &amp;#34;config&amp;#34;: { &amp;#34;@type&amp;#34;: &amp;#34;forward&amp;#34;, &amp;#34;port&amp;#34;: &amp;#34;24222&amp;#34;, &amp;#34;bind&amp;#34;: &amp;#34;0.0.0.0&amp;#34; }, &amp;#34;output_plugin&amp;#34;: false, &amp;#34;retry_count&amp;#34;: null }, { &amp;#34;plugin_id&amp;#34;: &amp;#34;object:3f8590d894c4&amp;#34;, &amp;#34;plugin_category&amp;#34;: &amp;#34;input&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;monitor_agent&amp;#34;, &amp;#34;config&amp;#34;: { &amp;#34;@type&amp;#34;: &amp;#34;monitor_agent&amp;#34;, &amp;#34;bind&amp;#34;: &amp;#34;0.0.0.0&amp;#34;, &amp;#34;port&amp;#34;: &amp;#34;24220&amp;#34; }, &amp;#34;output_plugin&amp;#34;: false, &amp;#34;retry_count&amp;#34;: null }, { &amp;#34;plugin_id&amp;#34;: &amp;#34;object:3f8590dc1f2c&amp;#34;, &amp;#34;plugin_category&amp;#34;: &amp;#34;output&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;file&amp;#34;, &amp;#34;config&amp;#34;: { &amp;#34;@type&amp;#34;: &amp;#34;file&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/var/log/td-agent/hoge.</description>
    </item>
    
    <item>
      <title>gcloudのアカウント切り替えとkubectlのcontext変更</title>
      <link>https://www.sambaiz.net/article/28/</link>
      <pubDate>Tue, 25 Oct 2016 20:29:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/28/</guid>
      <description>いつも迷うのでまとめた。
gcloudのアカウント一覧と切り替え $ gcloud auth list $ gcloud config set account `ACCOUNT` configにprojectなども設定している場合はconfig自体を作成して切り替えた方が楽。
$ gcloud config configurations create &amp;lt;name&amp;gt; $ gcloud config configurations activate &amp;lt;name&amp;gt; $ gcloud config list ... Your active configuration is: [&amp;lt;name&amp;gt;] $ gcloud config set account &amp;lt;accout&amp;gt; $ gcloud config set project &amp;lt;project&amp;gt; kubectlのcontext変更 $ kubectl config current-context $ kubectl config view # contexts $ kubectl config use-context minikube </description>
    </item>
    
    <item>
      <title>GKEで複数コンテナのアプリケーションを動かす</title>
      <link>https://www.sambaiz.net/article/18/</link>
      <pubDate>Fri, 26 Aug 2016 21:57:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/18/</guid>
      <description>前回は単一コンテナのアプリケーションを動かしたが、今回はコンテナ間でやり取りが発生するものを動かす。 流れとしては、クライアントからのリクエストをGATEWAYサーバーで受け取り、SERVICEサーバーにリクエストし、その結果を返すまで。
プログラムは以下の通り、環境変数TYPEの値によって挙動を変えていて、同じイメージを使い回す。コードはここ。
var http = require(&amp;#39;http&amp;#39;); var handleRequest = function(request, response) { if(process.env.TYPE == &amp;#34;GATEWAY&amp;#34;){ console.log(&amp;#39;Passed.&amp;#39;); var options = { host: &amp;#39;service&amp;#39;, port: 8080, method: &amp;#39;GET&amp;#39; }; var req = http.request(options, function(res) { data = &amp;#34;&amp;#34; res.on(&amp;#39;data&amp;#39;, function (chunk) { data+=chunk; }); res.on(&amp;#39;end&amp;#39;, () =&amp;gt; { response.writeHead(200); response.end(data); }); }); req.on(&amp;#39;error&amp;#39;, function(e) { response.writeHead(500) response.end(e.message); }); req.end(); }else{ console.log(&amp;#39;Received.&amp;#39;); response.writeHead(200); response.end(&amp;#39;ok&amp;#39;); } }; var www = http.createServer(handleRequest); www.listen(8080); これをContainer RegistryにPushする。</description>
    </item>
    
    <item>
      <title>Google Container Engine(GKE)で単一コンテナのアプリケーションを動かす</title>
      <link>https://www.sambaiz.net/article/17/</link>
      <pubDate>Sun, 21 Aug 2016 23:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/17/</guid>
      <description>Kubernetes - Hello World Walkthrough
CloudSDKとkubectlのインストール Cloud SDKをインストールしてgloudコマンドを使えるようにする。
$ gcloud --version Google Cloud SDK 122.0.0 $ gcloud components install kubectl Google Container RegistryにPush $ export PROJECT_ID=&amp;#34;******&amp;#34; $ docker build -t gcr.io/$PROJECT_ID/test:v1 . $ gcloud docker push gcr.io/$PROJECT_ID/test:v1 プロジェクトの課金を有効にしていないとこんなエラーメッセージが出る。
denied: Unable to create the repository, please check that you have access to do so. Clusterの作成 $ gcloud config set core/project $PROJECT_ID $ gcloud config set compute/zone asia-east1-b $ gcloud container clusters create test-cluster $ gcloud config set container/cluster test-cluster Container Engine APIが有効になっていない場合はこうなる。 一度コンソールからContainer Engineを選ぶと、サービスの準備が始まって有効になる。</description>
    </item>
    
  </channel>
</rss>
