<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gcp on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/gcp/</link>
    <description>Recent content in gcp on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Mon, 23 Nov 2020 22:03:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/gcp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GoのSheets API v4クライアントでSpreadsheetを読み書きする</title>
      <link>https://www.sambaiz.net/article/312/</link>
      <pubDate>Mon, 23 Nov 2020 22:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/312/</guid>
      <description>まずGCPのコンソールでプロジェクトを作成するか選んでGoogle Sheets APIを有効にする。 料金はかからないが、デフォルトのQuotaが、100秒ごとに100リクエスト/ユーザー、500リクエスト/プロジェクトとなっている。
OAuth 2.0で得たユーザーの権限でAPIを呼ぶこともできるが、今回はサービスアカウントを用いる。ロールは付与する必要はなく、対象のSheetの共有先に追加すればよい。 JWTの署名に用いる秘密鍵をダウンロードしておく。
OpenID ConnectのIDトークンの内容と検証 - sambaiz-net
全体のコードはGitHubにある。
クライアントの準備 サービスアカウントでアクセストークンを取得するにはJWTを認可サーバーに送る必要があるが、JWTをアクセストークンの代わりに用いることができるのでそうしている。
SpreadsheetIDはURLから得られる。
package main import ( &amp;quot;context&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;os&amp;quot; &amp;quot;golang.org/x/oauth2/google&amp;quot; &amp;quot;google.golang.org/api/sheets/v4&amp;quot; ) type SheetClient struct { srv *sheets.Service spreadsheetID string } func NewSheetClient(ctx context.Context, spreadsheetID string) (*SheetClient, error) { b, err := ioutil.ReadFile(&amp;quot;secret.json&amp;quot;) if err != nil { return nil, err } // read &amp;amp; write permission jwt, err := google.JWTConfigFromJSON(b, &amp;quot;https://www.googleapis.com/auth/spreadsheets&amp;quot;) if err != nil { return nil, err } srv, err := sheets.</description>
    </item>
    
    <item>
      <title>IstioをHelmでインストールしてRoutingとTelemetryを行いJaeger/Kialiで確認する</title>
      <link>https://www.sambaiz.net/article/185/</link>
      <pubDate>Sun, 02 Sep 2018 23:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/185/</guid>
      <description>IstioはEnvoyというProxyをSidecarとしてPodに入れてトラフィックを通すことでマイクロサービスのRoutingやTelemetryをサービスのコードに手を入れずに行うことができるサービスメッシュ。 もともとEnvoy自体は単体で、コネクションを張りっぱなしのgRPC(HTTP/2)をK8sのServiceのL4ロードバランサーでは分散できない問題の解決方法の一つとして 各PodのIPの一覧を返すHeadless Serviceと使われていたが、各Manifestに入れたりConfigMapを編集したりする必要があり少し面倒だった。 Istioにするとそれらが省けて、さらに賢いRoutingやモニタリングの仕組みまで付いてくるのでとても便利だ。
インストール IstioをダウンロードしてきてHelmでインストールする。Istioには様々なコンポーネントが含まれているが、パラメータでインストールするものを選択することができる。
KubernetesのパッケージマネージャーHelmを使う - sambaiz-net
今回はデフォルトではfalseになっているGrafana/Jaeger/Kialiをtrueにしてほぼ全て入るようにしている。
RBACが有効な場合はServiceAccountを作ってcluster-adminあるいは必要なRoleをBindしておく。
RBACが有効なGKEでHelmを使う - sambaiz-net
$ curl -L https://git.io/getLatestIstio | sh - $ cd istio-1.0.1/ # helm init --service-account tiller $ helm install install/kubernetes/helm/istio --name istio --namespace istio-system --set grafana.enabled=true --set grafana.persist=true --set grafana.storageClassName=standard --set tracing.enabled=true --set kiali.enabled=true $ kubectl get pod -n istio-system NAME READY STATUS RESTARTS AGE grafana-598678cbb-bglbq 1/1 Running 0 3m istio-citadel-6f9887d776-tvdg8 1/1 Running 0 3m istio-egressgateway-84d78d84bf-zpxrq 1/1 Running 0 3m istio-galley-556f5558f5-hk2r8 1/1 Running 0 3m istio-ingressgateway-78cccbddbb-gh2xl 1/1 Running 0 3m istio-pilot-799845f56d-l777d 2/2 Running 0 3m istio-policy-7666fcd574-nbx8s 2/2 Running 0 3m istio-sidecar-injector-7b6589c9-m7x77 1/1 Running 0 3m istio-statsd-prom-bridge-55965ff9c8-s6dmj 1/1 Running 0 3m istio-telemetry-844c8d6bff-9trcf 2/2 Running 0 3m istio-tracing-77f9f94b98-g7v6f 1/1 Running 0 3m kiali-bdf7fdc78-9lpd4 1/1 Running 0 3m prometheus-7456f56c96-drhlq 1/1 Running 0 3m default namespaceにラベルを貼って自動でEnvoyが各PodにInjectionされるようにする。</description>
    </item>
    
    <item>
      <title>CircleCI 2.0でDocker imageをbuildしてタグを付けてContainer Registryに上げる</title>
      <link>https://www.sambaiz.net/article/183/</link>
      <pubDate>Wed, 22 Aug 2018 23:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/183/</guid>
      <description>(追記: 2019-04-13) 2.1からのOrbを使うと自分でjobを書かなくてもよくなる CircleCI 2.1からのOrbでdocker buildしてECRにpushし、Slackに通知させる - sambaiz-net
 masterにpushしたときと、リリースタグを切ったときにビルドされるようにする。
version: 2 jobs: build: docker: - image: google/cloud-sdk environment: GCP_PROJECT: &amp;lt;project_name&amp;gt; IMAGE_NAME: &amp;lt;image_name&amp;gt; steps: - checkout - setup_remote_docker: version: 18.05.0-ce - run: name: gcloud auth command: | echo $GCLOUD_SERVICE_KEY | base64 --decode &amp;gt; ${HOME}/gcloud-service-key.json gcloud auth activate-service-account --key-file ${HOME}/gcloud-service-key.json gcloud --quiet auth configure-docker - run: name: docker build &amp;amp; push command: | docker build -t asia.gcr.io/${GCP_PROJECT}/${IMAGE_NAME}:${CIRCLE_BUILD_NUM} . docker tag asia.</description>
    </item>
    
    <item>
      <title>KubernetesのCustom Resource Definition(CRD)とCustom Controller</title>
      <link>https://www.sambaiz.net/article/182/</link>
      <pubDate>Thu, 09 Aug 2018 23:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/182/</guid>
      <description>K8sではDeploymentを作成したときにReplicaSetも作成されるようにしたり、 Load Balancer Serviceを作成したときにGCPなどその環境に応じたLoad Balancerも作成されるようにしたりするため、Controllerがそれらを監視してAPIを呼んでいる。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
Controllerは単なるAPIを呼ぶアプリケーションなので自分でCustom Controllerを作成してDeploymentとしてデプロイすることもできる。 また、監視する対象もpodsやdeploymentsといった標準のAPIだけではなく、 Custom Resource で拡張したものを使うことができる。
特定のアプリケーションのためのControllerはOperatorとも呼ばれる。
CustomResourceDefinition(CRD) Custom Resourceを定義する。
apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: # REST APIで使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt; group: stable.example.com version: v1 # Namespaced か Cluster scope: Namespaced names: # 複数形 URLに使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;/&amp;lt;plural&amp;gt; plural: crontabs # 単数形 CLIなどで使われる singular: crontab # manifestで使う kind: CronTab shortNames: - ct $ kubectl create -f crd.yaml $ kubectl get crd NAME AGE crontabs.</description>
    </item>
    
    <item>
      <title>KubernetesのNetworkPolicy Resource</title>
      <link>https://www.sambaiz.net/article/181/</link>
      <pubDate>Mon, 30 Jul 2018 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/181/</guid>
      <description>Network Policies - Kubernetes
PodのトラフィックをラベルやIPアドレスで許可するためのResource。AWSのセキュリティグループやGCPのファイアウォールルールのようなもの。 GKEでは今のところデフォルトでオフになっているので--enable-network-policyを付けてクラスタを作成する必要がある。
以前作成したmulti podのアプリケーションで挙動を確認する。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
$ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE clusterip-app ClusterIP 10.23.247.54 &amp;lt;none&amp;gt; 80/TCP 48m loadbalancer-app LoadBalancer 10.23.244.137 35.224.130.196 80:31508/TCP 48m nodeport-app NodePort 10.23.246.215 &amp;lt;none&amp;gt; 80:32181/TCP 48m ... $ curl -d &#39;{&amp;quot;url&amp;quot;: &amp;quot;http://nodeport-app&amp;quot;}&#39; http://35.224.130.196/ 200 作成するNetworkPolicyは以下の二つで、いずれも対象はapp: nodeport-appのラベルが付いたPod。 一つ目は対象Podへのリクエストを一旦全て拒否する。 二つ目はnodeport-access: &amp;quot;true&amp;quot;のラベルが付いたPodから対象Podへの8080ポートのリクエストを許可するもの。 今回は設定しないがegressも設定できる。
$ cat networkPolicies.yaml --- # Default deny all ingress traffic apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: matchLabels: app: nodeport-app --- apiVersion: networking.</description>
    </item>
    
    <item>
      <title>GCPのCloud Pub/Sub</title>
      <link>https://www.sambaiz.net/article/180/</link>
      <pubDate>Thu, 26 Jul 2018 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/180/</guid>
      <description>スケーラビリティに優れるメッセージングミドルウェア。 データはPullするだけではなくhttpsのエンドポイントにPushすることもでき、Cloud Dataflowを通してBigQueryやCloud MLに繋げることもできる。GAEのTaskQueueのように遅延させる機能は今のところない。
GAEのTaskQueue - sambaiz-net
料金はPublish/Pull/Pushしたデータ容量による。1TB送ると$60くらい。
Goのクライアントライブラリで動かしてみる。 まずTopicを作成して50件Publishした後、Subsriptionを作成して、再び50件Publishする。 Publishできるデータは10MB未満。
topic, err := client.CreateTopic(ctx, topicName) if err != nil { panic(err) } var wg sync.WaitGroup for i := 0; i &amp;lt; 50; i++ { wg.Add(1) go func() { if _, err := topic.Publish(ctx, &amp;amp;pubsub.Message{Data: []byte(strconv.Itoa(i))}).Get(ctx); err != nil { log.Fatalf(&amp;quot;Publish error: %s&amp;quot;, err.Error()) } else { log.Printf(&amp;quot;Publish successful: %d&amp;quot;, i) } wg.Done() }() wg.Wait() } log.Printf(&amp;quot;Create Subscription&amp;quot;) sub := createSubscription(ctx, client, topic, subscriptionName) for i := 50; i &amp;lt; 100; i++ { wg.</description>
    </item>
    
    <item>
      <title>GAEのTaskQueue</title>
      <link>https://www.sambaiz.net/article/178/</link>
      <pubDate>Sun, 15 Jul 2018 16:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/178/</guid>
      <description>GCPのマネージドなQueueサービスとしてGAEのTaskQueueがあることを教えてもらったので動かしてみる。 PushQueueとPullQueueがあって、それぞれおおよそAWSのSNSとSQSに相当する。PushQueueの場合はHTTPのリクエストとしてGAEのサービスに投げられる。PullQueueはCloud Tasks APIを使えばGAE外からも使えるらしいがまだalpha。
設定ファイルqueue.yamlはこんな感じ。bucket_sizeは最大同時実行数で空いていたらrateで埋められていく。
queue: - name: default rate: 10/m bucket_size: 5 retry_parameters: min_backoff_seconds: 10 max_backoff_seconds: 300 bucket_sizeの最大は500なのでこれ以上の性能が必要な場合は複数のQueueに分けるか Cloud Pub/Subを使うことになる。ただし、At-Least-Onceなのでレコードが重複しても問題ないように作る必要がある。SQSも同じ。
GCPのCloud Pub/Sub - sambaiz-net
アプリケーション /にアクセスすると2つのTaskをdefaultのTaskQueueにDelay25秒でPOSTする。 Taskによるリクエストは/workerで受け、30%の確率で500エラーを返すようにしている。
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;math/rand&amp;quot; &amp;quot;net/http&amp;quot; &amp;quot;net/url&amp;quot; &amp;quot;strconv&amp;quot; &amp;quot;time&amp;quot; &amp;quot;google.golang.org/appengine&amp;quot; &amp;quot;google.golang.org/appengine/log&amp;quot; &amp;quot;google.golang.org/appengine/taskqueue&amp;quot; ) func main() { http.HandleFunc(&amp;quot;/&amp;quot;, handler) http.HandleFunc(&amp;quot;/worker&amp;quot;, handlerQueue) appengine.Main() } func handler(w http.ResponseWriter, r *http.Request) { ctx := appengine.NewContext(r) // POST body: name=a%26&amp;amp;value=20 t := taskqueue.NewPOSTTask(&amp;quot;/worker&amp;quot;, map[string][]string{&amp;quot;name&amp;quot;: {&amp;quot;a&amp;amp;&amp;quot;}, &amp;quot;time&amp;quot;: {strconv.</description>
    </item>
    
    <item>
      <title>GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress</title>
      <link>https://www.sambaiz.net/article/173/</link>
      <pubDate>Sat, 23 Jun 2018 15:02:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/173/</guid>
      <description>疎通確認用アプリケーション GETでは200を返し、POSTではURLにGETリクエストを送ってステータスコードを返す。
package main import ( &amp;quot;encoding/json&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;net/http&amp;quot; ) type PostBody struct { URL string `json:&amp;quot;url&amp;quot;` } func handler(w http.ResponseWriter, r *http.Request) { if r.Method == http.MethodGet { fmt.Fprintln(w, &amp;quot;ok&amp;quot;) } else if r.Method == http.MethodPost { data, err := ioutil.ReadAll(r.Body) if err != nil { w.WriteHeader(http.StatusInternalServerError) fmt.Fprintln(w, err.Error()) return } p := PostBody{} if err := json.Unmarshal(data, &amp;amp;p); err != nil { w.WriteHeader(http.StatusBadRequest) fmt.Fprintln(w, err.Error()) return } resp, err := http.</description>
    </item>
    
    <item>
      <title>TerraformでGKEクラスタとBigQueryを立てる</title>
      <link>https://www.sambaiz.net/article/165/</link>
      <pubDate>Tue, 29 May 2018 02:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/165/</guid>
      <description>GKEクラスタからBigQueryを読み書きすることを想定している。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る - sambaiz-net
GKE  google_container_cluster  oauth_scopeにbigqueryを付けている。
resource &amp;quot;google_container_cluster&amp;quot; &amp;quot;sample&amp;quot; { name = &amp;quot;${var.cluster_name}&amp;quot; description = &amp;quot;sample k8s cluster&amp;quot; zone = &amp;quot;${var.gcp_zone}&amp;quot; initial_node_count = &amp;quot;${var.initial_node_count}&amp;quot; master_auth { username = &amp;quot;${var.master_username}&amp;quot; password = &amp;quot;${var.master_password}&amp;quot; } node_config { machine_type = &amp;quot;${var.node_machine_type}&amp;quot; disk_size_gb = &amp;quot;${var.node_disk_size}&amp;quot; oauth_scopes = [ &amp;quot;https://www.googleapis.com/auth/compute&amp;quot;, &amp;quot;https://www.googleapis.com/auth/devstorage.read_only&amp;quot;, &amp;quot;https://www.googleapis.com/auth/logging.write&amp;quot;, &amp;quot;https://www.googleapis.com/auth/monitoring&amp;quot;, &amp;quot;https://www.googleapis.com/auth/bigquery&amp;quot;, ] } } variable &amp;quot;env&amp;quot; { description = &amp;quot;system env&amp;quot; } variable &amp;quot;gcp_zone&amp;quot; { description = &amp;quot;GCP zone, e.</description>
    </item>
    
    <item>
      <title>Macでの開発環境構築メモ</title>
      <link>https://www.sambaiz.net/article/163/</link>
      <pubDate>Sat, 14 Apr 2018 14:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/163/</guid>
      <description>新しいMBPを買ったので開発環境の構築でやったことを残しておく
設定  アクセシビリティから3本指スクロールを有効にする ホットコーナーの左上にLaunchPad、右上にデスクトップを割り当てている 画面をなるべく広く使うためにDockは左に置いて自動的に隠す  bash_profile パッケージマネージャ以外で持ってきたバイナリは${HOME}/binに置くことにする。
touch ~/.bash_profile mkdir ${HOME}/bin echo &amp;quot;export PATH=\$PATH:${HOME}/bin&amp;quot; &amp;gt;&amp;gt; ~/.bash_profile HomeBrew &amp;amp; Cask /usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot; brew tap caskroom/cask 一般的なアプリケーション/コマンドのインストール XcodeとUnityとLINEは手動で入れる。
brew cask install google-chrome kap visual-studio-code slack kindle brew install jq gibo mysql wget Git git config --global user.name sambaiz git config --global user.email godgourd@gmail.com Docker &amp;amp; K8s brew cask install docker virtualbox minikube brew install docker kubernetes-helm fish bash前提で書かれたスクリプトも多いので、デフォルトシェルにはしない。</description>
    </item>
    
    <item>
      <title>KubernetesにHelmでLocustによる分散負荷試験環境を立てる</title>
      <link>https://www.sambaiz.net/article/161/</link>
      <pubDate>Sun, 18 Mar 2018 22:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/161/</guid>
      <description>OSSの負荷試験ツールLocustをK8sクラスタに立てる。 K8sならworkerの増減も簡単だし、HelmのChartもあるので立てるのも楽。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
LocustはPython製で、以下のようなコードで処理を書くことができる。
@task(10)のように括弧の中に数字を書いて実行される割合を偏らせることもできる。 異なるTaskSetに対応するユーザーを複数作ることもできて、こちらもweightで重みを付けられる。 ユーザー数はあとでWeb上から入力する。
$ mkdir tasks $ cat tasks/tasks.py from locust import HttpLocust, TaskSet, task class ElbTasks(TaskSet): @task def task1(self): with self.client.get(&amp;quot;/&amp;quot;, catch_response=True) as response: if response.content != &amp;quot;Success&amp;quot;: response.failure(&amp;quot;Got wrong response&amp;quot;) class ElbWarmer(HttpLocust): task_set = ElbTasks min_wait = 1000 max_wait = 3000 stableにChartはあるが、今のところtasksの中を書き換えなくてはいけないようなので、forkしてきてtasksの中を書き換え、package して、helm repo index でこれを参照するindex.yamlを生成した。
 追記(2020-03-11): 今はConfigmapを自分で作成し --set worker.config.configmapName=*** することでforkしなくてもよくなった kubectl create configmap locust-worker-configs --from-file tasks/tasks.py
 $ helm package .</description>
    </item>
    
    <item>
      <title>RBACが有効なGKEでHelmを使う</title>
      <link>https://www.sambaiz.net/article/160/</link>
      <pubDate>Sun, 18 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/160/</guid>
      <description>k8sのパッケージマネージャーHelmを使う - sambaiz-net
$ helm version Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;} Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;} GKEでhelm initしてhelm installしたところ以下のエラーが返ってきた。
Error: release my-locust failed: namespaces &amp;quot;default&amp;quot; is forbidden: User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot get namespaces in the namespace &amp;quot;default&amp;quot;: Unknown user &amp;quot;system:serviceaccount:kube-system:default&amp;quot; GKEではデフォルトでK8sのRBAC(Role-Based Access Control)が有効になっているため、Tillerインスタンスに権限を与える必要がある。
ということでTiller用にnamespaceを切って、その中では好きにできるRoleと、Tillerが使うServiceAccountを作成し、RoleBindingで紐づける。
kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-manager namespace: tiller-world rules: - apiGroups: [&amp;quot;&amp;quot;, &amp;quot;extensions&amp;quot;, &amp;quot;apps&amp;quot;] resources: [&amp;quot;*&amp;quot;] verbs: [&amp;quot;*&amp;quot;] --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: tiller-world --- kind: RoleBinding apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る</title>
      <link>https://www.sambaiz.net/article/159/</link>
      <pubDate>Tue, 13 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/159/</guid>
      <description>Logging AgentをNodeレベルのDaemonSetとして動かすのではなく、Podの中にSidecar Containerとして動かす。その分リソースは食うけど、独立した設定で動かせる。
アプリケーション https://github.com/sambaiz/go-logging-sample
Goで定期的にログを出すサンプルコードを書いたのでこれを使う。 viperで設定を持ち、 zapでログを出力する。 あとSIGINTを拾ってSync()してGraceful Shutdownするようにしている。
Golangの高速なロガーzapとlumberjackでログを出力してrotateさせる - sambaiz-net
multistage-buildして、GKEで動かすのでContainer Registryに上げる。
$ docker build -t go-logging-sample . $ docker tag go-logging-sample gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample:v1 $ gcloud docker -- push gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample Fluentdの設定 fluent-plugin-bigqueryプラグインを使う。
projectとdataset、パーティションの日付分割テーブルに入れる場合は、auto_create_tableできないのでtableも作成しておく。
fluentdの設定はConfigMapで持つ。
apiVersion: v1 kind: ConfigMap metadata: name: fluentd-config data: fluent.conf: | &amp;lt;source&amp;gt; @type tail format json path /var/log/app.log pos_file /var/log/app.log.pos tag bigquery &amp;lt;/source&amp;gt; &amp;lt;match bigquery&amp;gt; @type bigquery method load &amp;lt;buffer time&amp;gt; @type file path /var/log/bigquery.*.buffer timekey 1d flush_at_shutdown true &amp;lt;/buffer&amp;gt; auth_method	compute_engine project &amp;lt;project-name&amp;gt; dataset &amp;lt;dataset-name&amp;gt; table &amp;lt;table-name&amp;gt;$%Y%m%d fetch_schema true ignore_unknown_values true	&amp;lt;/match&amp;gt; プラグイン入りのfluentdイメージもビルドして上げる。</description>
    </item>
    
    <item>
      <title>fluentdのmonitor_agentのデータをGoでGoogle Stackdriverに送って監視する</title>
      <link>https://www.sambaiz.net/article/66/</link>
      <pubDate>Sun, 19 Feb 2017 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/66/</guid>
      <description>fluentdのmonitor_agent メトリクスをjsonで返すAPIを提供する。
&amp;lt;source&amp;gt; @type monitor_agent bind 0.0.0.0 port 24220 &amp;lt;/source&amp;gt; $ curl localhost:24220/api/plugins.json | jq { &amp;quot;plugins&amp;quot;: [ { &amp;quot;plugin_id&amp;quot;: &amp;quot;object:3f8590d8c250&amp;quot;, &amp;quot;plugin_category&amp;quot;: &amp;quot;input&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;forward&amp;quot;, &amp;quot;config&amp;quot;: { &amp;quot;@type&amp;quot;: &amp;quot;forward&amp;quot;, &amp;quot;port&amp;quot;: &amp;quot;24222&amp;quot;, &amp;quot;bind&amp;quot;: &amp;quot;0.0.0.0&amp;quot; }, &amp;quot;output_plugin&amp;quot;: false, &amp;quot;retry_count&amp;quot;: null }, { &amp;quot;plugin_id&amp;quot;: &amp;quot;object:3f8590d894c4&amp;quot;, &amp;quot;plugin_category&amp;quot;: &amp;quot;input&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;monitor_agent&amp;quot;, &amp;quot;config&amp;quot;: { &amp;quot;@type&amp;quot;: &amp;quot;monitor_agent&amp;quot;, &amp;quot;bind&amp;quot;: &amp;quot;0.0.0.0&amp;quot;, &amp;quot;port&amp;quot;: &amp;quot;24220&amp;quot; }, &amp;quot;output_plugin&amp;quot;: false, &amp;quot;retry_count&amp;quot;: null }, { &amp;quot;plugin_id&amp;quot;: &amp;quot;object:3f8590dc1f2c&amp;quot;, &amp;quot;plugin_category&amp;quot;: &amp;quot;output&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;file&amp;quot;, &amp;quot;config&amp;quot;: { &amp;quot;@type&amp;quot;: &amp;quot;file&amp;quot;, &amp;quot;path&amp;quot;: &amp;quot;/var/log/td-agent/hoge.</description>
    </item>
    
    <item>
      <title>gcloudのアカウント切り替えとkubectlのcontext変更</title>
      <link>https://www.sambaiz.net/article/28/</link>
      <pubDate>Tue, 25 Oct 2016 20:29:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/28/</guid>
      <description>いつも迷うのでまとめた。
gcloudのアカウント一覧と切り替え $ gcloud auth list $ gcloud config set account `ACCOUNT` configにprojectなども設定している場合はconfig自体を作成して切り替えた方が楽。
$ gcloud config configurations create &amp;lt;name&amp;gt; $ gcloud config configurations activate &amp;lt;name&amp;gt; $ gcloud config list ... Your active configuration is: [&amp;lt;name&amp;gt;] $ gcloud config set account &amp;lt;accout&amp;gt; $ gcloud config set project &amp;lt;project&amp;gt; kubectlのcontext変更 $ kubectl config current-context $ kubectl config view # contexts $ kubectl config use-context minikube </description>
    </item>
    
    <item>
      <title>GKEで複数コンテナのアプリケーションを動かす</title>
      <link>https://www.sambaiz.net/article/18/</link>
      <pubDate>Fri, 26 Aug 2016 21:57:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/18/</guid>
      <description>前回は単一コンテナのアプリケーションを動かしたが、今回はコンテナ間でやり取りが発生するものを動かす。 流れとしては、クライアントからのリクエストをGATEWAYサーバーで受け取り、SERVICEサーバーにリクエストし、その結果を返すまで。
プログラムは以下の通り、環境変数TYPEの値によって挙動を変えていて、同じイメージを使い回す。コードはここ。
var http = require(&#39;http&#39;); var handleRequest = function(request, response) { if(process.env.TYPE == &amp;quot;GATEWAY&amp;quot;){ console.log(&#39;Passed.&#39;); var options = { host: &#39;service&#39;, port: 8080, method: &#39;GET&#39; }; var req = http.request(options, function(res) { data = &amp;quot;&amp;quot; res.on(&#39;data&#39;, function (chunk) { data+=chunk; }); res.on(&#39;end&#39;, () =&amp;gt; { response.writeHead(200); response.end(data); }); }); req.on(&#39;error&#39;, function(e) { response.writeHead(500) response.end(e.message); }); req.end(); }else{ console.log(&#39;Received.&#39;); response.writeHead(200); response.end(&#39;ok&#39;); } }; var www = http.createServer(handleRequest); www.listen(8080); これをContainer RegistryにPushする。</description>
    </item>
    
    <item>
      <title>Google Container Engine(GKE)で単一コンテナのアプリケーションを動かす</title>
      <link>https://www.sambaiz.net/article/17/</link>
      <pubDate>Sun, 21 Aug 2016 23:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/17/</guid>
      <description>Kubernetes - Hello World Walkthrough
CloudSDKとkubectlのインストール Cloud SDKをインストールしてgloudコマンドを使えるようにする。
$ gcloud --version Google Cloud SDK 122.0.0 $ gcloud components install kubectl Google Container RegistryにPush $ export PROJECT_ID=&amp;quot;******&amp;quot; $ docker build -t gcr.io/$PROJECT_ID/test:v1 . $ gcloud docker push gcr.io/$PROJECT_ID/test:v1 プロジェクトの課金を有効にしていないとこんなエラーメッセージが出る。
denied: Unable to create the repository, please check that you have access to do so. Clusterの作成 $ gcloud config set core/project $PROJECT_ID $ gcloud config set compute/zone asia-east1-b $ gcloud container clusters create test-cluster $ gcloud config set container/cluster test-cluster Container Engine APIが有効になっていない場合はこうなる。 一度コンソールからContainer Engineを選ぶと、サービスの準備が始まって有効になる。</description>
    </item>
    
  </channel>
</rss>
