<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/tensorflow/index.xml</link>
    <language>ja</language>
    <author>sambaiz</author>
    <rights>(C) 2018</rights>
    <updated>0001-01-01 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>TensorflowのRNN(LSTM)のチュートリアルのコードを読む</title>
          <link>https://www.sambaiz.net/article/146/</link>
          <pubDate>Wed, 03 Jan 2018 21:12:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/146/</guid>
          <description>

&lt;p&gt;TensorflowのRNN(Recurrent Neural Networks)の&lt;a href=&#34;https://www.tensorflow.org/tutorials/recurrent&#34;&gt;チュートリアル&lt;/a&gt;の&lt;a href=&#34;https://github.com/tensorflow/models/blob/12f279d6f4cb33574bc20109b41eb8a59f40cfd1/tutorials/rnn/ptb/ptb_word_lm.py&#34;&gt;コード&lt;/a&gt;を読む。これは文章のそれまでの単語の履歴から、その次に続く単語を予測することで言語モデルを作るもの。&lt;/p&gt;

&lt;h2 id=&#34;rnn-lstmとは&#34;&gt;RNN/LSTMとは&lt;/h2&gt;

&lt;p&gt;RNNは入力に対して出力のほかに情報を次のステップに渡すネットワーク。
展開すると同じネットワークに単語を一つずつ入れていくように表現できる。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/146-rnn.png&#34; alt=&#34;RNN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;TensorflowではいくつかRNNの実装が用意されていて、このコードではそのうちの&lt;code&gt;CudnnLSTM&lt;/code&gt;や&lt;code&gt;BasicLSTMCell&lt;/code&gt;、&lt;code&gt;LSTMBlockCell&lt;/code&gt;を選べるようになっている。&lt;a href=&#34;https://developer.nvidia.com/cudnn&#34;&gt;cuDNN&lt;/a&gt;というのはNVIDIAのCUDAのDNNライブラリのこと。&lt;code&gt;LSTMBlockCell&lt;/code&gt;は&lt;code&gt;BasicLSTMCell&lt;/code&gt;より速い。&lt;/p&gt;

&lt;p&gt;LSTM(Long Short Term Memory networks)はRNNの一種で、入力にtanhを通す通常のRNNの処理に加え、それぞれ重みを持ち、どの値を更新するか決定する&lt;code&gt;input gate&lt;/code&gt;や、どの値を忘れるかを決定する&lt;code&gt;forget gate&lt;/code&gt;、何を出力するか決定する&lt;code&gt;output gate&lt;/code&gt;を通す。
こちらはtanhではなく値域(0,1)のシグモイドを通したものを掛けていくので、0であれば情報は失われ、1であれば完全に残る。&lt;/p&gt;

&lt;h2 id=&#34;動かしてみる&#34;&gt;動かしてみる&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ git clone https://github.com/tensorflow/models.git
$ cd models/tutorials/rnn/ptb/
$ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz
$ tar xvf simple-examples.tgz 
$ python3 -m venv env
$ . ./env/bin/activate
$ pip install numpy tensorflow
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ python ptb_word_lm.py --data_path=simple-examples/data/ --num_gpus=0
Epoch: 1 Learning rate: 1.000
0.004 perplexity: 5534.452 speed: 894 wps
0.104 perplexity: 845.383 speed: 1277 wps
...
0.803 perplexity: 316.808 speed: 1195 wps
0.903 perplexity: 298.087 speed: 1205 wps
Epoch: 1 Train Perplexity: 283.825
Epoch: 1 Valid Perplexity: 182.132
Epoch: 2 Learning rate: 1.000
...
Epoch: 4 Learning rate: 1.000
...
Epoch: 5 Learning rate: 0.500
...
Epoch: 6 Learning rate: 0.250
...
Epoch: 7 Learning rate: 0.125
...
Epoch: 13 Learning rate: 0.002
...
Test Perplexity: 121.759
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;reader&#34;&gt;reader&lt;/h2&gt;

&lt;p&gt;readerにはテストがあったので、これを使って実際にどんな出力をしているか見てみる。&lt;/p&gt;

&lt;h3 id=&#34;ptb-raw-data&#34;&gt;ptb_raw_data&lt;/h3&gt;

&lt;p&gt;単語をIDに変換したものと語彙数が返る。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def setUp(self):
  self._string_data = &amp;quot;\n&amp;quot;.join(
    [&amp;quot; hello there i am&amp;quot;,
     &amp;quot; rain as day&amp;quot;,
     &amp;quot; want some cheesy puffs ?&amp;quot;])

def testPtbRawData(self):
  tmpdir = tf.test.get_temp_dir()
  for suffix in &amp;quot;train&amp;quot;, &amp;quot;valid&amp;quot;, &amp;quot;test&amp;quot;:
    filename = os.path.join(tmpdir, &amp;quot;ptb.%s.txt&amp;quot; % suffix)
    with tf.gfile.GFile(filename, &amp;quot;w&amp;quot;) as fh:
    fh.write(self._string_data)
  # Smoke test
  output = reader.ptb_raw_data(tmpdir)
  print(&#39;output: {0}&#39;.format(output))
  # output: (
  #   [5, 10, 6, 1, 8, 2, 4, 11, 9, 3, 7, 0], # train
  #   [5, 10, 6, 1, 8, 2, 4, 11, 9, 3, 7, 0], # valid
  #   [5, 10, 6, 1, 8, 2, 4, 11, 9, 3, 7, 0], # test
  #   12 # vocabulary
  # )
  self.assertEqual(len(output), 4)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;print(word_to_id)
=&amp;gt; {&#39;?&#39;: 0, &#39;am&amp;lt;eos&amp;gt;&#39;: 1, &#39;as&#39;: 2, &#39;cheesy&#39;: 3, &#39;day&amp;lt;eos&amp;gt;&#39;: 4, &#39;hello&#39;: 5, &#39;i&#39;: 6, &#39;puffs&#39;: 7, &#39;rain&#39;: 8, &#39;some&#39;: 9, &#39;there&#39;: 10, &#39;want&#39;: 11}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ptb-producer&#34;&gt;ptb_producer&lt;/h3&gt;

&lt;p&gt;session.runする度に時系列順に[batch_size, num_steps]のTensorを出力する。
二つ目の返り値は一つ右にずらしたもの。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def testPtbProducer(self):
  raw_data = [
  # t=0↓  t=1↓
    4, 3, 2, 1, 0, 
    5, 6, 1, 1, 1, 
    1, 0, 3, 4, 1
  ]
  batch_size = 3
  num_steps = 2
  x, y = reader.ptb_producer(raw_data, batch_size, num_steps)
  with self.test_session() as session:
    coord = tf.train.Coordinator()
    tf.train.start_queue_runners(session, coord=coord)
    try:
      xval, yval = session.run([x, y])
      self.assertAllEqual(xval, [[4, 3], [5, 6], [1, 0]])
      self.assertAllEqual(yval, [[3, 2], [6, 1], [0, 3]])
      xval, yval = session.run([x, y])
      self.assertAllEqual(xval, [[2, 1], [1, 1], [3, 4]])
      self.assertAllEqual(yval, [[1, 0], [1, 1], [4, 1]])
    finally:
      coord.request_stop()
      coord.join()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;実装はこんな感じ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()
x = tf.strided_slice(data, [0, i * num_steps],
                        [batch_size, (i + 1) * num_steps])
x.set_shape([batch_size, num_steps])
y = tf.strided_slice(data, [0, i * num_steps + 1],
                        [batch_size, (i + 1) * num_steps + 1])
y.set_shape([batch_size, num_steps])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;range_input_producerはその名の通りrangeのように0から値を生成するが、
Threadを調整する&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/Coordinator&#34;&gt;Coordinator&lt;/a&gt;を生成し、
&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/start_queue_runners&#34;&gt;start_queue_runners&lt;/a&gt;に渡す必要がある。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# example of range_input_producer
with self.test_session() as session:
  i = tf.train.range_input_producer(100, shuffle=False).dequeue()
  coord = tf.train.Coordinator()
  tf.train.start_queue_runners(session, coord=coord)
  try:
    print(session.run(i)) # =&amp;gt; 0
    print(session.run(i)) # =&amp;gt; 1
    print(session.run(i)) # =&amp;gt; 2
  finally:
    coord.request_stop()
    coord.join() # Wait for all the threads to terminate.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;

&lt;h3 id=&#34;入力の準備&#34;&gt;入力の準備&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup&#34;&gt;embedding_lookup&lt;/a&gt;で
embeddingから各stepの単語のものを抽出する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;with tf.device(&amp;quot;/cpu:0&amp;quot;):
  embedding = tf.get_variable(
    &amp;quot;embedding&amp;quot;, [vocab_size, size], dtype=data_type())
  # shape=(batch_size, num_steps, size), dtype=float32
  inputs = tf.nn.embedding_lookup(embedding, input_.input_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# example of embedding_lookup
with tf.Session() as session:
  print(session.run(tf.nn.embedding_lookup(
    [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11]],
    [[0,1,2], [3,4,5], [6,7,8]]
  ))) 
  # =&amp;gt; [[ 0  2  4]
  #     [ 6  8 10]
  #     [ 1  3  5]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;学習中の場合、過学習を防ぐためkeep_prob残してDropoutし、RNNのグラフを作り始める。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if is_training and config.keep_prob &amp;lt; 1:
  inputs = tf.nn.dropout(inputs, config.keep_prob)

output, state = self._build_rnn_graph(inputs, config, is_training)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;rnnのグラフ&#34;&gt;RNNのグラフ&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;rnn_mode&lt;/code&gt;で実装を選べるようになっているが、基本BLOCK。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def _build_rnn_graph(self, inputs, config, is_training):
  if config.rnn_mode == CUDNN:
    return self._build_rnn_graph_cudnn(inputs, config, is_training)
  else:
    return self._build_rnn_graph_lstm(inputs, config, is_training)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Cellは&lt;code&gt;LSTMBlockCell&lt;/code&gt;をDroopoutWrapperでラップしたもの。
さらにこれをCellの出力が次のCellの入力になる&lt;code&gt;MultiRNNCell&lt;/code&gt;で&lt;code&gt;num_layers&lt;/code&gt;重ねている。&lt;/p&gt;

&lt;p&gt;最初に&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell#zero_state&#34;&gt;zero_state&lt;/a&gt;の
初期状態から&lt;code&gt;num_steps&lt;/code&gt;まわして各stepでのoutputと最後のstateを返す。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def _get_lstm_cell(self, config, is_training):
  if config.rnn_mode == BASIC:
    return tf.contrib.rnn.BasicLSTMCell(
      config.hidden_size, forget_bias=0.0, state_is_tuple=True,
      reuse=not is_training)
  if config.rnn_mode == BLOCK:
    return tf.contrib.rnn.LSTMBlockCell(
      config.hidden_size, forget_bias=0.0)
  raise ValueError(&amp;quot;rnn_mode %s not supported&amp;quot; % config.rnn_mode)

def _build_rnn_graph_lstm(self, inputs, config, is_training):
  def make_cell():
    cell = self._get_lstm_cell(config, is_training)
    if is_training and config.keep_prob &amp;lt; 1:
      cell = tf.contrib.rnn.DropoutWrapper(
        cell, output_keep_prob=config.keep_prob)
    return cell

  cell = tf.contrib.rnn.MultiRNNCell(
    [make_cell() for _ in range(config.num_layers)], state_is_tuple=True)

  self._initial_state = cell.zero_state(config.batch_size, data_type())
  state = self._initial_state

  # [shape=(batch_size, hidden_size) dtype=float32, ...]
  outputs = []
  with tf.variable_scope(&amp;quot;RNN&amp;quot;):
    for time_step in range(self.num_steps):
      if time_step &amp;gt; 0: tf.get_variable_scope().reuse_variables()
      (cell_output, state) = cell(inputs[:, time_step, :], state)
      outputs.append(cell_output)

  # shape=(batch_size * num_steps, hidden_size), dtype=float32
  output = tf.reshape(tf.concat(outputs, 1), [-1, config.hidden_size])
  return output, state
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;コスト&#34;&gt;コスト&lt;/h3&gt;

&lt;p&gt;このoutputにもう一つ層を通してlogits(&lt;code&gt;log(p/(1-p)) (0≦p≦1)&lt;/code&gt;)として扱い、
&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq/sequence_loss&#34;&gt;sequence_loss&lt;/a&gt;で
logitsのシーケンスの交差エントロピーを求め、その和をコストとする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;output, state = self._build_rnn_graph(inputs, config, is_training)

softmax_w = tf.get_variable(
    &amp;quot;softmax_w&amp;quot;, [size, vocab_size], dtype=data_type())
softmax_b = tf.get_variable(&amp;quot;softmax_b&amp;quot;, [vocab_size], dtype=data_type())
logits = tf.nn.xw_plus_b(output, softmax_w, softmax_b)
# shape=(batch_size, num_steps, vocab_size), dtype=float32
logits = tf.reshape(logits, [self.batch_size, self.num_steps, vocab_size])

loss = tf.contrib.seq2seq.sequence_loss(
    # logits: [batch_size, sequence_length=num_steps, num_decoder_symbols=vocab_size] and dtype float
    # The logits correspond to the prediction across all classes at each timestep.
    logits,

    # targets: [batch_size, sequence_length=num_steps] and dtype int
    # The target represents the true class at each timestep.
    input_.targets,

    # weights: [batch_size, sequence_length] and dtype float
    # When using weights as masking, set all valid timesteps to 1 and all padded timesteps to 0
    tf.ones([self.batch_size, self.num_steps], dtype=data_type()),

    average_across_timesteps=False,
    average_across_batch=True)

# Update the cost
self._cost = tf.reduce_sum(loss)
self._final_state = state
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;勾配&#34;&gt;勾配&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/trainable_variables&#34;&gt;trainable_variables&lt;/a&gt;で
&lt;code&gt;trainable=True(デフォルト)&lt;/code&gt;のvariableを取得し、
&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/gradients&#34;&gt;gradients&lt;/a&gt;で各variableに対しての勾配を求め、
&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/clip_by_global_norm&#34;&gt;clip_by_global_norm&lt;/a&gt;で
全体のノルムの大きさを抑える。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if not is_training:
    return

self._lr = tf.Variable(0.0, trainable=False)
tvars = tf.trainable_variables()
grads, _ = tf.clip_by_global_norm(tf.gradients(self._cost, tvars),
                                    config.max_grad_norm)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# example of trainable_variables
with tf.Session() as session:
  a = tf.Variable(10.0, trainable=False)
  b = tf.Variable(20.0)
  c = tf.get_variable(&amp;quot;c&amp;quot;, [2, 2])
  d = tf.get_variable(&amp;quot;d&amp;quot;, [3, 3], trainable=False)
  session.run(tf.global_variables_initializer())
  print(session.run(tf.trainable_variables()))
  # [20.0, array([[ 1.10110056,  0.6373167 ],
  # [ 0.44673324, -0.11995673]], dtype=float32)]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;# example of gradients &amp;amp; clip_by_global_norm
with tf.Session() as session:
  xs = tf.Variable([10., 20., 30.])
  ys = [xs ** 2 + 123, xs * 5]
  grad = tf.gradients(ys,xs)
  session.run(tf.global_variables_initializer())
  print(session.run(grad)) # [20 + 5, 40 + 5, 60 + 5]

  list_clipped, global_norm = session.run(tf.clip_by_global_norm(grad,2))
  # global_norm = sqrt(sum([l2norm(t)**2 for t in t_list]))
  # = sqrt(25 ** 2 + 45 ** 2 + 65 ** 2)
  print(global_norm) # 82.9156

  # t_list[i] * clip_norm / max(global_norm, clip_norm)
  # = [25, 45, 65] * 2 / global_norm
  print(list_clipped) # [0.60302269, 1.08544087, 1.56785905]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;optimize&#34;&gt;Optimize&lt;/h3&gt;

&lt;p&gt;学習率_lrの&lt;code&gt;GradientDescenetOptimizer&lt;/code&gt;でoptimizeする。
&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer#apply_gradients&#34;&gt;apply_gradients&lt;/a&gt;するたびに
global_stepがインクリメントされる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;optimizer = tf.train.GradientDescentOptimizer(self._lr)
self._train_op = optimizer.apply_gradients(
  zip(grads, tvars),
  global_step=tf.train.get_or_create_global_step())

self._new_lr = tf.placeholder(
  tf.float32, shape=[], name=&amp;quot;new_learning_rate&amp;quot;)
self._lr_update = tf.assign(self._lr, self._new_lr)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;run-epoch&#34;&gt;run_epoch&lt;/h3&gt;

&lt;p&gt;&lt;code&gt;session.run&lt;/code&gt;する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fetches = {
  &amp;quot;cost&amp;quot;: model.cost,
  &amp;quot;final_state&amp;quot;: model.final_state,
}
if eval_op is not None:
  fetches[&amp;quot;eval_op&amp;quot;] = eval_op

for step in range(model.input.epoch_size):
  feed_dict = {}
  for i, (c, h) in enumerate(model.initial_state):
    feed_dict[c] = state[i].c
    feed_dict[h] = state[i].h

  vals = session.run(fetches, feed_dict)
  cost = vals[&amp;quot;cost&amp;quot;]
  state = vals[&amp;quot;final_state&amp;quot;]

  costs += cost
  iters += model.input.num_steps

  if verbose and step % (model.input.epoch_size // 10) == 10:
    print(&amp;quot;%.3f perplexity: %.3f speed: %.0f wps&amp;quot; %
      (step * 1.0 / model.input.epoch_size, np.exp(costs / iters),
       iters * model.input.batch_size * max(1, FLAGS.num_gpus) /
       (time.time() - start_time)))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;main&#34;&gt;main&lt;/h3&gt;

&lt;p&gt;起点。学習率はmax_epochまで初期値で、それ以後のepochでは指数的に減少させていく。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;with tf.Graph().as_default():
  tf.train.import_meta_graph(metagraph)
  for model in models.values():
    model.import_ops()
  sv = tf.train.Supervisor(logdir=FLAGS.save_path)
  config_proto = tf.ConfigProto(allow_soft_placement=soft_placement)
  with sv.managed_session(config=config_proto) as session:
    for i in range(config.max_max_epoch):
      lr_decay = config.lr_decay ** max(i + 1 - config.max_epoch, 0.0)
      m.assign_lr(session, config.learning_rate * lr_decay)

      print(&amp;quot;Epoch: %d Learning rate: %.3f&amp;quot; % (i + 1, session.run(m.lr)))
      train_perplexity = run_epoch(session, m, eval_op=m.train_op,
                                    verbose=True)
      print(&amp;quot;Epoch: %d Train Perplexity: %.3f&amp;quot; % (i + 1, train_perplexity))
      valid_perplexity = run_epoch(session, mvalid)
      print(&amp;quot;Epoch: %d Valid Perplexity: %.3f&amp;quot; % (i + 1, valid_perplexity))

    test_perplexity = run_epoch(session, mtest)
    print(&amp;quot;Test Perplexity: %.3f&amp;quot; % test_perplexity)

    if FLAGS.save_path:
      print(&amp;quot;Saving model to %s.&amp;quot; % FLAGS.save_path)
      sv.saver.save(session, FLAGS.save_path, global_step=sv.global_step)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://colah.github.io/posts/2015-08-Understanding-LSTMs/&#34;&gt;Understanding LSTM Networks &amp;ndash; colah&amp;rsquo;s blog&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://qiita.com/t_Signull/items/21b82be280b46f467d1b&#34;&gt;わかるLSTM ～ 最近の動向と共に - Qiita&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://returnn.readthedocs.io/en/latest/tf_lstm_benchmark.html&#34;&gt;TensorFlow LSTM benchmark — RETURNN 1.0-dev documentation&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>ニューラルネットワークと活性化関数</title>
          <link>https://www.sambaiz.net/article/133/</link>
          <pubDate>Mon, 18 Sep 2017 23:50:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/133/</guid>
          <description>

&lt;p&gt;ニューラルネットワークの活性化関数は各層での重み掛けバイアス足しのあとに適用する非線形の関数。
というのも、線形な計算を繰り返したところで&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f(x) = ax + b
g(x) = f(f(x)) = (a^2)x + (ab + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように単一の線形関数で表現できてしまい、多層にする意味がないため。
また、バックプロバゲーション(誤差逆伝播法)のために微分できる必要もある。&lt;/p&gt;

&lt;p&gt;Tensorflowでも以下の活性化関数が&lt;a href=&#34;https://www.tensorflow.org/api_guides/python/nn#Activation_Functions&#34;&gt;用意されている&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;sigmoid-https-www-tensorflow-org-api-docs-python-tf-sigmoid&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/sigmoid&#34;&gt;sigmoid&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-sigmoid.png&#34; alt=&#34;シグモイド関数&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;y = 1 / (1 + exp(-x))&lt;/code&gt;。値域は(0,1)で&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89&#34;&gt;シグマの語末系ςに似たS字を描く&lt;/a&gt;。
xが大きいときに微分係数が小さくなるため、何層もこの関数を適用するとき、バックプロバゲーションで微分係数を掛けた結果、勾配が消滅(Gradient vanishing)する問題があり、あまり使われないようだ。値域が(-1,1)で似たグラフを描く&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/tanh&#34;&gt;tanh&lt;/a&gt;(Hyperbolic tangent)もある。&lt;/p&gt;

&lt;h3 id=&#34;softsign-https-www-tensorflow-org-api-docs-python-tf-nn-softsign&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/softsign&#34;&gt;softsign&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-softsign.png&#34; alt=&#34;softsign&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x/(1 + abs(x))&lt;/code&gt;。tanhと比べて漸近線に近づく速度が遅くなっている。
それほど性能は変わらないが、初期化においてロバストになるはたらきがあるようだ。&lt;/p&gt;

&lt;h3 id=&#34;softplus-https-www-tensorflow-org-api-docs-python-tf-nn-softplus&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/softplus&#34;&gt;softplus&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-softplus.png&#34; alt=&#34;softplus&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;log(1 + exp(x))&lt;/code&gt;。ReLUに続く。&lt;/p&gt;

&lt;h3 id=&#34;relu-https-www-tensorflow-org-api-docs-python-tf-nn-relu-rectified-linear-unit&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/relu&#34;&gt;ReLU&lt;/a&gt;(Rectified Linear Unit)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-relu.png&#34; alt=&#34;ReLU&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;max(0, x)&lt;/code&gt;。単純だけど最有力。Gradient vanishingも起きない。
softplusと比べてexpやlogを含まない分高速に計算できるので、
膨大で複雑なデータセットに対して多くの層を用いることができる。&lt;/p&gt;

&lt;p&gt;0以下は等しく0になるため、トレーニング中に落ちてしまうとニューロンが死んでしまうことがある。
そのような場合は0以下のとき&lt;code&gt;y = exp(x) - 1&lt;/code&gt;にする&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/elu&#34;&gt;ELU&lt;/a&gt;(Exponential Linear Unit)
などを使う。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-elu.png&#34; alt=&#34;ELU&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f&#34;&gt;Activation functions and it’s types-Which is better?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.orsj.or.jp/archive2/or60-4/or60_4_191.pdf&#34;&gt;最適化から見たディープラーニングの考え方&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&#34;&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&#34;&gt;Rectifier (neural networks) - Wikipedia&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>DeepMindのTensorFlowライブラリSonnetを使う</title>
          <link>https://www.sambaiz.net/article/124/</link>
          <pubDate>Sun, 06 Aug 2017 23:54:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/124/</guid>
          <description>

&lt;p&gt;AlphaGoを開発したGoogle DeepMind社のTensorFlowライブラリ&lt;a href=&#34;https://github.com/deepmind/sonnet&#34;&gt;Sonnet&lt;/a&gt;を使う。
当初はPython2しか対応していないようだったけど、今は3にも対応している。&lt;/p&gt;

&lt;h2 id=&#34;準備&#34;&gt;準備&lt;/h2&gt;

&lt;p&gt;TensorFlowを使うライブラリはほかにもいくつかあるのだけど、
&lt;a href=&#34;https://github.com/fchollet/keras&#34;&gt;Keras&lt;/a&gt;と比較してみると、
KerasがTensorFlowの部分を完全にラップしているのに対して、
Sonnetは必要に応じてTensorFlowの関数も呼ぶ、比較的抽象度が低いライブラリのようだ。&lt;/p&gt;

&lt;p&gt;SonnetとTensorFlowとPython3入りイメージをDockerHubに&lt;a href=&#34;https://hub.docker.com/r/sambaiz/sonnet/&#34;&gt;上げた&lt;/a&gt;。
Dockerfileは&lt;a href=&#34;https://github.com/sambaiz/docker-sonnet&#34;&gt;ここ&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;内容は基本的にREADME通りだけど、
configureのところで対話的に聞かれないように前もって環境変数で設定を与えている。
あとは、&lt;a href=&#34;https://github.com/deepmind/sonnet/issues/25&#34;&gt;TensorFlowのビルドに使われているGCCのバージョンが古い&lt;/a&gt;ようで、sonnetをimportするときに以下のエラーが出たため、bazelのオプションに&lt;code&gt;--copt=&amp;quot;-D_GLIBCXX_USE_CXX11_ABI=0&amp;quot;&lt;/code&gt;を付けている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.5/dist-packages/sonnet/python/ops/_resampler.so: undefined symbol: _ZN10tensorflow7strings6StrCatB5cxx11ERKNS0_8AlphaNumES3_S3_S3_
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;起動。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -itd --name sonnet -p 6006:6006 -p 8888:8888 sambaiz/sonnet
$ docker logs sonnet
...
   Copy/paste this URL into your browser when you connect for the first time,
    to login with a token:
        http://localhost:8888/?token=*****
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Jupyter Notebookを開いてSonnetとTensorFlowがimportできることを確認する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import sonnet as snt
import tensorflow as tf
snt.resampler(tf.constant([0.]), tf.constant([0.]))
# =&amp;gt; &amp;lt;tf.Tensor &#39;resampler/Resampler:0&#39; shape=(1,) dtype=float32&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;mnist&#34;&gt;MNIST&lt;/h2&gt;

&lt;p&gt;TensorFlowのチュートリアルのデータを使って、畳み込みを行わない簡単なMNISTをやってみる。
このデータはtrain、validation、test用に最初から&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/contrib/learn/python/learn/datasets/base.py#L37&#34;&gt;分かれていて&lt;/a&gt;、
それぞれピクセル濃度配列の画像データと、その画像がどの数字なのかを表すone-hot vectorのラベルを&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/v1.2.1/tensorflow/contrib/learn/python/learn/datasets/mnist.py#L105&#34;&gt;含んでいる&lt;/a&gt;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&amp;quot;MNIST_data/&amp;quot;, one_hot=True)
train, validation, test = mnist
print(train.images[0]) # ピクセルの濃度を[0,1]の値で表した配列: [0, 0, ..., 0.41568631  0.6156863, 0.99607849, ...]
print(len(train.images[0])) # 28 * 28 = 784
print(train.labels[0]) # 正解のみ1のone-hot vector: [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]
images, labels = mnist.train.next_batch(100)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Sonnetではニューラルネットワークの一部をModuleとして表現し、それらをTensorFlowの計算グラフに接続していく。
Moduleはグラフに複数回接続することができ、中の変数は共有される。
素のTensorFlowだと&lt;a href=&#34;https://www.tensorflow.org/programmers_guide/variable_scope&#34;&gt;変数のスコープ&lt;/a&gt;を作って共有するのに
reuse=Trueで&lt;code&gt;tf.variable_scope&lt;/code&gt;して&lt;code&gt;tf.get_variable&lt;/code&gt;したりする必要があるけど、そのあたりは抽象化されているので
&lt;code&gt;tf.Variable&lt;/code&gt;を含むような処理はModuleで行う。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/deepmind/sonnet/blob/v1.8/sonnet/python/modules/basic.py&#34;&gt;Linear Module&lt;/a&gt;は
重みの乗算とバイアスの加算をするもの。
これに&lt;code&gt;tf.Sigmoid&lt;/code&gt;のような活性化関数を適用するのを繰り返し、最後に出力層とつなげるとMulti Layer Perceptronを構築できる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import sonnet as snt
import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

FLAGS = tf.flags.FLAGS

tf.flags.DEFINE_integer(&amp;quot;hidden_size&amp;quot;, 100, &amp;quot;Size of hidden layer.&amp;quot;)
tf.flags.DEFINE_integer(&amp;quot;output_size&amp;quot;, 10, &amp;quot;Size of output layer.&amp;quot;)

mnist = input_data.read_data_sets(&amp;quot;MNIST_data/&amp;quot;, one_hot=True)

x = tf.placeholder(tf.float32, [None, 784])
y_ = tf.placeholder(tf.float32, [None, 10])
lin_to_hidden = snt.Linear(output_size=FLAGS.hidden_size, name=&#39;inp_to_hidden&#39;)
hidden_to_out = snt.Linear(output_size=FLAGS.output_size, name=&#39;hidden_to_out&#39;)
mlp = snt.Sequential([lin_to_hidden, tf.sigmoid, hidden_to_out, tf.nn.softmax])
y = mlp(x)
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(1000):
        images, labels = mnist.train.next_batch(100)
        sess.run(train_step, feed_dict={x: mnist.train.images, y_: mnist.train.labels})
    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
    # =&amp;gt; 0.9307
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;moduleを作る&#34;&gt;Moduleを作る&lt;/h2&gt;

&lt;p&gt;Moduleを作るには&lt;code&gt;snt.AbstractModule&lt;/code&gt;を継承し、
スーパークラスのコンストラクタを呼んで、グラフに接続されるたびに呼ばれる&lt;code&gt;_build&lt;/code&gt;メソッドを実装する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class MyMLP(snt.AbstractModule):
  &amp;quot;&amp;quot;&amp;quot;test mlp module&amp;quot;&amp;quot;&amp;quot;
  def __init__(self, hidden_size, output_size,
               nonlinearity=tf.sigmoid, name=&amp;quot;my_mlp&amp;quot;):
    &amp;quot;&amp;quot;&amp;quot;hidden_size &amp;amp; output_size is required&amp;quot;&amp;quot;&amp;quot;
    super(MyMLP, self).__init__(name=name)
    self._hidden_size = hidden_size
    self._output_size = output_size
    self._nonlinearity = nonlinearity
  
  def _build(self, inputs):
    &amp;quot;&amp;quot;&amp;quot;Compute output Tensor from input Tensor.&amp;quot;&amp;quot;&amp;quot;
    lin_to_hidden = snt.Linear(output_size=self._hidden_size, name=&#39;inp_to_hidden&#39;)
    hidden_to_out = snt.Linear(output_size=self._output_size, name=&#39;hidden_to_out&#39;)
    return snt.Sequential([lin_to_hidden, self._nonlinearity, hidden_to_out, tf.nn.softmax])(inputs)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このModuleを使うとこんな感じ。
&lt;a href=&#34;https://github.com/deepmind/sonnet/blob/v1.8/sonnet/examples/dataset_shakespeare.py#L177&#34;&gt;example&lt;/a&gt;のように
データセットもModuleにすることができる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mnist = input_data.read_data_sets(&amp;quot;MNIST_data/&amp;quot;, one_hot=True)

mymlp = MyMLP(hidden_size=FLAGS.hidden_size, output_size=FLAGS.output_size)

x = tf.placeholder(tf.float32, [None, 784])
y_ = tf.placeholder(tf.float32, [None, 10])
y = mymlp(x)
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(1000):
        images, labels = mnist.train.next_batch(100)
        sess.run(train_step, feed_dict={x: mnist.train.images, y_: mnist.train.labels})
    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
    # =&amp;gt; 0.9307
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>DeepDreaming with TensorFlowをやる(2)</title>
          <link>https://www.sambaiz.net/article/21/</link>
          <pubDate>Sat, 10 Sep 2016 14:46:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/21/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/20&#34;&gt;前回&lt;/a&gt;の続き。&lt;/p&gt;

&lt;h2 id=&#34;multiscale-image-generation&#34;&gt;Multiscale image generation&lt;/h2&gt;

&lt;p&gt;様々なスケールで勾配上昇させる。小さなスケールで上昇させたものをより大きなスケールでさらに上昇させていく。
ただ、壁紙のようなサイズを生成するような場合にそれを行うと、GPUのメモリを食いつぶしてしまう。
これを避けるために、画像を小さなタイルに分割し、それぞれ独立に勾配を計算する。
また、毎回画像をランダムにシフトしていくことで、タイルに見えることを避け、画像全体の品質を向上させる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def tffunc(*argtypes):
    &#39;&#39;&#39;Helper that transforms TF-graph generating function into a regular one.
    See &amp;quot;resize&amp;quot; function below.
    &#39;&#39;&#39;
    placeholders = list(map(tf.placeholder, argtypes))
    def wrap(f):
        out = f(*placeholders)
        def wrapper(*args, **kw):
            return out.eval(dict(zip(placeholders, args)), session=kw.get(&#39;session&#39;))
        return wrapper
    return wrap

# Helper function that uses TF to resize an image
def resize(img, size):
    img = tf.expand_dims(img, 0)
    return tf.image.resize_bilinear(img, size)[0,:,:,:]
resize = tffunc(np.float32, np.int32)(resize)


def calc_grad_tiled(img, t_grad, tile_size=512):
    &#39;&#39;&#39;Compute the value of tensor t_grad over the image in a tiled way.
    Random shifts are applied to the image to blur tile boundaries over
    multiple iterations.&#39;&#39;&#39;
    sz = tile_size
    h, w = img.shape[:2]
    sx, sy = np.random.randint(sz, size=2)
    img_shift = np.roll(np.roll(img, sx, 1), sy, 0)
    grad = np.zeros_like(img)
    for y in range(0, max(h-sz//2, sz),sz):
        for x in range(0, max(w-sz//2, sz),sz):
            sub = img_shift[y:y+sz,x:x+sz]
            g = sess.run(t_grad, {t_input:sub})
            grad[y:y+sz,x:x+sz] = g
    return np.roll(np.roll(grad, -sx, 1), -sy, 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf.image.resize_bilinear&lt;/code&gt;は&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#resize_bilinear&#34;&gt;双線形補間によってリサイズする。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;numpy.roll&lt;/code&gt;は&lt;a href=&#34;http://docs.scipy.org/doc/numpy/reference/generated/numpy.roll.html&#34;&gt;配列を第三引数axisによってローリングする&lt;/a&gt;。
axisを指定しない場合、フラットなものとして扱われる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hoge = [[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]]

print(np.roll(hoge, 1))
# [[8 0 1]
#  [2 3 4]
#  [5 6 7]]

print(np.roll(hoge, 1, axis=0))
# [[6 7 8]
#  [0 1 2]
#  [3 4 5]]

print(np.roll(hoge, 1, axis=1))
# [[2 0 1]
#  [5 3 4]
#  [8 6 7]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;つまり、&lt;code&gt;calc_grad_tiled&lt;/code&gt;では、ランダムにローリングして、タイルに分割して勾配を求め、ローリングした分を戻して返している。
これと、画像サイズを&lt;code&gt;octave_scale&lt;/code&gt;倍にしていく以外は前回やったのと基本的に同じだ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def render_multiscale(t_obj, img0=img_noise, iter_n=10, step=1.0, octave_n=3, octave_scale=1.4):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!

    img = img0.copy()
    for octave in range(octave_n):
        if octave&amp;gt;0:
            hw = np.float32(img.shape[:2])*octave_scale
            img = resize(img, np.int32(hw))
        for i in range(iter_n):
            g = calc_grad_tiled(img, t_grad)
            # normalizing the gradient, so the same step size should work
            g /= g.std()+1e-8         # for different layers and networks
            img += g*step
            print(&#39;.&#39;, end = &#39; &#39;)
        clear_output()
        showarray(visstd(img))

render_multiscale(T(layer)[:,:,:,channel])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;laplacian-pyramid-gradient-normalization&#34;&gt;Laplacian Pyramid Gradient Normalization&lt;/h2&gt;

&lt;p&gt;結果の画像は、高い周波数(ピクセルの変化の度合が高い)が多く含まれている。
これを改善するための一つの方法として、毎回画像をぼかし、高周波数を抑え、画像を滑らかにするものがある。
ただ、この方法は良い画像にするためにより多くの繰り返しが必要になってしまう。
逆に、低周波数を上げるのは、ラプラシアンピラミッドを使う方法があって、これで勾配を正規化する。&lt;/p&gt;

&lt;p&gt;ラプラシアンピラミッドというのは、ガウシアンピラミッドにおける、ある解像度の画像と、
その一つレベルの高い(解像度1/2 * &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; = &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;)画像をアップサンプルしたものの差分だ。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_imgproc/py_pyramids/py_pyramids.html&#34;&gt;画像ピラミッド — OpenCV-Python Tutorials 1 documentation&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;k = np.float32([1,4,6,4,1])
k = np.outer(k, k)
# [[  1.   4.   6.   4.   1.]
#  [  4.  16.  24.  16.   4.]
#  [  6.  24.  36.  24.   6.]
#  [  4.  16.  24.  16.   4.]
#  [  1.   4.   6.   4.   1.]]

k5x5 = k[:,:,None,None]/k.sum()*np.eye(3, dtype=np.float32)

print(len(k5x5))
# 5

print(k5x5[0])
# [[[ 0.00390625  0.          0.        ]
#  [ 0.          0.00390625  0.        ]
#  [ 0.          0.          0.00390625]]

# [[ 0.015625    0.          0.        ]
#  [ 0.          0.015625    0.        ]
#  [ 0.          0.          0.015625  ]]

# [[ 0.0234375   0.          0.        ]
#  [ 0.          0.0234375   0.        ]
#  [ 0.          0.          0.0234375 ]]

# [[ 0.015625    0.          0.        ]
#  [ 0.          0.015625    0.        ]
#  [ 0.          0.          0.015625  ]]

# [[ 0.00390625  0.          0.        ]
#  [ 0.          0.00390625  0.        ]
#  [ 0.          0.          0.00390625]]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;numpy.outer&lt;/code&gt;は&lt;a href=&#34;http://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html&#34;&gt;外積を求める&lt;/a&gt;もので、
&lt;code&gt;numpy.eye&lt;/code&gt;は&lt;a href=&#34;http://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html&#34;&gt;対角線が1で、それ以外は0の2次元行列を返す&lt;/a&gt;。
&lt;code&gt;k&lt;/code&gt;を指定すると、対角線の位置を変更できるが、指定していない場合はNxNの単位行列が返ることになる。
このフィルターで畳み込むことで、ラプラシアンピラミッドの1レベル高い画像に変換できる。
&lt;code&gt;tf.nn.conv2d_transpose&lt;/code&gt;は
&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#conv2d_transpose&#34;&gt;畳み込みの逆処理&lt;/a&gt;のようなもので、
これでアップサンプルした画像と元画像の差分を取って、ラプラシアンピラミッドを生成している。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def lap_split(img):
    &#39;&#39;&#39;Split the image into lo and hi frequency components&#39;&#39;&#39;
    with tf.name_scope(&#39;split&#39;):
        lo = tf.nn.conv2d(img, k5x5, [1,2,2,1], &#39;SAME&#39;)
        lo2 = tf.nn.conv2d_transpose(lo, k5x5*4, tf.shape(img), [1,2,2,1])
        hi = img-lo2
    return lo, hi

def lap_split_n(img, n):
    &#39;&#39;&#39;Build Laplacian pyramid with n splits&#39;&#39;&#39;
    levels = []
    for i in range(n):
        img, hi = lap_split(img)
        levels.append(hi)
    levels.append(img)
    return levels[::-1]

def lap_merge(levels):
    &#39;&#39;&#39;Merge Laplacian pyramid&#39;&#39;&#39;
    img = levels[0]
    for hi in levels[1:]:
        with tf.name_scope(&#39;merge&#39;):
            img = tf.nn.conv2d_transpose(img, k5x5*4, tf.shape(hi), [1,2,2,1]) + hi
    return img

def normalize_std(img, eps=1e-10):
    &#39;&#39;&#39;Normalize image by making its standard deviation = 1.0&#39;&#39;&#39;
    with tf.name_scope(&#39;normalize&#39;):
        std = tf.sqrt(tf.reduce_mean(tf.square(img)))
        return img/tf.maximum(std, eps)

def lap_normalize(img, scale_n=4):
    &#39;&#39;&#39;Perform the Laplacian pyramid normalization.&#39;&#39;&#39;
    img = tf.expand_dims(img,0)
    tlevels = lap_split_n(img, scale_n)
    tlevels = list(map(normalize_std, tlevels))
    out = lap_merge(tlevels)
    return out[0,:,:,:]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;lap_normalize&lt;/code&gt;で画像からラプラシアンピラミッドを生成し、それぞれで正規化してからマージして元の画像に戻す処理をしている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def render_lapnorm(t_obj, img0=img_noise, visfunc=visstd,
                   iter_n=10, step=1.0, octave_n=3, octave_scale=1.4, lap_n=4):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!
    # build the laplacian normalization graph
    lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=lap_n))

    img = img0.copy()
    for octave in range(octave_n):
        if octave&amp;gt;0:
            hw = np.float32(img.shape[:2])*octave_scale
            img = resize(img, np.int32(hw))
        for i in range(iter_n):
            g = calc_grad_tiled(img, t_grad)
            g = lap_norm_func(g)
            img += g*step
            print(&#39;.&#39;, end = &#39; &#39;)
        clear_output()
        showarray(visfunc(img))

render_lapnorm(T(layer)[:,:,:,channel])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deepdream&#34;&gt;DeepDream&lt;/h2&gt;

&lt;p&gt;で、これがDeepDreamのアルゴリズム。
ラプラシアンピラミッドを生成して、リサイズの際に次のレベルのを足していっている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def render_deepdream(t_obj, img0=img_noise,
                     iter_n=10, step=1.5, octave_n=4, octave_scale=1.4):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!

    # split the image into a number of octaves
    img = img0
    octaves = []
    for i in range(octave_n-1):
        hw = img.shape[:2]
        lo = resize(img, np.int32(np.float32(hw)/octave_scale))
        hi = img-resize(lo, hw)
        img = lo
        octaves.append(hi)

    # generate details octave by octave
    for octave in range(octave_n):
        if octave&amp;gt;0:
            hi = octaves[-octave]
            img = resize(img, hi.shape[:2])+hi
        for i in range(iter_n):
            g = calc_grad_tiled(img, t_grad)
            img += g*(step / (np.abs(g).mean()+1e-7))
            print(&#39;.&#39;,end = &#39; &#39;)
        clear_output()
        showarray(img/255.0)
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>DeepDreaming with Tensorflowをやる(1)</title>
          <link>https://www.sambaiz.net/article/20/</link>
          <pubDate>Wed, 07 Sep 2016 01:06:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/20/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/examples/tutorials/deepdream/deepdream.ipynb&#34;&gt;https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/examples/tutorials/deepdream/deepdream.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;例の通りまとめながら進めていく。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;このノートブックは、畳み込みニューラルネットワークによる画像生成の手法を説明するものだ。
ネットワークは入力画像へ変換させる配列のレイヤーの集合から成り立っている。
変換のパラメータは勾配降下法で変形しながら学習していく。
内部的な画像の表現は意味不明なように見えるが、可視化し、解釈することができる。&lt;/p&gt;

&lt;h3 id=&#34;loading-and-displaying-the-model-graph&#34;&gt;Loading and displaying the model graph&lt;/h3&gt;

&lt;p&gt;学習済みネットワークのprotobufファイルが用意されていて、これをダウンロードして使う。
ただ&lt;code&gt;gcr.io/tensorflow/tensorflow&lt;/code&gt;にwgetもunzipも入っていなかったので、中に入ってapt-getした。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model_fn = &#39;tensorflow_inception_graph.pb&#39;

# creating TensorFlow session and loading the model
graph = tf.Graph()
sess = tf.InteractiveSession(graph=graph)
with tf.gfile.FastGFile(model_fn, &#39;rb&#39;) as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
t_input = tf.placeholder(np.float32, name=&#39;input&#39;) # define the input tensor
imagenet_mean = 117.0
t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)
tf.import_graph_def(graph_def, {&#39;input&#39;:t_preprocessed})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf.gfile.FastGFile&lt;/code&gt;のドキュメントが見つからないので
&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/568092d4507996d8aff0c46d6c57488a26596dd5/tensorflow/python/platform/gfile.py#L218&#34;&gt;ソース&lt;/a&gt;
を探したところFile I/Oのラッパーのようだ。これでprotobufファイルを読み、ParseFromStringでGraphDefにする。&lt;/p&gt;

&lt;p&gt;さらにこれと入力データを&lt;code&gt;tf.import_graph_def&lt;/code&gt;に
渡すことで&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html#import_graph_def&#34;&gt;Graphに取り込む&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tf.expand_dims&lt;/code&gt;は&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html#expand_dims&#34;&gt;値が1の次元を指定の場所に挿入する&lt;/a&gt;
もの。なんでそんなことをしたり、&lt;code&gt;imagenet_mean&lt;/code&gt;を引いているのかは説明がなかった。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;layers = [op.name for op in graph.get_operations() if op.type==&#39;Conv2D&#39; and &#39;import/&#39; in op.name]
feature_nums = [int(graph.get_tensor_by_name(name+&#39;:0&#39;).get_shape()[-1]) for name in layers]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このlayersに入っているのはこんな感じ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import/conv2d0_pre_relu/conv
import/conv2d1_pre_relu/conv
import/conv2d2_pre_relu/conv
import/mixed3a_1x1_pre_relu/conv
import/mixed3a_3x3_bottleneck_pre_relu/conv
import/mixed3a_3x3_pre_relu/conv
import/mixed3a_5x5_bottleneck_pre_relu/conv
import/mixed3a_5x5_pre_relu/conv
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これらのレイヤーのうち、&lt;code&gt;mixed4d_3x3_bottleneck_pre_relu&lt;/code&gt;を可視化してみる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;layer = &#39;mixed4d_3x3_bottleneck_pre_relu&#39;
channel = 139 # picking some feature channel to visualize

def T(layer):
    &#39;&#39;&#39;Helper for getting layer output tensor&#39;&#39;&#39;
    return graph.get_tensor_by_name(&amp;quot;import/%s:0&amp;quot;%layer)

render_naive(T(layer)[:,:,:,channel])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;mixed4d_3x3_bottleneck_pre_relu&#39;&lt;/code&gt;は144チャンネルのフィルターで、今回はそのうち139番目のチャンネルを選んでいる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print(T(layer))
-&amp;gt; Tensor(&amp;quot;import/mixed4d_3x3_bottleneck_pre_relu:0&amp;quot;, shape=(?, ?, ?, 144), dtype=float32, device=/device:CPU:0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初期値はRGB100(グレー)にノイズを加えた画像。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# start with a gray image with a little noise
img_noise = np.random.uniform(size=(224,224,3)) + 100.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;スコアはそのチャンネルの値の平均で、これが高くなるように画像を変化させていく。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def render_naive(t_obj, img0=img_noise, iter_n=20, step=1.0):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!

    img = img0.copy()
    for i in range(iter_n):
        g, score = sess.run([t_grad, t_score], {t_input:img})
        # normalizing the gradient, so the same step size should work
        g /= g.std()+1e-8         # for different layers and networks
        img += g*step
        print(score, end = &#39; &#39;)
    clear_output()
    showarray(visstd(img))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf.gradients(ys,xs)&lt;/code&gt;で
&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#gradients&#34;&gt;xそれぞれで偏微分したyの和&lt;/a&gt;が得られる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a = tf.Variable(tf.constant([
            [1., 2.],
            [3., 4.]]))
b = tf.Variable(tf.constant([
            [2., 3.],
            [4., 5.]]))
c = tf.matmul(a, b)
​
grad = tf.gradients(c, a)[0]
init = tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)

    print(sess.run(c))
    # [[ 10.  13.]
    # [ 22.  29.]]

    print(sess.run(grad))
    # [[ 5.  9.]
    # [ 5.  9.]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;入力画像にこれをを加算していくと、その状態からスコアが上がるパラメータが増え、下がるパラメータが減るため、勾配を上っていくことになる。
スコアが上昇するに従って、そのフィルターによる模様が浮かんできた。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/21&#34;&gt;続く&lt;/a&gt;。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Tensorflowの学習データを使ったAPIを作る</title>
          <link>https://www.sambaiz.net/article/13/</link>
          <pubDate>Fri, 05 Aug 2016 22:08:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/13/</guid>
          <description>

&lt;p&gt;チュートリアルのMNISTの学習データを使って、手書き数字画像のデータを受け取り、数字を返すAPIを作る。
コードは&lt;a href=&#34;https://github.com/sambaiz/tensorflow-use-api-sample&#34;&gt;ここ&lt;/a&gt;にある。&lt;/p&gt;

&lt;h2 id=&#34;学習して結果を保存する&#34;&gt;学習して結果を保存する&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/6&#34;&gt;前回&lt;/a&gt;の学習結果のチェックポイントファイルを出力する。
&lt;code&gt;tf.train.Saver().save&lt;/code&gt;でnameで対応するVariableの値が保存できる。&lt;/p&gt;

&lt;p&gt;今回は、学習側、アプリケーション側共にPythonを使うので、以下のようなクラスを作った。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

class Mnist:

    def __init__(self):

        g = tf.Graph()

        with g.as_default():

            W_conv1 = self._weight_variable([5, 5, 1, 32],  &amp;quot;W_conv1&amp;quot;)
            b_conv1 = self._bias_variable([32],  &amp;quot;b_conv1&amp;quot;)

            self._x = tf.placeholder(tf.float32, [None, 784])
            x_image = tf.reshape(self._x, [-1,28,28,1])

            h_conv1 = tf.nn.relu(self._conv2d(x_image, W_conv1) + b_conv1)
            h_pool1 = self._max_pool_2x2(h_conv1)

            W_conv2 = self._weight_variable([5, 5, 32, 64],  &amp;quot;W_conv2&amp;quot;)
            b_conv2 = self._bias_variable([64],  &amp;quot;b_conv2&amp;quot;)

            h_conv2 = tf.nn.relu(self._conv2d(h_pool1, W_conv2) + b_conv2)
            h_pool2 = self._max_pool_2x2(h_conv2)

            W_fc1 = self._weight_variable([7 * 7 * 64, 1024],  &amp;quot;W_fc1&amp;quot;)
            b_fc1 = self._bias_variable([1024],  &amp;quot;b_fc1&amp;quot;)

            h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

            self._keep_prob = tf.placeholder(tf.float32)
            h_fc1_drop = tf.nn.dropout(h_fc1, self._keep_prob)

            W_fc2 = self._weight_variable([1024, 10],  &amp;quot;W_fc2&amp;quot;)
            b_fc2 = self._bias_variable([10],  &amp;quot;b_fc2&amp;quot;)

            y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
            self._what_number = tf.argmax(y_conv, 1)

            self._y_ = tf.placeholder(tf.float32, [None, 10])
            cross_entropy = tf.reduce_mean(-tf.reduce_sum(self._y_ * tf.log(y_conv), reduction_indices=[1]))
            self._train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
            correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(self._y_,1))
            self._accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

            self.sess = tf.Session()
            init = tf.initialize_all_variables()
            self.sess.run(init)
            self._saver = tf.train.Saver()

    def save(self, ckpt_file_name):
        self._saver.save(self.sess, ckpt_file_name)

    def restore(self, ckpt_file_name):
        self._saver.restore(self.sess, ckpt_file_name)

    def what_number(self, image_array):
        return self.sess.run(self._what_number, feed_dict={self._x: image_array, self._keep_prob: 1.0})

    def train(self, num):
        if not hasattr(self, &amp;quot;_mnist&amp;quot;):
            self._mnist = input_data.read_data_sets(&amp;quot;MNIST_data/&amp;quot;, one_hot=True)

        for i in range(num):
            batch = self._mnist.train.next_batch(50)
            if i%100 == 0:
              train_accuracy = self._accuracy.eval(session=self.sess, feed_dict={
                      self._x:batch[0], self._y_: batch[1], self._keep_prob: 1.0
                  })
              print(&amp;quot;step %d, training accuracy %g&amp;quot;%(i, train_accuracy))
            self.sess.run(self._train_step, feed_dict={self._x: batch[0], self._y_: batch[1], self._keep_prob: 0.5})

        print(&amp;quot;test accuracy %g&amp;quot;%self._accuracy.eval(session=self.sess, feed_dict={
                self._x: self._mnist.test.images, self._y_: self._mnist.test.labels, self._keep_prob: 1.0
            }))

    def close(self):
        self.sess.close()

    def _weight_variable(self, shape, name):
      initial = tf.truncated_normal(shape, stddev=0.1)
      return tf.Variable(initial, name=name)

    def _bias_variable(self, shape, name):
      initial = tf.constant(0.1, shape=shape)
      return tf.Variable(initial, name=name)

    def _conv2d(self, x, W):
      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)

    def _max_pool_2x2(self, x):
      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;train&lt;/code&gt;で学習し、&lt;code&gt;save&lt;/code&gt;でチェックポイントファイルを保存できる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from mnist import Mnist

mnist = Mnist()
mnist.train(20000)
mnist.save(&amp;quot;model.ckpt&amp;quot;)
mnist.close()
print(&amp;quot;done&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なので、例えばこんなDockerfileを書いて&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM gcr.io/tensorflow/tensorflow

ADD training.py /
ADD mnist.py /

CMD python /training.py &amp;amp;&amp;amp; /bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらく待つ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t training .
$ docker run -itd training
$ docker logs -f &amp;lt;CONTAINER_ID&amp;gt;
$ docker cp &amp;lt;CONTAINER_ID&amp;gt;:/notebooks/model.ckpt .
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;学習データを使う&#34;&gt;学習データを使う&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;tf.train.Saver().restore&lt;/code&gt;でチェックポイントファイルを読み、Variableの値を復元できる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from flask import Flask, request, jsonify
import tensorflow as tf
from mnist import Mnist

app = Flask(__name__)

mnist = Mnist()
mnist.restore(&amp;quot;/model.ckpt&amp;quot;)

@app.route(&amp;quot;/&amp;quot;, methods=[&#39;POST&#39;])
def what_number():

    json = request.json
    if(json is None or &amp;quot;image&amp;quot; not in json or len(json[&amp;quot;image&amp;quot;]) != 784):
        return jsonify(error=&amp;quot;Need json includes image property which is 784(28 * 28) length, float([0, 1.0]) array&amp;quot;)
    else:
        result = list(mnist.what_number([json[&amp;quot;image&amp;quot;]]))
        return jsonify(result=result[0])

if __name__ == &amp;quot;__main__&amp;quot;:
    app.run(port=3000, host=&#39;0.0.0.0&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これもDockerで動かすならこんな感じ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM gcr.io/tensorflow/tensorflow

ADD requirements.txt /tmp
ADD model.ckpt /
ADD training/mnist.py /
ADD app.py /

RUN pip install -q -r /tmp/requirements.txt

EXPOSE 3000

CMD [&amp;quot;python&amp;quot;, &amp;quot;/app.py&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t tensor_api .
$ docker run -itd -p 3000:3000 tensor_app
$ docker logs -f &amp;lt;CONTAINER_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これに以下のようにして28*28の画像のデータを渡すと、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;image&amp;quot;: [ ..., 0.32941177, 0.72549021, 0.62352943, ...]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;それが何の数字なのかが返ってくる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;result&amp;quot;: 7
}
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>TensorFlow チュートリアル2(Deep MNIST for Experts)</title>
          <link>https://www.sambaiz.net/article/6/</link>
          <pubDate>Tue, 12 Jul 2016 21:16:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/6/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/3&#34;&gt;前回&lt;/a&gt;に引き続き、まとめながら進めていく。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html&#34;&gt;Deep MNIST for Experts&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;start-tensorflow-interactivesession&#34;&gt;Start TensorFlow InteractiveSession&lt;/h3&gt;

&lt;p&gt;今回は、前回のようにグラフを作成してからSessionを開始する代わりに
&lt;code&gt;InteractiveSession&lt;/code&gt;を使う。
グラフを作成し実行するのをインタラクティブに行うことができ、IPythonのような環境で便利だ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
sess = tf.InteractiveSession()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;build-a-multilayer-convolutional-network&#34;&gt;Build a Multilayer Convolutional Network&lt;/h3&gt;

&lt;p&gt;前回のシンプルなモデルでは、あまり良い結果が出なかった。
そこで、今回はもう少し洗練されたモデル、小さな畳み込みニューラルネットワークを作成する。&lt;/p&gt;

&lt;h3 id=&#34;weight-initialization&#34;&gt;Weight Initialization&lt;/h3&gt;

&lt;p&gt;このモデルを作成するためには、たくさんの重みとバイアスを作成する必要がある。&lt;/p&gt;

&lt;p&gt;重みは、対称性を破り、勾配0を避けるために、少しのノイズで初期化すべきだ。&lt;/p&gt;

&lt;p&gt;また、&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0#ReLU.EF.BC.88.E3.83.A9.E3.83.B3.E3.83.97.E9.96.A2.E6.95.B0.EF.BC.89&#34;&gt;ReLU(Rectified Linear Unit, 正規化線形関数)&lt;/a&gt;ニューロンを使うので、&amp;rdquo;死んだニューロン&amp;rdquo;を避けるためにわずかな正の値のバイアスで初期化すると良い。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tf.truncated_normal&lt;/code&gt;は&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/api_docs/python/constant_op.html#truncated_normal&#34;&gt;正規分布で、μ±2σ範囲内のランダムな値を返す。&lt;/a&gt;
以下の例だと、&lt;code&gt;mean&lt;/code&gt;のデフォルトが0.0なので、正規分布 &lt;code&gt;N(0, 0.01)&lt;/code&gt;の、&lt;code&gt;-0.2&amp;lt;=x&amp;lt;=0.2&lt;/code&gt;な値がランダムに返ることになる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;convolution-and-pooling&#34;&gt;Convolution and Pooling&lt;/h3&gt;

&lt;p&gt;TensorFlowは柔軟な畳み込みとプーリングの手続きを提供している。&lt;/p&gt;

&lt;p&gt;畳み込みというのは、画像に対してフィルターを少しずつ動かしながら掛けていく処理のことだ。&lt;a href=&#34;http://www.clg.niigata-u.ac.jp/~medimg/practice_medical_imaging/imgproc_scion/4filter/index.htm&#34;&gt;このページ&lt;/a&gt;が分かりやすい。
例えば、&lt;a href=&#34;https://en.wikipedia.org/wiki/Sobel_operator&#34;&gt;ソーベルフィルタ&lt;/a&gt;で輪郭になっているところを抽出するように、
フィルターの値によって、その区域における、ある特徴を際立たせたりすることができる。
今回はこのフィルターが重みとなり、際立たせたいのはその数字を識別するための特徴ということになる。
前回は画像を一次元の配列として扱い重みを学習していたので、縦の情報が失われていたが、この方法ではそれがない。&lt;/p&gt;

&lt;p&gt;プーリングというのは画像から区域ごとにサンプリングする処理だ。最大プーリングや、平均プーリングなどの手法がある。
畳み込みのように順番に区域を見ていって、最大プーリングならそのうちの最大のものを採用し、他のものは無視する。
サイズが小さくなるだけではなく、ちょっとした位置のずれを吸収することができる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf.nn.conv2d&lt;/code&gt;は&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#conv2d&#34;&gt;畳み込みを行う。&lt;/a&gt;
主な入力は&lt;code&gt;[画像の数, 縦サイズ, 横サイズ, チャンネル数(色とか)]&lt;/code&gt;の画像と、
&lt;code&gt;[縦サイズ, 横サイズ, 入力チャンネル数, 出力チャンネル数]&lt;/code&gt;のフィルターだ。
&lt;code&gt;strides&lt;/code&gt;は一度にどれくらいフィルターを動かしていくかで、&lt;code&gt;strides[1]&lt;/code&gt;が縦、&lt;code&gt;strides[2]&lt;/code&gt;が横に動かす量だ。
&lt;code&gt;strides[0]&lt;/code&gt;と&lt;code&gt;strides[3]&lt;/code&gt;は1でなくてはならない。
&lt;code&gt;padding&lt;/code&gt;は&amp;rdquo;SAME&amp;rdquo;か&amp;rdquo;VALID&amp;rdquo;から選択できるパディングについての設定だ。詳細は
&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#convolution&#34;&gt;ここ&lt;/a&gt;
に書いてある。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tf.nn.max_pool&lt;/code&gt;は&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#max_pool&#34;&gt;最大プーリングを行う。&lt;/a&gt;
&lt;code&gt;ksize&lt;/code&gt;は入力のそれぞれの次元に対応した見ていく区域のサイズだ。&lt;code&gt;[1, 2, 2, 1]&lt;/code&gt;ならそれぞれの画像を&lt;code&gt;2*2&lt;/code&gt;ずつ見ていくことになる。&lt;/p&gt;

&lt;h3 id=&#34;first-convolutional-layer&#34;&gt;First Convolutional Layer&lt;/h3&gt;

&lt;p&gt;最初のレイヤーは、畳み込みと最大プーリングで構成される。
畳み込みはそれぞれ&lt;code&gt;5*5&lt;/code&gt;の32チャンネル出力のフィルターでされる。
つまり、重みであるフィルターは&lt;code&gt;[5, 5, 1, 32]&lt;/code&gt;のtensorということになる。
そして、それぞれのチャンネルに対してバイアスが存在する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;W_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;画像に対して、畳み込みを適用するするために&lt;code&gt;tf.reshape&lt;/code&gt;で
&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html#reshape&#34;&gt;変形する&lt;/a&gt;
必要がある。-1というのは特別な値で、他の次元との積が合計が元のものと変わらないように決定される。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = tf.placeholder(tf.float32, [None, 784])
x_image = tf.reshape(x, [-1,28,28,1])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで畳み込めるようになったので、画像と重みを畳み込み、バイアスを足したものにReLUを適用した後、最大プーリングする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;second-convolutional-layer&#34;&gt;Second Convolutional Layer&lt;/h3&gt;

&lt;p&gt;deepネットワークにするために最初のレイヤーのようなものをいくつか重ねる。
2番目のレイヤーでは64チャンネル出力のフィルターで畳み込みを行い、プーリングする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;W_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;densely-connected-layer&#34;&gt;Densely Connected Layer&lt;/h3&gt;

&lt;p&gt;2つのレイヤーを経て元々&lt;code&gt;28*28&lt;/code&gt;だった画像は&lt;code&gt;7*7&lt;/code&gt;にまで削減された。
2つめのレイヤーの出力は64チャンネルだったので、&lt;code&gt;7*7*64&lt;/code&gt;次元のデータになっている。
このレイヤーでは、これらにそれぞれ1024のニューロンを結びつける。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;W_fc1 = weight_variable([7 * 7 * 64, 1024])
b_fc1 = bias_variable([1024])

h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;dropout&#34;&gt;Dropout&lt;/h3&gt;

&lt;p&gt;過学習を防ぐために
&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0#.E3.83.89.E3.83.AD.E3.83.83.E3.83.97.E3.82.A2.E3.82.A6.E3.83.88&#34;&gt;ドロップアウト&lt;/a&gt;
を行う。
ドロップアウトというのは訓練データごとにニューロンを何割か無視することだ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;readout-layer&#34;&gt;Readout Layer&lt;/h3&gt;

&lt;p&gt;最後にsoftmaxでそれぞれの数字である確率を求める。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;W_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;train-and-evaluate-the-model&#34;&gt;Train and Evaluate the Model&lt;/h3&gt;

&lt;p&gt;今回は前回使った&lt;code&gt;GradientDescentOptimizer&lt;/code&gt;よりも洗練されている&lt;code&gt;AdamOptimizer&lt;/code&gt;というのが使われている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;y_ = tf.placeholder(tf.float32, [None, 10])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
sess.run(tf.initialize_all_variables())
for i in range(20000):
  batch = mnist.train.next_batch(50)
  if i%100 == 0:
    train_accuracy = accuracy.eval(feed_dict={
        x:batch[0], y_: batch[1], keep_prob: 1.0})
    print(&amp;quot;step %d, training accuracy %g&amp;quot;%(i, train_accuracy))
  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

print(&amp;quot;test accuracy %g&amp;quot;%accuracy.eval(feed_dict={
    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを実行するとこんな出力が得られた。
かなり時間がかかったのでここで打ち切ったが、およそ99.2%の正解率になるらしい。
前回が92%だったのに比べてもすごく良い値に見える。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;step 0, training accuracy 0.12
step 100, training accuracy 0.8
step 200, training accuracy 0.9
step 300, training accuracy 0.9
step 400, training accuracy 0.98
step 500, training accuracy 0.88
step 600, training accuracy 0.98
step 700, training accuracy 0.98
step 800, training accuracy 0.9
step 900, training accuracy 1
step 1000, training accuracy 0.98
step 1100, training accuracy 0.98
step 1200, training accuracy 1
step 1300, training accuracy 0.98
step 1400, training accuracy 0.94
step 1500, training accuracy 0.98
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>TensorFlow チュートリアルまで</title>
          <link>https://www.sambaiz.net/article/3/</link>
          <pubDate>Sun, 03 Jul 2016 23:37:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/3/</guid>
          <description>

&lt;p&gt;Googleが公開した人工知能ライブラリTensorFlowを使ってみる。
&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html&#34;&gt;セットアップ&lt;/a&gt;方法はいくつか提供されているが、Dockerで動かすことにした。
Jupyter Notebookが立ち上がるのですぐに試せて良い。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://localhost:8888/tree&#34;&gt;http://localhost:8888/tree&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;公式のチュートリアルをまとめながら進めてみる。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html&#34;&gt;MNIST For ML Beginners&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-mnist-data&#34;&gt;The MNIST Data&lt;/h3&gt;

&lt;p&gt;MNISTというのは0~9の書き数字の画像のデータセットのことで、これらを正しく分類するのが目的。&lt;/p&gt;

&lt;p&gt;それぞれの画像は一律28*28=784ピクセルで、それぞれのピクセルは0と1の間の数値(強さ)で表されている。
今回はこれの縦横を取っ払って784次元のベクトルとして扱っている。&lt;/p&gt;

&lt;p&gt;したがって、学習用の画像データは[55000, 784]のtensor(n次元の配列)で表される。
55000というのが画像の数で、784というのがそれぞれの画像の次元を意味している。&lt;/p&gt;

&lt;p&gt;それぞれの画像に対応した数字のラベルは[55000, 10]で表される。
10というのは、0~9それぞれに対応した次元のうち、一つだけ1で、それ以外が0という風に使われる。これをone-hot vectorという。&lt;/p&gt;

&lt;h3 id=&#34;softmax-regressions&#34;&gt;Softmax Regressions&lt;/h3&gt;

&lt;p&gt;Softmaxはいくつかの異なるもの(今回でいうと数字)に確率を割り当てる。&lt;/p&gt;

&lt;p&gt;画像が特定のクラス(0~9)に属するかどうか計算するために、ピクセルの強さに重みを付けた合計を計算する。
もし、そのピクセルがそのクラスに属さない根拠になるなら負の重みがかかり、属する根拠になるなら、正の重みがかかるようにする。
また合計にさらに入力と無関係なクラスごとに異なるバイアスを足す。&lt;/p&gt;

&lt;p&gt;全てのクラスで計算した値をsoftmax関数に入れ、それぞれ確率に変換する。この確率の和は1になるようになっている。&lt;/p&gt;

&lt;h3 id=&#34;implementing-the-regression&#34;&gt;Implementing the Regression&lt;/h3&gt;

&lt;p&gt;Pythonでは行列の積のような重い処理をするとき、NumPyのようなライブラリを使ってPythonの外で行うが、
外からPythonに戻るときにオーバーヘッドが発生してしまう。
TensorFlowでも重い処理を外で行うが、オーバーヘッドを避けるために、
単体の重い処理をPythonから独立して実行する代わりに、Pythonの外側で実行される関連した処理のグラフを記述させる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
x = tf.placeholder(tf.float32, [None, 784])
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
y = tf.nn.softmax(tf.matmul(x, W) + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf.placeholder&lt;/code&gt;は実行時に与えられる値で、今回が画像データ。
W(重み)とb(バイアス)は学習する変数。
&lt;code&gt;tf.matmul(x, W) + b&lt;/code&gt;の部分が重みを付けた合計にバイアスを足したものに対応している。
matmulはmatrix multiple、つまり行列の積。&lt;/p&gt;

&lt;h3 id=&#34;training&#34;&gt;Training&lt;/h3&gt;

&lt;p&gt;機械学習では一般的に、悪いモデルとは何か定義し、それを最小化しようとする。
一般的で、良い誤差関数として&amp;rdquo;cross-entropy&amp;rdquo;というものがある。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;y_ = tf.placeholder(tf.float32, [None, 10])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;y_が正しい答えで、これとy(softmaxで求めた各数字の確率)の対数の積をを次元ごとにとり、それらの和を求めて-1を掛けている。
&lt;code&gt;reduction_indices=[1]&lt;/code&gt;というのは[784, 10]の10の方を指している模様。
全ての学習データにおいてこれを求め、さらにそれらの平均をとり、これをcross_entropyとしている。
この値はy_とyが離れていれば大きくなるので、なるべく小さくすることが良いモデルにするということになる。&lt;/p&gt;

&lt;p&gt;ではどうやってこの値を小さくするか、TensorFlowは関係する計算のグラフを持っているので、自動的に&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%83%E3%82%AF%E3%83%97%E3%83%AD%E3%83%91%E3%82%B2%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3&#34;&gt;バックプロパゲーション&lt;/a&gt;を使って、どの変数がcross_entropyに影響しているか効率的に特定することができる。&lt;/p&gt;

&lt;p&gt;以下のようにGradientDescent(勾配降下)Optimizerで0.5のleartning rateでcross_entropyが小さくなるように、変数を少しずつ変えていく。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを使って学習していく。学習とテストに使うデータは&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/mnist/input_data.py&#34;&gt;これ&lt;/a&gt;
と&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&amp;quot;MNIST_data/&amp;quot;, one_hot=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;を実行すれば用意できるようになっている。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tf.Session()&lt;/code&gt;と&lt;code&gt;tf.run()&lt;/code&gt;はSessionを取得し、モデルを実行するもの。
変数は&lt;code&gt;tf.initialize_all_variables()&lt;/code&gt;で初期化する必要がある。
batch_xsが画像のピクセルデータで、batch_ysが正しい答え。
&lt;code&gt;sess.run()&lt;/code&gt;の&lt;code&gt;feed_dict={x: batch_xs, y_: batch_ys}&lt;/code&gt;はそれぞれ対応するplaceholderのところに与えられる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
for i in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このような小さいランダムなデータを使うのはstochastic(確率的) trainingと呼ばれていて、
今回はstochastic gradient descent。
理想的には全てのデータを全ての訓練のステップで使いたいが、コストがかかるので代わりに毎回異なるサブセットを使うことで
同じ効果を得ている。&lt;/p&gt;

&lt;h3 id=&#34;evaluating-our-model&#34;&gt;Evaluating Our Model&lt;/h3&gt;

&lt;p&gt;モデルがどのくらい良いかを測る。&lt;/p&gt;

&lt;p&gt;以下のcorrect_predictionでは&lt;code&gt;tf.argmax&lt;/code&gt;で最も数値の大きい、つまり確率の高いラベルを取得し、これが正解のものと一致するかというのを
画像データごとに比較している。
結果、[True, False, True, True]であるなら、これを[1, 0, 1, 1]にキャストし、平均を取ったものがaccuracy、正解率となる。
そしてこの値をテスト用のデータで出力している。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
&amp;gt; 0.9206
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この簡単なモデルだと正解率は92%になったが、実はこれは非常に悪いとのこと。
モデルにもう少し変更を加えるだけで97%になったりするらしい。&lt;/p&gt;

&lt;p&gt;続き: &lt;a href=&#34;https://www.sambaiz.net/article/6&#34;&gt;TensorFlow チュートリアル2(Deep MNIST for Experts)&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
