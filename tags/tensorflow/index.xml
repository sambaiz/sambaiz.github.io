<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sambaiz.net</title>
    <link>http://sambaiz.net/tags/tensorflow/</link>
    <language>ja</language>
    <author>sambaiz</author>
    <rights>(C) 2016</rights>
    <updated>2016-09-10 14:46:00 &#43;0900 JST</updated>

    
      
        <item>
          <title>DeepDreaming with TensorFlowをやる(2)</title>
          <link>http://sambaiz.net/article/21/</link>
          <pubDate>Sat, 10 Sep 2016 14:46:00 &#43;0900</pubDate>
          <author></author>
          <guid>http://sambaiz.net/article/21/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;http://sambaiz.net/article/20&#34;&gt;前回&lt;/a&gt;の続き。&lt;/p&gt;

&lt;h2 id=&#34;multiscale-image-generation&#34;&gt;Multiscale image generation&lt;/h2&gt;

&lt;p&gt;様々なスケールで勾配上昇させる。小さなスケールで上昇させたものをより大きなスケールでさらに上昇させていく。
ただ、壁紙のようなサイズを生成するような場合にそれを行うと、GPUのメモリを食いつぶしてしまう。
これを避けるために、画像を小さなタイルに分割し、それぞれ独立に勾配を計算する。
また、毎回画像をランダムにシフトしていくことで、タイルに見えることを避け、画像全体の品質を向上させる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def tffunc(*argtypes):
    &#39;&#39;&#39;Helper that transforms TF-graph generating function into a regular one.
    See &amp;quot;resize&amp;quot; function below.
    &#39;&#39;&#39;
    placeholders = list(map(tf.placeholder, argtypes))
    def wrap(f):
        out = f(*placeholders)
        def wrapper(*args, **kw):
            return out.eval(dict(zip(placeholders, args)), session=kw.get(&#39;session&#39;))
        return wrapper
    return wrap

# Helper function that uses TF to resize an image
def resize(img, size):
    img = tf.expand_dims(img, 0)
    return tf.image.resize_bilinear(img, size)[0,:,:,:]
resize = tffunc(np.float32, np.int32)(resize)


def calc_grad_tiled(img, t_grad, tile_size=512):
    &#39;&#39;&#39;Compute the value of tensor t_grad over the image in a tiled way.
    Random shifts are applied to the image to blur tile boundaries over
    multiple iterations.&#39;&#39;&#39;
    sz = tile_size
    h, w = img.shape[:2]
    sx, sy = np.random.randint(sz, size=2)
    img_shift = np.roll(np.roll(img, sx, 1), sy, 0)
    grad = np.zeros_like(img)
    for y in range(0, max(h-sz//2, sz),sz):
        for x in range(0, max(w-sz//2, sz),sz):
            sub = img_shift[y:y+sz,x:x+sz]
            g = sess.run(t_grad, {t_input:sub})
            grad[y:y+sz,x:x+sz] = g
    return np.roll(np.roll(grad, -sx, 1), -sy, 0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf.image.resize_bilinear&lt;/code&gt;は&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/api_docs/python/image.html#resize_bilinear&#34;&gt;双線形補間によってリサイズする。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;numpy.roll&lt;/code&gt;は&lt;a href=&#34;http://docs.scipy.org/doc/numpy/reference/generated/numpy.roll.html&#34;&gt;配列を第三引数axisによってローリングする&lt;/a&gt;。
axisを指定しない場合、フラットなものとして扱われる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;hoge = [[0, 1, 2],
        [3, 4, 5],
        [6, 7, 8]]

print(np.roll(hoge, 1))
# [[8 0 1]
#  [2 3 4]
#  [5 6 7]]

print(np.roll(hoge, 1, axis=0))
# [[6 7 8]
#  [0 1 2]
#  [3 4 5]]

print(np.roll(hoge, 1, axis=1))
# [[2 0 1]
#  [5 3 4]
#  [8 6 7]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;つまり、&lt;code&gt;calc_grad_tiled&lt;/code&gt;では、ランダムにローリングして、タイルに分割して勾配を求め、ローリングした分を戻して返している。
これと、画像サイズを&lt;code&gt;octave_scale&lt;/code&gt;倍にしていく以外は前回やったのと基本的に同じだ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def render_multiscale(t_obj, img0=img_noise, iter_n=10, step=1.0, octave_n=3, octave_scale=1.4):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!

    img = img0.copy()
    for octave in range(octave_n):
        if octave&amp;gt;0:
            hw = np.float32(img.shape[:2])*octave_scale
            img = resize(img, np.int32(hw))
        for i in range(iter_n):
            g = calc_grad_tiled(img, t_grad)
            # normalizing the gradient, so the same step size should work
            g /= g.std()+1e-8         # for different layers and networks
            img += g*step
            print(&#39;.&#39;, end = &#39; &#39;)
        clear_output()
        showarray(visstd(img))

render_multiscale(T(layer)[:,:,:,channel])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;laplacian-pyramid-gradient-normalization&#34;&gt;Laplacian Pyramid Gradient Normalization&lt;/h2&gt;

&lt;p&gt;結果の画像は、高い周波数(ピクセルの変化の度合が高い)が多く含まれている。
これを改善するための一つの方法として、毎回画像をぼかし、高周波数を抑え、画像を滑らかにするものがある。
ただ、この方法は良い画像にするためにより多くの繰り返しが必要になってしまう。
逆に、低周波数を上げるのは、ラプラシアンピラミッドを使う方法があって、これで勾配を正規化する。&lt;/p&gt;

&lt;p&gt;ラプラシアンピラミッドというのは、ガウシアンピラミッドにおける、ある解像度の画像と、
その一つレベルの高い(解像度1/2 * &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; = &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;)画像をアップサンプルしたものの差分だ。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://labs.eecs.tottori-u.ac.jp/sd/Member/oyamada/OpenCV/html/py_tutorials/py_imgproc/py_pyramids/py_pyramids.html&#34;&gt;画像ピラミッド — OpenCV-Python Tutorials 1 documentation&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;k = np.float32([1,4,6,4,1])
k = np.outer(k, k)
# [[  1.   4.   6.   4.   1.]
#  [  4.  16.  24.  16.   4.]
#  [  6.  24.  36.  24.   6.]
#  [  4.  16.  24.  16.   4.]
#  [  1.   4.   6.   4.   1.]]

k5x5 = k[:,:,None,None]/k.sum()*np.eye(3, dtype=np.float32)

print(len(k5x5))
# 5

print(k5x5[0])
# [[[ 0.00390625  0.          0.        ]
#  [ 0.          0.00390625  0.        ]
#  [ 0.          0.          0.00390625]]

# [[ 0.015625    0.          0.        ]
#  [ 0.          0.015625    0.        ]
#  [ 0.          0.          0.015625  ]]

# [[ 0.0234375   0.          0.        ]
#  [ 0.          0.0234375   0.        ]
#  [ 0.          0.          0.0234375 ]]

# [[ 0.015625    0.          0.        ]
#  [ 0.          0.015625    0.        ]
#  [ 0.          0.          0.015625  ]]

# [[ 0.00390625  0.          0.        ]
#  [ 0.          0.00390625  0.        ]
#  [ 0.          0.          0.00390625]]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;numpy.outer&lt;/code&gt;は&lt;a href=&#34;http://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html&#34;&gt;外積を求める&lt;/a&gt;もので、
&lt;code&gt;numpy.eye&lt;/code&gt;は&lt;a href=&#34;http://docs.scipy.org/doc/numpy/reference/generated/numpy.eye.html&#34;&gt;対角線が1で、それ以外は0の2次元行列を返す&lt;/a&gt;。
&lt;code&gt;k&lt;/code&gt;を指定すると、対角線の位置を変更できるが、指定していない場合はNxNの単位行列が返ることになる。
このフィルターで畳み込むことで、ラプラシアンピラミッドの1レベル高い画像に変換できる。
&lt;code&gt;tf.nn.conv2d_transpose&lt;/code&gt;は
&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#conv2d_transpose&#34;&gt;畳み込みの逆処理&lt;/a&gt;のようなもので、
これでアップサンプルした画像と元画像の差分を取って、ラプラシアンピラミッドを生成している。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def lap_split(img):
    &#39;&#39;&#39;Split the image into lo and hi frequency components&#39;&#39;&#39;
    with tf.name_scope(&#39;split&#39;):
        lo = tf.nn.conv2d(img, k5x5, [1,2,2,1], &#39;SAME&#39;)
        lo2 = tf.nn.conv2d_transpose(lo, k5x5*4, tf.shape(img), [1,2,2,1])
        hi = img-lo2
    return lo, hi

def lap_split_n(img, n):
    &#39;&#39;&#39;Build Laplacian pyramid with n splits&#39;&#39;&#39;
    levels = []
    for i in range(n):
        img, hi = lap_split(img)
        levels.append(hi)
    levels.append(img)
    return levels[::-1]

def lap_merge(levels):
    &#39;&#39;&#39;Merge Laplacian pyramid&#39;&#39;&#39;
    img = levels[0]
    for hi in levels[1:]:
        with tf.name_scope(&#39;merge&#39;):
            img = tf.nn.conv2d_transpose(img, k5x5*4, tf.shape(hi), [1,2,2,1]) + hi
    return img

def normalize_std(img, eps=1e-10):
    &#39;&#39;&#39;Normalize image by making its standard deviation = 1.0&#39;&#39;&#39;
    with tf.name_scope(&#39;normalize&#39;):
        std = tf.sqrt(tf.reduce_mean(tf.square(img)))
        return img/tf.maximum(std, eps)

def lap_normalize(img, scale_n=4):
    &#39;&#39;&#39;Perform the Laplacian pyramid normalization.&#39;&#39;&#39;
    img = tf.expand_dims(img,0)
    tlevels = lap_split_n(img, scale_n)
    tlevels = list(map(normalize_std, tlevels))
    out = lap_merge(tlevels)
    return out[0,:,:,:]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;lap_normalize&lt;/code&gt;で画像からラプラシアンピラミッドを生成し、それぞれで正規化してからマージして元の画像に戻す処理をしている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def render_lapnorm(t_obj, img0=img_noise, visfunc=visstd,
                   iter_n=10, step=1.0, octave_n=3, octave_scale=1.4, lap_n=4):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!
    # build the laplacian normalization graph
    lap_norm_func = tffunc(np.float32)(partial(lap_normalize, scale_n=lap_n))

    img = img0.copy()
    for octave in range(octave_n):
        if octave&amp;gt;0:
            hw = np.float32(img.shape[:2])*octave_scale
            img = resize(img, np.int32(hw))
        for i in range(iter_n):
            g = calc_grad_tiled(img, t_grad)
            g = lap_norm_func(g)
            img += g*step
            print(&#39;.&#39;, end = &#39; &#39;)
        clear_output()
        showarray(visfunc(img))

render_lapnorm(T(layer)[:,:,:,channel])
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;deepdream&#34;&gt;DeepDream&lt;/h2&gt;

&lt;p&gt;で、これがDeepDreamのアルゴリズム。
ラプラシアンピラミッドを生成して、リサイズの際に次のレベルのを足していっている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def render_deepdream(t_obj, img0=img_noise,
                     iter_n=10, step=1.5, octave_n=4, octave_scale=1.4):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!

    # split the image into a number of octaves
    img = img0
    octaves = []
    for i in range(octave_n-1):
        hw = img.shape[:2]
        lo = resize(img, np.int32(np.float32(hw)/octave_scale))
        hi = img-resize(lo, hw)
        img = lo
        octaves.append(hi)

    # generate details octave by octave
    for octave in range(octave_n):
        if octave&amp;gt;0:
            hi = octaves[-octave]
            img = resize(img, hi.shape[:2])+hi
        for i in range(iter_n):
            g = calc_grad_tiled(img, t_grad)
            img += g*(step / (np.abs(g).mean()+1e-7))
            print(&#39;.&#39;,end = &#39; &#39;)
        clear_output()
        showarray(img/255.0)
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>DeepDreaming with Tensorflowをやる(1)</title>
          <link>http://sambaiz.net/article/20/</link>
          <pubDate>Wed, 07 Sep 2016 01:06:00 &#43;0900</pubDate>
          <author></author>
          <guid>http://sambaiz.net/article/20/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/examples/tutorials/deepdream/deepdream.ipynb&#34;&gt;https://github.com/tensorflow/tensorflow/blob/r0.10/tensorflow/examples/tutorials/deepdream/deepdream.ipynb&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;例の通りまとめながら進めていく。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;このノートブックは、畳み込みニューラルネットワークによる画像生成の手法を説明するものだ。
ネットワークは入力画像へ変換させる配列のレイヤーの集合から成り立っている。
変換のパラメータは勾配降下法で変形しながら学習していく。
内部的な画像の表現は意味不明なように見えるが、可視化し、解釈することができる。&lt;/p&gt;

&lt;h3 id=&#34;loading-and-displaying-the-model-graph&#34;&gt;Loading and displaying the model graph&lt;/h3&gt;

&lt;p&gt;学習済みネットワークのprotobufファイルが用意されていて、これをダウンロードして使う。
ただ&lt;code&gt;gcr.io/tensorflow/tensorflow&lt;/code&gt;にwgetもunzipも入っていなかったので、中に入ってapt-getした。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;model_fn = &#39;tensorflow_inception_graph.pb&#39;

# creating TensorFlow session and loading the model
graph = tf.Graph()
sess = tf.InteractiveSession(graph=graph)
with tf.gfile.FastGFile(model_fn, &#39;rb&#39;) as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
t_input = tf.placeholder(np.float32, name=&#39;input&#39;) # define the input tensor
imagenet_mean = 117.0
t_preprocessed = tf.expand_dims(t_input-imagenet_mean, 0)
tf.import_graph_def(graph_def, {&#39;input&#39;:t_preprocessed})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf.gfile.FastGFile&lt;/code&gt;のドキュメントが見つからないので
&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/568092d4507996d8aff0c46d6c57488a26596dd5/tensorflow/python/platform/gfile.py#L218&#34;&gt;ソース&lt;/a&gt;
を探したところFile I/Oのラッパーのようだ。これでprotobufファイルを読み、ParseFromStringでGraphDefにする。&lt;/p&gt;

&lt;p&gt;さらにこれと入力データを&lt;code&gt;tf.import_graph_def&lt;/code&gt;に
渡すことで&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/api_docs/python/framework.html#import_graph_def&#34;&gt;Graphに取り込む&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tf.expand_dims&lt;/code&gt;は&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/api_docs/python/array_ops.html#expand_dims&#34;&gt;値が1の次元を指定の場所に挿入する&lt;/a&gt;
もの。なんでそんなことをしたり、&lt;code&gt;imagenet_mean&lt;/code&gt;を引いているのかは説明がなかった。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;layers = [op.name for op in graph.get_operations() if op.type==&#39;Conv2D&#39; and &#39;import/&#39; in op.name]
feature_nums = [int(graph.get_tensor_by_name(name+&#39;:0&#39;).get_shape()[-1]) for name in layers]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このlayersに入っているのはこんな感じ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import/conv2d0_pre_relu/conv
import/conv2d1_pre_relu/conv
import/conv2d2_pre_relu/conv
import/mixed3a_1x1_pre_relu/conv
import/mixed3a_3x3_bottleneck_pre_relu/conv
import/mixed3a_3x3_pre_relu/conv
import/mixed3a_5x5_bottleneck_pre_relu/conv
import/mixed3a_5x5_pre_relu/conv
...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これらのレイヤーのうち、&lt;code&gt;mixed4d_3x3_bottleneck_pre_relu&lt;/code&gt;を可視化してみる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;layer = &#39;mixed4d_3x3_bottleneck_pre_relu&#39;
channel = 139 # picking some feature channel to visualize

def T(layer):
    &#39;&#39;&#39;Helper for getting layer output tensor&#39;&#39;&#39;
    return graph.get_tensor_by_name(&amp;quot;import/%s:0&amp;quot;%layer)

render_naive(T(layer)[:,:,:,channel])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;mixed4d_3x3_bottleneck_pre_relu&#39;&lt;/code&gt;は144チャンネルのフィルターで、今回はそのうち139番目のチャンネルを選んでいる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print(T(layer))
-&amp;gt; Tensor(&amp;quot;import/mixed4d_3x3_bottleneck_pre_relu:0&amp;quot;, shape=(?, ?, ?, 144), dtype=float32, device=/device:CPU:0)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;初期値はRGB100(グレー)にノイズを加えた画像。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# start with a gray image with a little noise
img_noise = np.random.uniform(size=(224,224,3)) + 100.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;スコアはそのチャンネルの値の平均で、これが高くなるように画像を変化させていく。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def render_naive(t_obj, img0=img_noise, iter_n=20, step=1.0):
    t_score = tf.reduce_mean(t_obj) # defining the optimization objective
    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!

    img = img0.copy()
    for i in range(iter_n):
        g, score = sess.run([t_grad, t_score], {t_input:img})
        # normalizing the gradient, so the same step size should work
        g /= g.std()+1e-8         # for different layers and networks
        img += g*step
        print(score, end = &#39; &#39;)
    clear_output()
    showarray(visstd(img))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf.gradients(ys,xs)&lt;/code&gt;で
&lt;a href=&#34;https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#gradients&#34;&gt;xそれぞれで偏微分したyの和&lt;/a&gt;が得られる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;a = tf.Variable(tf.constant([
            [1., 2.],
            [3., 4.]]))
b = tf.Variable(tf.constant([
            [2., 3.],
            [4., 5.]]))
c = tf.matmul(a, b)
​
grad = tf.gradients(c, a)[0]
init = tf.initialize_all_variables()
with tf.Session() as sess:
    sess.run(init)

    print(sess.run(c))
    # [[ 10.  13.]
    # [ 22.  29.]]

    print(sess.run(grad))
    # [[ 5.  9.]
    # [ 5.  9.]]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;入力画像にこれをを加算していくと、その状態からスコアが上がるパラメータが増え、下がるパラメータが減るため、勾配を上っていくことになる。
スコアが上昇するに従って、そのフィルターによる模様が浮かんできた。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://sambaiz.net/article/21&#34;&gt;続く&lt;/a&gt;。&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Tensorflowの学習データを使ったAPIを作る</title>
          <link>http://sambaiz.net/article/13/</link>
          <pubDate>Fri, 05 Aug 2016 22:08:00 &#43;0900</pubDate>
          <author></author>
          <guid>http://sambaiz.net/article/13/</guid>
          <description>

&lt;p&gt;チュートリアルのMNISTの学習データを使って、手書き数字画像のデータを受け取り、数字を返すAPIを作る。
コードは&lt;a href=&#34;https://github.com/sambaiz/tensorflow-use-api-sample&#34;&gt;ここ&lt;/a&gt;にある。&lt;/p&gt;

&lt;h2 id=&#34;学習して結果を保存する&#34;&gt;学習して結果を保存する&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://sambaiz.net/article/6&#34;&gt;前回&lt;/a&gt;の学習結果のチェックポイントファイルを出力する。
&lt;code&gt;tf.train.Saver().save&lt;/code&gt;でnameで対応するVariableの値が保存できる。&lt;/p&gt;

&lt;p&gt;今回は、学習側、アプリケーション側共にPythonを使うので、以下のようなクラスを作った。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
from tensorflow.examples.tutorials.mnist import input_data

class Mnist:

    def __init__(self):

        g = tf.Graph()

        with g.as_default():

            W_conv1 = self._weight_variable([5, 5, 1, 32],  &amp;quot;W_conv1&amp;quot;)
            b_conv1 = self._bias_variable([32],  &amp;quot;b_conv1&amp;quot;)

            self._x = tf.placeholder(tf.float32, [None, 784])
            x_image = tf.reshape(self._x, [-1,28,28,1])

            h_conv1 = tf.nn.relu(self._conv2d(x_image, W_conv1) + b_conv1)
            h_pool1 = self._max_pool_2x2(h_conv1)

            W_conv2 = self._weight_variable([5, 5, 32, 64],  &amp;quot;W_conv2&amp;quot;)
            b_conv2 = self._bias_variable([64],  &amp;quot;b_conv2&amp;quot;)

            h_conv2 = tf.nn.relu(self._conv2d(h_pool1, W_conv2) + b_conv2)
            h_pool2 = self._max_pool_2x2(h_conv2)

            W_fc1 = self._weight_variable([7 * 7 * 64, 1024],  &amp;quot;W_fc1&amp;quot;)
            b_fc1 = self._bias_variable([1024],  &amp;quot;b_fc1&amp;quot;)

            h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
            h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

            self._keep_prob = tf.placeholder(tf.float32)
            h_fc1_drop = tf.nn.dropout(h_fc1, self._keep_prob)

            W_fc2 = self._weight_variable([1024, 10],  &amp;quot;W_fc2&amp;quot;)
            b_fc2 = self._bias_variable([10],  &amp;quot;b_fc2&amp;quot;)

            y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
            self._what_number = tf.argmax(y_conv, 1)

            self._y_ = tf.placeholder(tf.float32, [None, 10])
            cross_entropy = tf.reduce_mean(-tf.reduce_sum(self._y_ * tf.log(y_conv), reduction_indices=[1]))
            self._train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
            correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(self._y_,1))
            self._accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

            self.sess = tf.Session()
            init = tf.initialize_all_variables()
            self.sess.run(init)
            self._saver = tf.train.Saver()

    def save(self, ckpt_file_name):
        self._saver.save(self.sess, ckpt_file_name)

    def restore(self, ckpt_file_name):
        self._saver.restore(self.sess, ckpt_file_name)

    def what_number(self, image_array):
        return self.sess.run(self._what_number, feed_dict={self._x: image_array, self._keep_prob: 1.0})

    def train(self, num):
        if not hasattr(self, &amp;quot;_mnist&amp;quot;):
            self._mnist = input_data.read_data_sets(&amp;quot;MNIST_data/&amp;quot;, one_hot=True)

        for i in range(num):
            batch = self._mnist.train.next_batch(50)
            if i%100 == 0:
              train_accuracy = self._accuracy.eval(session=self.sess, feed_dict={
                      self._x:batch[0], self._y_: batch[1], self._keep_prob: 1.0
                  })
              print(&amp;quot;step %d, training accuracy %g&amp;quot;%(i, train_accuracy))
            self.sess.run(self._train_step, feed_dict={self._x: batch[0], self._y_: batch[1], self._keep_prob: 0.5})

        print(&amp;quot;test accuracy %g&amp;quot;%self._accuracy.eval(session=self.sess, feed_dict={
                self._x: self._mnist.test.images, self._y_: self._mnist.test.labels, self._keep_prob: 1.0
            }))

    def close(self):
        self.sess.close()

    def _weight_variable(self, shape, name):
      initial = tf.truncated_normal(shape, stddev=0.1)
      return tf.Variable(initial, name=name)

    def _bias_variable(self, shape, name):
      initial = tf.constant(0.1, shape=shape)
      return tf.Variable(initial, name=name)

    def _conv2d(self, x, W):
      return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)

    def _max_pool_2x2(self, x):
      return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                            strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;train&lt;/code&gt;で学習し、&lt;code&gt;save&lt;/code&gt;でチェックポイントファイルを保存できる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from mnist import Mnist

mnist = Mnist()
mnist.train(20000)
mnist.save(&amp;quot;model.ckpt&amp;quot;)
mnist.close()
print(&amp;quot;done&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;なので、例えばこんなDockerfileを書いて&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM gcr.io/tensorflow/tensorflow

ADD training.py /
ADD mnist.py /

CMD python /training.py &amp;amp;&amp;amp; /bin/bash
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;しばらく待つ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t training .
$ docker run -itd training
$ docker logs -f &amp;lt;CONTAINER_ID&amp;gt;
$ docker cp &amp;lt;CONTAINER_ID&amp;gt;:/notebooks/model.ckpt .
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;学習データを使う&#34;&gt;学習データを使う&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;tf.train.Saver().restore&lt;/code&gt;でチェックポイントファイルを読み、Variableの値を復元できる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from flask import Flask, request, jsonify
import tensorflow as tf
from mnist import Mnist

app = Flask(__name__)

mnist = Mnist()
mnist.restore(&amp;quot;/model.ckpt&amp;quot;)

@app.route(&amp;quot;/&amp;quot;, methods=[&#39;POST&#39;])
def what_number():

    json = request.json
    if(json is None or &amp;quot;image&amp;quot; not in json or len(json[&amp;quot;image&amp;quot;]) != 784):
        return jsonify(error=&amp;quot;Need json includes image property which is 784(28 * 28) length, float([0, 1.0]) array&amp;quot;)
    else:
        result = list(mnist.what_number([json[&amp;quot;image&amp;quot;]]))
        return jsonify(result=result[0])

if __name__ == &amp;quot;__main__&amp;quot;:
    app.run(port=3000, host=&#39;0.0.0.0&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これもDockerで動かすならこんな感じ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM gcr.io/tensorflow/tensorflow

ADD requirements.txt /tmp
ADD model.ckpt /
ADD training/mnist.py /
ADD app.py /

RUN pip install -q -r /tmp/requirements.txt

EXPOSE 3000

CMD [&amp;quot;python&amp;quot;, &amp;quot;/app.py&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ docker build -t tensor_api .
$ docker run -itd -p 3000:3000 tensor_app
$ docker logs -f &amp;lt;CONTAINER_ID&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これに以下のようにして28*28の画像のデータを渡すと、&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;image&amp;quot;: [ ..., 0.32941177, 0.72549021, 0.62352943, ...]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;それが何の数字なのかが返ってくる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;result&amp;quot;: 7
}
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>TensorFlow チュートリアル2(Deep MNIST for Experts)</title>
          <link>http://sambaiz.net/article/6/</link>
          <pubDate>Tue, 12 Jul 2016 21:16:00 &#43;0900</pubDate>
          <author></author>
          <guid>http://sambaiz.net/article/6/</guid>
          <description>

&lt;p&gt;&lt;a href=&#34;http://sambaiz.net/article/3&#34;&gt;前回&lt;/a&gt;に引き続き、まとめながら進めていく。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/tutorials/mnist/pros/index.html&#34;&gt;Deep MNIST for Experts&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;start-tensorflow-interactivesession&#34;&gt;Start TensorFlow InteractiveSession&lt;/h3&gt;

&lt;p&gt;今回は、前回のようにグラフを作成してからSessionを開始する代わりに
&lt;code&gt;InteractiveSession&lt;/code&gt;を使う。
グラフを作成し実行するのをインタラクティブに行うことができ、IPythonのような環境で便利だ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
sess = tf.InteractiveSession()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;build-a-multilayer-convolutional-network&#34;&gt;Build a Multilayer Convolutional Network&lt;/h3&gt;

&lt;p&gt;前回のシンプルなモデルでは、あまり良い結果が出なかった。
そこで、今回はもう少し洗練されたモデル、小さな畳み込みニューラルネットワークを作成する。&lt;/p&gt;

&lt;h3 id=&#34;weight-initialization&#34;&gt;Weight Initialization&lt;/h3&gt;

&lt;p&gt;このモデルを作成するためには、たくさんの重みとバイアスを作成する必要がある。&lt;/p&gt;

&lt;p&gt;重みは、対称性を破り、勾配0を避けるために、少しのノイズで初期化すべきだ。&lt;/p&gt;

&lt;p&gt;また、&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E6%B4%BB%E6%80%A7%E5%8C%96%E9%96%A2%E6%95%B0#ReLU.EF.BC.88.E3.83.A9.E3.83.B3.E3.83.97.E9.96.A2.E6.95.B0.EF.BC.89&#34;&gt;ReLU(Rectified Linear Unit, 正規化線形関数)&lt;/a&gt;ニューロンを使うので、&amp;rdquo;死んだニューロン&amp;rdquo;を避けるためにわずかな正の値のバイアスで初期化すると良い。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tf.truncated_normal&lt;/code&gt;は&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/api_docs/python/constant_op.html#truncated_normal&#34;&gt;正規分布で、μ±2σ範囲内のランダムな値を返す。&lt;/a&gt;
以下の例だと、&lt;code&gt;mean&lt;/code&gt;のデフォルトが0.0なので、正規分布 &lt;code&gt;N(0, 0.01)&lt;/code&gt;の、&lt;code&gt;-0.2&amp;lt;=x&amp;lt;=0.2&lt;/code&gt;な値がランダムに返ることになる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;convolution-and-pooling&#34;&gt;Convolution and Pooling&lt;/h3&gt;

&lt;p&gt;TensorFlowは柔軟な畳み込みとプーリングの手続きを提供している。&lt;/p&gt;

&lt;p&gt;畳み込みというのは、画像に対してフィルターを少しずつ動かしながら掛けていく処理のことだ。&lt;a href=&#34;http://www.clg.niigata-u.ac.jp/~medimg/practice_medical_imaging/imgproc_scion/4filter/index.htm&#34;&gt;このページ&lt;/a&gt;が分かりやすい。
例えば、&lt;a href=&#34;https://en.wikipedia.org/wiki/Sobel_operator&#34;&gt;ソーベルフィルタ&lt;/a&gt;で輪郭になっているところを抽出するように、
フィルターの値によって、その区域における、ある特徴を際立たせたりすることができる。
今回はこのフィルターが重みとなり、際立たせたいのはその数字を識別するための特徴ということになる。
前回は画像を一次元の配列として扱い重みを学習していたので、縦の情報が失われていたが、この方法ではそれがない。&lt;/p&gt;

&lt;p&gt;プーリングというのは画像から区域ごとにサンプリングする処理だ。最大プーリングや、平均プーリングなどの手法がある。
畳み込みのように順番に区域を見ていって、最大プーリングならそのうちの最大のものを採用し、他のものは無視する。
サイズが小さくなるだけではなく、ちょっとした位置のずれを吸収することができる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=&#39;SAME&#39;)

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding=&#39;SAME&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf.nn.conv2d&lt;/code&gt;は&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#conv2d&#34;&gt;畳み込みを行う。&lt;/a&gt;
主な入力は&lt;code&gt;[画像の数, 縦サイズ, 横サイズ, チャンネル数(色とか)]&lt;/code&gt;の画像と、
&lt;code&gt;[縦サイズ, 横サイズ, 入力チャンネル数, 出力チャンネル数]&lt;/code&gt;のフィルターだ。
&lt;code&gt;strides&lt;/code&gt;は一度にどれくらいフィルターを動かしていくかで、&lt;code&gt;strides[1]&lt;/code&gt;が縦、&lt;code&gt;strides[2]&lt;/code&gt;が横に動かす量だ。
&lt;code&gt;strides[0]&lt;/code&gt;と&lt;code&gt;strides[3]&lt;/code&gt;は1でなくてはならない。
&lt;code&gt;padding&lt;/code&gt;は&amp;rdquo;SAME&amp;rdquo;か&amp;rdquo;VALID&amp;rdquo;から選択できるパディングについての設定だ。詳細は
&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#convolution&#34;&gt;ここ&lt;/a&gt;
に書いてある。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tf.nn.max_pool&lt;/code&gt;は&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#max_pool&#34;&gt;最大プーリングを行う。&lt;/a&gt;
&lt;code&gt;ksize&lt;/code&gt;は入力のそれぞれの次元に対応した見ていく区域のサイズだ。&lt;code&gt;[1, 2, 2, 1]&lt;/code&gt;ならそれぞれの画像を&lt;code&gt;2*2&lt;/code&gt;ずつ見ていくことになる。&lt;/p&gt;

&lt;h3 id=&#34;first-convolutional-layer&#34;&gt;First Convolutional Layer&lt;/h3&gt;

&lt;p&gt;最初のレイヤーは、畳み込みと最大プーリングで構成される。
畳み込みはそれぞれ&lt;code&gt;5*5&lt;/code&gt;の32チャンネル出力のフィルターでされる。
つまり、重みであるフィルターは&lt;code&gt;[5, 5, 1, 32]&lt;/code&gt;のtensorということになる。
そして、それぞれのチャンネルに対してバイアスが存在する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;W_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;画像に対して、畳み込みを適用するするために&lt;code&gt;tf.reshape&lt;/code&gt;で
&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/api_docs/python/array_ops.html#reshape&#34;&gt;変形する&lt;/a&gt;
必要がある。-1というのは特別な値で、他の次元との積が合計が元のものと変わらないように決定される。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = tf.placeholder(tf.float32, [None, 784])
x_image = tf.reshape(x, [-1,28,28,1])
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これで畳み込めるようになったので、画像と重みを畳み込み、バイアスを足したものにReLUを適用した後、最大プーリングする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;second-convolutional-layer&#34;&gt;Second Convolutional Layer&lt;/h3&gt;

&lt;p&gt;deepネットワークにするために最初のレイヤーのようなものをいくつか重ねる。
2番目のレイヤーでは64チャンネル出力のフィルターで畳み込みを行い、プーリングする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;W_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;densely-connected-layer&#34;&gt;Densely Connected Layer&lt;/h3&gt;

&lt;p&gt;2つのレイヤーを経て元々&lt;code&gt;28*28&lt;/code&gt;だった画像は&lt;code&gt;7*7&lt;/code&gt;にまで削減された。
2つめのレイヤーの出力は64チャンネルだったので、&lt;code&gt;7*7*64&lt;/code&gt;次元のデータになっている。
このレイヤーでは、これらにそれぞれ1024のニューロンを結びつける。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;W_fc1 = weight_variable([7 * 7 * 64, 1024])
b_fc1 = bias_variable([1024])

h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;dropout&#34;&gt;Dropout&lt;/h3&gt;

&lt;p&gt;過学習を防ぐために
&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0#.E3.83.89.E3.83.AD.E3.83.83.E3.83.97.E3.82.A2.E3.82.A6.E3.83.88&#34;&gt;ドロップアウト&lt;/a&gt;
を行う。
ドロップアウトというのは訓練データごとにニューロンを何割か無視することだ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;readout-layer&#34;&gt;Readout Layer&lt;/h3&gt;

&lt;p&gt;最後にsoftmaxでそれぞれの数字である確率を求める。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;W_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;train-and-evaluate-the-model&#34;&gt;Train and Evaluate the Model&lt;/h3&gt;

&lt;p&gt;今回は前回使った&lt;code&gt;GradientDescentOptimizer&lt;/code&gt;よりも洗練されている&lt;code&gt;AdamOptimizer&lt;/code&gt;というのが使われている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;y_ = tf.placeholder(tf.float32, [None, 10])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))
train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)
correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
sess.run(tf.initialize_all_variables())
for i in range(20000):
  batch = mnist.train.next_batch(50)
  if i%100 == 0:
    train_accuracy = accuracy.eval(feed_dict={
        x:batch[0], y_: batch[1], keep_prob: 1.0})
    print(&amp;quot;step %d, training accuracy %g&amp;quot;%(i, train_accuracy))
  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})

print(&amp;quot;test accuracy %g&amp;quot;%accuracy.eval(feed_dict={
    x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを実行するとこんな出力が得られた。
かなり時間がかかったのでここで打ち切ったが、およそ99.2%の正解率になるらしい。
前回が92%だったのに比べてもすごく良い値に見える。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;step 0, training accuracy 0.12
step 100, training accuracy 0.8
step 200, training accuracy 0.9
step 300, training accuracy 0.9
step 400, training accuracy 0.98
step 500, training accuracy 0.88
step 600, training accuracy 0.98
step 700, training accuracy 0.98
step 800, training accuracy 0.9
step 900, training accuracy 1
step 1000, training accuracy 0.98
step 1100, training accuracy 0.98
step 1200, training accuracy 1
step 1300, training accuracy 0.98
step 1400, training accuracy 0.94
step 1500, training accuracy 0.98
&lt;/code&gt;&lt;/pre&gt;
</description>
        </item>
      
    
      
        <item>
          <title>TensorFlow チュートリアルまで</title>
          <link>http://sambaiz.net/article/3/</link>
          <pubDate>Sun, 03 Jul 2016 23:37:00 &#43;0900</pubDate>
          <author></author>
          <guid>http://sambaiz.net/article/3/</guid>
          <description>

&lt;p&gt;Googleが公開した人工知能ライブラリTensorFlowを使ってみる。
&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html&#34;&gt;セットアップ&lt;/a&gt;方法はいくつか提供されているが、Dockerで動かすことにした。
Jupyter Notebookが立ち上がるのですぐに試せて良い。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://localhost:8888/tree&#34;&gt;http://localhost:8888/tree&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;公式のチュートリアルをまとめながら進めてみる。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/versions/r0.9/tutorials/mnist/beginners/index.html&#34;&gt;MNIST For ML Beginners&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&#34;the-mnist-data&#34;&gt;The MNIST Data&lt;/h3&gt;

&lt;p&gt;MNISTというのは0~9の書き数字の画像のデータセットのことで、これらを正しく分類するのが目的。&lt;/p&gt;

&lt;p&gt;それぞれの画像は一律28*28=784ピクセルで、それぞれのピクセルは0と1の間の数値(強さ)で表されている。
今回はこれの縦横を取っ払って784次元のベクトルとして扱っている。&lt;/p&gt;

&lt;p&gt;したがって、学習用の画像データは[55000, 784]のtensor(n次元の配列)で表される。
55000というのが画像の数で、784というのがそれぞれの画像の次元を意味している。&lt;/p&gt;

&lt;p&gt;それぞれの画像に対応した数字のラベルは[55000, 10]で表される。
10というのは、0~9それぞれに対応した次元のうち、一つだけ1で、それ以外が0という風に使われる。これをone-hot vectorという。&lt;/p&gt;

&lt;h3 id=&#34;softmax-regressions&#34;&gt;Softmax Regressions&lt;/h3&gt;

&lt;p&gt;Softmaxはいくつかの異なるもの(今回でいうと数字)に確率を割り当てる。&lt;/p&gt;

&lt;p&gt;画像が特定のクラス(0~9)に属するかどうか計算するために、ピクセルの強さに重みを付けた合計を計算する。
もし、そのピクセルがそのクラスに属さない根拠になるなら負の重みがかかり、属する根拠になるなら、正の重みがかかるようにする。
また合計にさらに入力と無関係なクラスごとに異なるバイアスを足す。&lt;/p&gt;

&lt;p&gt;全てのクラスで計算した値をsoftmax関数に入れ、それぞれ確率に変換する。この確率の和は1になるようになっている。&lt;/p&gt;

&lt;h3 id=&#34;implementing-the-regression&#34;&gt;Implementing the Regression&lt;/h3&gt;

&lt;p&gt;Pythonでは行列の積のような重い処理をするとき、NumPyのようなライブラリを使ってPythonの外で行うが、
外からPythonに戻るときにオーバーヘッドが発生してしまう。
TensorFlowでも重い処理を外で行うが、オーバーヘッドを避けるために、
単体の重い処理をPythonから独立して実行する代わりに、Pythonの外側で実行される関連した処理のグラフを記述させる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
x = tf.placeholder(tf.float32, [None, 784])
W = tf.Variable(tf.zeros([784, 10]))
b = tf.Variable(tf.zeros([10]))
y = tf.nn.softmax(tf.matmul(x, W) + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;code&gt;tf.placeholder&lt;/code&gt;は実行時に与えられる値で、今回が画像データ。
W(重み)とb(バイアス)は学習する変数。
&lt;code&gt;tf.matmul(x, W) + b&lt;/code&gt;の部分が重みを付けた合計にバイアスを足したものに対応している。
matmulはmatrix multiple、つまり行列の積。&lt;/p&gt;

&lt;h3 id=&#34;training&#34;&gt;Training&lt;/h3&gt;

&lt;p&gt;機械学習では一般的に、悪いモデルとは何か定義し、それを最小化しようとする。
一般的で、良い誤差関数として&amp;rdquo;cross-entropy&amp;rdquo;というものがある。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;y_ = tf.placeholder(tf.float32, [None, 10])
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;y_が正しい答えで、これとy(softmaxで求めた各数字の確率)の対数の積をを次元ごとにとり、それらの和を求めて-1を掛けている。
&lt;code&gt;reduction_indices=[1]&lt;/code&gt;というのは[784, 10]の10の方を指している模様。
全ての学習データにおいてこれを求め、さらにそれらの平均をとり、これをcross_entropyとしている。
この値はy_とyが離れていれば大きくなるので、なるべく小さくすることが良いモデルにするということになる。&lt;/p&gt;

&lt;p&gt;ではどうやってこの値を小さくするか、TensorFlowは関係する計算のグラフを持っているので、自動的に&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%83%E3%82%AF%E3%83%97%E3%83%AD%E3%83%91%E3%82%B2%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3&#34;&gt;バックプロパゲーション&lt;/a&gt;を使って、どの変数がcross_entropyに影響しているか効率的に特定することができる。&lt;/p&gt;

&lt;p&gt;以下のようにGradientDescent(勾配降下)Optimizerで0.5のleartning rateでcross_entropyが小さくなるように、変数を少しずつ変えていく。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;これを使って学習していく。学習とテストに使うデータは&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/r0.9/tensorflow/examples/tutorials/mnist/input_data.py&#34;&gt;これ&lt;/a&gt;
と&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&amp;quot;MNIST_data/&amp;quot;, one_hot=True)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;を実行すれば用意できるようになっている。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;tf.Session()&lt;/code&gt;と&lt;code&gt;tf.run()&lt;/code&gt;はSessionを取得し、モデルを実行するもの。
変数は&lt;code&gt;tf.initialize_all_variables()&lt;/code&gt;で初期化する必要がある。
batch_xsが画像のピクセルデータで、batch_ysが正しい答え。
&lt;code&gt;sess.run()&lt;/code&gt;の&lt;code&gt;feed_dict={x: batch_xs, y_: batch_ys}&lt;/code&gt;はそれぞれ対応するplaceholderのところに与えられる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;init = tf.initialize_all_variables()
sess = tf.Session()
sess.run(init)
for i in range(1000):
  batch_xs, batch_ys = mnist.train.next_batch(100)
  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;このような小さいランダムなデータを使うのはstochastic(確率的) trainingと呼ばれていて、
今回はstochastic gradient descent。
理想的には全てのデータを全ての訓練のステップで使いたいが、コストがかかるので代わりに毎回異なるサブセットを使うことで
同じ効果を得ている。&lt;/p&gt;

&lt;h3 id=&#34;evaluating-our-model&#34;&gt;Evaluating Our Model&lt;/h3&gt;

&lt;p&gt;モデルがどのくらい良いかを測る。&lt;/p&gt;

&lt;p&gt;以下のcorrect_predictionでは&lt;code&gt;tf.argmax&lt;/code&gt;で最も数値の大きい、つまり確率の高いラベルを取得し、これが正解のものと一致するかというのを
画像データごとに比較している。
結果、[True, False, True, True]であるなら、これを[1, 0, 1, 1]にキャストし、平均を取ったものがaccuracy、正解率となる。
そしてこの値をテスト用のデータで出力している。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
&amp;gt; 0.9206
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;この簡単なモデルだと正解率は92%になったが、実はこれは非常に悪いとのこと。
モデルにもう少し変更を加えるだけで97%になったりするらしい。&lt;/p&gt;

&lt;p&gt;続き: &lt;a href=&#34;http://sambaiz.net/article/6&#34;&gt;TensorFlow チュートリアル2(Deep MNIST for Experts)&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
