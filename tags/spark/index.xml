<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>spark on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/spark/</link>
    <description>Recent content in spark on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Fri, 16 Apr 2021 20:00:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>AWS GlueのJobのBookmarkを有効にして前回の続きから処理を行う</title>
      <link>https://www.sambaiz.net/article/333/</link>
      <pubDate>Fri, 16 Apr 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/333/</guid>
      <description>GlueのJobのBookmarkは どこまで処理したかを記録し、次回はその続きから実行できるようにする機能。 1.0以前は対応していなかったParquetやORCも今は対応している。
AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する - sambaiz-net
Job bookamarkをEnableにして、DynamicFrameのメソッドを呼ぶ際にtranscation_ctxを渡し、job.commit()するとBookmarkされる。
例えば、S3のjsonをソースとするテーブルをカウントして出力する次のjobは、Bookmarkを有効にすると既にカウントしたものが読まれなくなるため、 トータルの件数ではなく前回との増分が出力される。 S3の結果整合性による、参照できるようになるまでのラグも考慮されていて、 単純に保存時間のみによって対象を選ぶのではなく、リストを持って対象の時間の少し前から見るようになっている。
import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, [&amp;#39;JOB_NAME&amp;#39;]) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[&amp;#39;JOB_NAME&amp;#39;], args) source = glueContext.create_dynamic_frame.from_catalog( database=&amp;#34;test&amp;#34;, table_name=&amp;#34;test&amp;#34;, transformation_ctx=&amp;#34;source&amp;#34;) source.toDF().createOrReplaceTempView(&amp;#34;t&amp;#34;) df = spark.sql(&amp;#34;SELECT COUNT(1) as cnt FROM t&amp;#34;) df.write.mode(&amp;#34;append&amp;#34;).csv( &amp;#34;s3://*****/test-table-summary&amp;#34;, compression=&amp;#34;gzip&amp;#34;) job.</description>
    </item>
    
    <item>
      <title>Apache SparkのRDD, DataFrame, DataSetとAction, Transformation</title>
      <link>https://www.sambaiz.net/article/208/</link>
      <pubDate>Wed, 13 Feb 2019 21:17:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/208/</guid>
      <description>Sparkとは ハイパフォーマンスな汎用分散処理システム。 HDFSやS3といった分散ストレージとHadoop YARNといったクラスタマネージャと共に使われる。 中間データをメモリに置いておくことでHadoopのMapReduceよりも高速に処理することができる。 APIはJava, Scala, Python, Rのものがあって、 Pythonは手軽に書ける一方、パフォーマンスはJVMとのやりとりが発生するため落ちる。
HDFS(Hadoop Distributed File System)とは - sambaiz-net
RDDとDataFrameとDataSet RDD(Resilient Distributed Dataset)はSpark Coreの低レベルな インタフェースで、 型を持つイミュータブルな分散コレクション。
DataFrameはSpark SQLのテーブル状で型を持たないデータ形式で、 Tungstenというバイトレベルの最適化や Catalystというクエリのオプティマイザが効くので自分でRDDのAPIを呼ぶよりパフォーマンスが良い。
DataSetは型を持つDataFrameで、Spark 2.0からはDataFrameもDataset[Row]のエイリアスになった。
ActionとTransformation 外部への出力といった副作用を持つActionに対して RDDを返すだけの処理をTransformationという。 Transformationは都度実行されるのではなく Actionが実行される際に、依存関係を表すDAG(Directed Acyclic Graph) をもとに必要なものが遅延評価される。DAGはWeb UIで可視化できる。 また、ネットワークやノードの障害によって計算結果が失われても依存関係をたどり、 並列に計算することで高速に復旧できるようになっている。
narrow dependencyとwide dependency TransformationはRDDのパーティションに対して各Executorが並列に実行する。 mapやfilterなどの、見るパーティションが単一か他と被らない依存関係をnarrow dependencyといって、 そのExecutorだけで処理が完結するためパイプラインでまとめて実行でき速い。 一方、groupByKeyなどの一つのパーティションが複数のパーティションの処理で必要とされる依存関係をwide dependencyといって、 そのパーティションをExecutorが持っていない場合、コストが高いシャッフルが発生する。 シャッフルを必要としない一連の実行単位をstageという。
参考 High Performance Spark - O&amp;rsquo;Reilly Media
RDD vs DataFrames and Datasets: A Tale of Three Apache Spark APIs</description>
    </item>
    
    <item>
      <title>AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する</title>
      <link>https://www.sambaiz.net/article/203/</link>
      <pubDate>Tue, 01 Jan 2019 17:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/203/</guid>
      <description>AWS GlueはマネージドなETL(Extract/Transform/Load)サービスで、Sparkを使ってS3などにあるデータを読み込み加工して変換出力したり、AthenaやRedshift Spectrumで参照できるデータカタログを提供する。 今回はS3のCSVを読み込んで加工し、列指向フォーマットParquetに変換しパーティションを切って出力、その後クローラを回してデータカタログにテーブルを作成してAthenaで参照できることを確認する。
料金はジョブがDPU(4vCPU/16GBメモリ)時間あたり$0.44(最低2DPU/10分)かかる。 また、クローラも同様にDPUで課金される。結構高い。
なお、AthenaのCTASでもParquetを出力することができる。 出力先にファイルがないようにする必要があったり重いクエリは失敗することがあるが手軽で良い。
import * as athena from &amp;#39;athena-client&amp;#39; const clientConfig: athena.AthenaClientConfig = { bucketUri: &amp;#39;s3://*****/*****&amp;#39; skipFetchResult: true, }; const awsConfig: athena.AwsConfig = { region: &amp;#39;us-east-1&amp;#39;, }; const client = athena.createClient(clientConfig, awsConfig); (async () =&amp;gt; { await client.execute(` CREATE TABLE ***** WITH ( format = &amp;#39;PARQUET&amp;#39;, external_location = &amp;#39;s3://*****&amp;#39; ) AS ( SELECT ~~ ) })(); 開発用エンドポイント ジョブの立ち上がりにやや時間がかかるため開発用エンドポイントを立ち上げておくとDPUが確保されて効率よく開発できる。 立ち上げている間のDPUの料金がかかる。つまりずっとジョブを実行し続けているようなもので結構高くつくので終わったら閉じるようにしたい。
ローカルやEC2から自分で開発用エンドポイントにsshしてREPLで実行したりNotebookを立てることもできるが、 コンソールから立ち上げたNotebookは最初からつながっていて鍵の登録も必要なくて楽。
$ ssh -i private-key-file-path 9007:169.</description>
    </item>
    
  </channel>
</rss>
