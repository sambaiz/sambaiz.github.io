<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/machinelearning/index.xml</link>
    <language>ja</language>
    <author>sambaiz</author>
    <rights>(C) 2017</rights>
    <updated>0001-01-01 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>ニューラルネットワークと活性化関数</title>
          <link>https://www.sambaiz.net/article/133/</link>
          <pubDate>Mon, 18 Sep 2017 23:50:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/133/</guid>
          <description>

&lt;p&gt;ニューラルネットワークの活性化関数は各層での重み掛けバイアス足しのあとに適用する非線形の関数。
というのも、線形な計算を繰り返したところで&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f(x) = ax + b
g(x) = f(f(x)) = (a^2)x + (ab + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように単一の線形関数で表現できてしまい、多層にする意味がないため。
また、バックプロバゲーション(誤差逆伝播法)のために微分できる必要もある。&lt;/p&gt;

&lt;p&gt;Tensorflowでも以下の活性化関数が&lt;a href=&#34;https://www.tensorflow.org/api_guides/python/nn#Activation_Functions&#34;&gt;用意されている&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;sigmoid-https-www-tensorflow-org-api-docs-python-tf-sigmoid&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/sigmoid&#34;&gt;sigmoid&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-sigmoid.png&#34; alt=&#34;シグモイド関数&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;y = 1 / (1 + exp(-x))&lt;/code&gt;。値域は(0,1)で&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89&#34;&gt;シグマの語末系ςに似たS字を描く&lt;/a&gt;。
xが大きいときに微分係数が小さくなるため、何層もこの関数を適用するとき、バックプロバゲーションで微分係数を掛けた結果、勾配が消滅(Gradient vanishing)する問題があり、あまり使われないようだ。値域が(-1,1)で似たグラフを描く&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/tanh&#34;&gt;tanh&lt;/a&gt;(Hyperbolic tangent)もある。&lt;/p&gt;

&lt;h3 id=&#34;softsign-https-www-tensorflow-org-api-docs-python-tf-nn-softsign&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/softsign&#34;&gt;softsign&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-softsign.png&#34; alt=&#34;softsign&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x/(1 + abs(x))&lt;/code&gt;。tanhと比べて漸近線に近づく速度が遅くなっている。
それほど性能は変わらないが、初期化においてロバストになるはたらきがあるようだ。&lt;/p&gt;

&lt;h3 id=&#34;softplus-https-www-tensorflow-org-api-docs-python-tf-nn-softplus&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/softplus&#34;&gt;softplus&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-softplus.png&#34; alt=&#34;softplus&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;log(1 + exp(x))&lt;/code&gt;。ReLUに続く。&lt;/p&gt;

&lt;h3 id=&#34;relu-https-www-tensorflow-org-api-docs-python-tf-nn-relu-rectified-linear-unit&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/relu&#34;&gt;ReLU&lt;/a&gt;(Rectified Linear Unit)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-relu.png&#34; alt=&#34;ReLU&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;max(0, x)&lt;/code&gt;。単純だけど最有力。Gradient vanishingも起きない。
softplusと比べてexpやlogを含まない分高速に計算できるので、
膨大で複雑なデータセットに対して多くの層を用いることができる。&lt;/p&gt;

&lt;p&gt;0以下は等しく0になるため、トレーニング中に落ちてしまうとニューロンが死んでしまうことがある。
そのような場合は0以下のとき&lt;code&gt;y = exp(x) - 1&lt;/code&gt;にする&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/elu&#34;&gt;ELU&lt;/a&gt;(Exponential Linear Unit)
などを使う。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-elu.png&#34; alt=&#34;ELU&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f&#34;&gt;Activation functions and it’s types-Which is better?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.orsj.or.jp/archive2/or60-4/or60_4_191.pdf&#34;&gt;最適化から見たディープラーニングの考え方&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&#34;&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&#34;&gt;Rectifier (neural networks) - Wikipedia&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
