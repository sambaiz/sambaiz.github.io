<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/machinelearning/index.xml</link>
    <language>ja</language>
    <author>sambaiz</author>
    <rights>(C) 2017</rights>
    <updated>0001-01-01 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>自己情報量、エントロピー、KL情報量、交差エントロピー</title>
          <link>https://www.sambaiz.net/article/134/</link>
          <pubDate>Mon, 25 Sep 2017 23:50:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/134/</guid>
          <description>

&lt;h2 id=&#34;自己情報量&#34;&gt;自己情報量&lt;/h2&gt;

&lt;p&gt;P(ω)の確率で起きる事象ωの自己情報量は以下の式で定義される。logの底を2にしてbitsで表すのが一般的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/134-self-information.png&#34; alt=&#34;自己情報量の定義&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/134-self-information-graph.png&#34; alt=&#34;自己情報量のグラフ&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;log(P)+log(Q)=log(P*Q)&lt;/code&gt;より加法性がある。
例えば、サイコロで1の目が2回連続で出る(P=&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;36&lt;/sub&gt;)情報量(5.16bits)はサイコロで1の目が出る(P=&lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;6&lt;/sub&gt;)情報量(2.58bits)の2倍と等しい。
確率が高ければ高いほど自己情報量は小さくなり、&lt;code&gt;P(ω)=1&lt;/code&gt;では0bitになる。&lt;/p&gt;

&lt;h2 id=&#34;エントロピー&#34;&gt;エントロピー&lt;/h2&gt;

&lt;p&gt;確率分布Pに従う確率変数Xのエントロピーは以下の式で定義される。情報量の平均。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/134-entropy.png&#34; alt=&#34;エントロピーの定義&#34; /&gt;&lt;/p&gt;

&lt;p&gt;これは情報を送る際に必要なビット数の平均の下限になっている。
例えば、Xが1~4の値を(0.8, 0.1, 0.06, 0.04)の確率でとるとする。
4通りなのだからそれぞれ2bits(00, 01, 10, 11)のコードで表すこともできるが、
ほとんど3や4は出ないのだからbit数を偏らせて(0, 10, 110, 111)のコードで表すと
&lt;code&gt;0.8*1 + 0.1*2 + 0.06*3 + 0.04*3 = 1.3bits&lt;/code&gt;まで減らすことができる。
この場合のエントロピーは1.01bitsで、これより小さくすることはできない。&lt;/p&gt;

&lt;h2 id=&#34;カルバック-ライブラー情報量&#34;&gt;カルバック・ライブラー情報量&lt;/h2&gt;

&lt;p&gt;離散確率分布PのQに対するカルバック・ライブラー情報量は以下の式で定義される。連続確率分布では積分する。
Qの自己情報量からPの自己情報量を引いて平均を取ったもの。ギブスの不等式より非負の値を取る。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/134-kl.png&#34; alt=&#34;KL情報量の定義&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;交差エントロピー&#34;&gt;交差エントロピー&lt;/h2&gt;

&lt;p&gt;離散確率分布PとQの交差エントロピーは以下の式で定義される。連続確率分布では積分する。
PのエントロピーにPのQに対するKL情報量を足したもの。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/134-cross-entropy.png&#34; alt=&#34;交差エントロピーの定義&#34; /&gt;&lt;/p&gt;

&lt;p&gt;これはQの分布に最適化されたコードでPの分布の確率変数の情報を送ってしまった際に必要なビット数の平均の下限になっている。KL情報量が余分な分。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Self-information&#34;&gt;Self-information - Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence&#34;&gt;Kullback–Leibler divergence - Wikipedia&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://postd.cc/visual-information-theory-3/&#34;&gt;情報理論を視覚的に理解する (&lt;sup&gt;3&lt;/sup&gt;&amp;frasl;&lt;sub&gt;4&lt;/sub&gt;) | コンピュータサイエンス | POSTD&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>ニューラルネットワークと活性化関数</title>
          <link>https://www.sambaiz.net/article/133/</link>
          <pubDate>Mon, 18 Sep 2017 23:50:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/133/</guid>
          <description>

&lt;p&gt;ニューラルネットワークの活性化関数は各層での重み掛けバイアス足しのあとに適用する非線形の関数。
というのも、線形な計算を繰り返したところで&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f(x) = ax + b
g(x) = f(f(x)) = (a^2)x + (ab + b)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;のように単一の線形関数で表現できてしまい、多層にする意味がないため。
また、バックプロバゲーション(誤差逆伝播法)のために微分できる必要もある。&lt;/p&gt;

&lt;p&gt;Tensorflowでも以下の活性化関数が&lt;a href=&#34;https://www.tensorflow.org/api_guides/python/nn#Activation_Functions&#34;&gt;用意されている&lt;/a&gt;。&lt;/p&gt;

&lt;h3 id=&#34;sigmoid-https-www-tensorflow-org-api-docs-python-tf-sigmoid&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/sigmoid&#34;&gt;sigmoid&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-sigmoid.png&#34; alt=&#34;シグモイド関数&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;y = 1 / (1 + exp(-x))&lt;/code&gt;。値域は(0,1)で&lt;a href=&#34;https://ja.wikipedia.org/wiki/%E3%82%B7%E3%82%B0%E3%83%A2%E3%82%A4%E3%83%89&#34;&gt;シグマの語末系ςに似たS字を描く&lt;/a&gt;。
xが大きいときに微分係数が小さくなるため、何層もこの関数を適用するとき、バックプロバゲーションで微分係数を掛けた結果、勾配が消滅(Gradient vanishing)する問題があり、あまり使われないようだ。値域が(-1,1)で似たグラフを描く&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/tanh&#34;&gt;tanh&lt;/a&gt;(Hyperbolic tangent)もある。&lt;/p&gt;

&lt;h3 id=&#34;softsign-https-www-tensorflow-org-api-docs-python-tf-nn-softsign&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/softsign&#34;&gt;softsign&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-softsign.png&#34; alt=&#34;softsign&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;x/(1 + abs(x))&lt;/code&gt;。tanhと比べて漸近線に近づく速度が遅くなっている。
それほど性能は変わらないが、初期化においてロバストになるはたらきがあるようだ。&lt;/p&gt;

&lt;h3 id=&#34;softplus-https-www-tensorflow-org-api-docs-python-tf-nn-softplus&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/softplus&#34;&gt;softplus&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-softplus.png&#34; alt=&#34;softplus&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;log(1 + exp(x))&lt;/code&gt;。ReLUに続く。&lt;/p&gt;

&lt;h3 id=&#34;relu-https-www-tensorflow-org-api-docs-python-tf-nn-relu-rectified-linear-unit&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/relu&#34;&gt;ReLU&lt;/a&gt;(Rectified Linear Unit)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-relu.png&#34; alt=&#34;ReLU&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;max(0, x)&lt;/code&gt;。単純だけど最有力。Gradient vanishingも起きない。
softplusと比べてexpやlogを含まない分高速に計算できるので、
膨大で複雑なデータセットに対して多くの層を用いることができる。&lt;/p&gt;

&lt;p&gt;0以下は等しく0になるため、トレーニング中に落ちてしまうとニューロンが死んでしまうことがある。
そのような場合は0以下のとき&lt;code&gt;y = exp(x) - 1&lt;/code&gt;にする&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/nn/elu&#34;&gt;ELU&lt;/a&gt;(Exponential Linear Unit)
などを使う。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/133-elu.png&#34; alt=&#34;ELU&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.com/towards-data-science/activation-functions-and-its-types-which-is-better-a9a5310cc8f&#34;&gt;Activation functions and it’s types-Which is better?&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.orsj.or.jp/archive2/or60-4/or60_4_191.pdf&#34;&gt;最適化から見たディープラーニングの考え方&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf&#34;&gt;Understanding the difficulty of training deep feedforward neural networks&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Rectifier_(neural_networks)&#34;&gt;Rectifier (neural networks) - Wikipedia&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
