<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machinelearning on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/machinelearning/</link>
    <description>Recent content in Machinelearning on sambaiz-net</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Thu, 16 Nov 2023 23:36:00 +0900</lastBuildDate>
    <atom:link href="https://www.sambaiz.net/tags/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SageMaker Canvas の Custom models で AutoML のジョブをノーコードで実行する</title>
      <link>https://www.sambaiz.net/article/458/</link>
      <pubDate>Thu, 16 Nov 2023 23:36:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/458/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/sagemaker/canvas/&#34;&gt;SageMaker Canvas&lt;/a&gt; は&#xA;&lt;a href=&#34;https://aws.amazon.com/sagemaker/jumpstart/&#34;&gt;SageMaker JumpStart&lt;/a&gt; で提供されている学習済みモデルを利用できるインタフェースおよび、&#xA;AutoML の機能である &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/autopilot-automate-model-development.html&#34;&gt;SageMaker Autopilot&lt;/a&gt; を用いた学習ジョブを&#xA;ノーコードで実行できる機能を提供するサービス。&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenAIのGPTを国会会議録の総理大臣の発言でファインチューニングする</title>
      <link>https://www.sambaiz.net/article/452/</link>
      <pubDate>Mon, 11 Sep 2023 23:22:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/452/</guid>
      <description>&lt;p&gt;OpenAI は GPT による&lt;a href=&#34;https://platform.openai.com/docs/api-reference/chat/create&#34;&gt;会話文の生成&lt;/a&gt;や&#xA;文章を&lt;a href=&#34;https://platform.openai.com/docs/api-reference/embeddings/create&#34;&gt;ベクトルに変換する&lt;/a&gt;ほかに&#xA;独自のデータセットによってモデルを&lt;a href=&#34;https://platform.openai.com/docs/guides/fine-tuning&#34;&gt;ファインチューニング&lt;/a&gt;する API を提供しており、&#xA;これを用いることで出力の質を上げたり few-shot learning の例示にかかるコストを節約したりすることができる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SageMakerのHuggingFaceModelでOpenCALM-7BやELYZA-japanese-Llama-2-7bをTGIコンテナでデプロイし日本語の文章を生成する</title>
      <link>https://www.sambaiz.net/article/451/</link>
      <pubDate>Tue, 05 Sep 2023 23:55:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/451/</guid>
      <description>&lt;p&gt;最近 &lt;a href=&#34;https://www.cyberagent.co.jp/news/detail/id=28817&#34;&gt;CyberAgentによる&lt;/a&gt;商用利用可能な68億パラメータの日本語LLMである&#xA;&lt;a href=&#34;https://huggingface.co/cyberagent/open-calm-7b&#34;&gt;OpenCALM-7B&lt;/a&gt; や&#xA;東大松尾研発の&lt;a href=&#34;https://prtimes.jp/main/html/rd/p/000000034.000047565.html&#34;&gt;ELYZAによる&lt;/a&gt; Llama2 ベースの &lt;a href=&#34;https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b&#34;&gt;ELYZA-japanese-Llama-2-7b&lt;/a&gt; など日本語LLMがHugging Face に公開されてきている。&#xA;SageMaker SDK には HuggingFaceModel クラスがあり、これを用いるとモデルIDを指定してデプロイできる。&#xA;また、Hugging Face の Deploy ボタンを押すと SageMaker で最低限動かすためのコードを確認できる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SageMakerのBatch Transformのパラメータの挙動をentrypointの関数の呼び出しと引数から確認する</title>
      <link>https://www.sambaiz.net/article/448/</link>
      <pubDate>Mon, 14 Aug 2023 18:16:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/448/</guid>
      <description>&lt;p&gt;SageMaker の &lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html&#34;&gt;Batch Transform&lt;/a&gt; は単発のバッチ推論ジョブを実行する機能。&#xA;その際推論エンドポイントなどの場合と同じく呼ばれる Model の entrypoint の関数とその引数から、ジョブのパラメータがどのようにはたらくかを確認する。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SageMaker Inference Recommender でコスト最適なインスタンスタイプの推論エンドポイントを立てる</title>
      <link>https://www.sambaiz.net/article/447/</link>
      <pubDate>Thu, 15 Jun 2023 09:38:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/447/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/inference-recommender.html&#34;&gt;SageMaker Inference Recommender&lt;/a&gt; は、&#xA;推論エンドポイントのインスタンスタイプや設定の候補を挙げてくれる機能。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/290/&#34;&gt;SageMakerで学習したPyTorchのモデルをElastic Inferenceを有効にしてデプロイする - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>SageMaker Processing で前処理を行って Training で学習したモデルのパラメータや精度を Experiments で記録する</title>
      <link>https://www.sambaiz.net/article/442/</link>
      <pubDate>Thu, 04 May 2023 19:20:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/442/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html&#34;&gt;SageMaker Experiments&lt;/a&gt;は&#xA;&lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html&#34;&gt;Processing&lt;/a&gt; での前処理や&#xA;&lt;a href=&#34;https://docs.aws.amazon.com/sagemaker/latest/dg/train-model.html&#34;&gt;Training&lt;/a&gt; での学習に用いたパラメータやモデルの精度を記録する機能。&#xA;今回は前処理から学習までの流れを Experiments の Run として追跡し、複数の結果を比較できることを確認する。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark の MLlib で k-means法によるクラスタリングを行う</title>
      <link>https://www.sambaiz.net/article/446/</link>
      <pubDate>Sun, 09 Apr 2023 17:08:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/446/</guid>
      <description>&lt;p&gt;Spark には &lt;a href=&#34;https://spark.apache.org/docs/latest/ml-guide.html&#34;&gt;MLlib&lt;/a&gt; という機械学習のライブラリがあり、&#xA;今回はその中の &lt;a href=&#34;https://spark.apache.org/docs/latest/ml-clustering.html#k-means&#34;&gt;Kmeans&lt;/a&gt; によるクラスタリングを行う。&#xA;k-means法は各データのクラスタを事前に決めた数からランダムに決めて、クラスタごとに中心を取ってから、各データのクラスタを最も近い中心のクラスタに変更する、というのを収束するまで繰り返すという手法。&#xA;Kmeans には収束が早い k-means++法が実装されており、distanceMeasure はデフォルトで euclidean となっている。&lt;/p&gt;</description>
    </item>
    <item>
      <title>最小二乗法(OLS)による線形回帰と決定係数</title>
      <link>https://www.sambaiz.net/article/395/</link>
      <pubDate>Fri, 11 Feb 2022 00:11:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/395/</guid>
      <description>&lt;h2 id=&#34;最小二乗法olsによる線形回帰&#34;&gt;最小二乗法(OLS)による線形回帰&lt;/h2&gt;&#xA;&lt;p&gt;線形回帰は時系列データに見られる前の時間との自己相関がないことを前提に&lt;/p&gt;</description>
    </item>
    <item>
      <title>Glue DataBrewでデータを可視化して分析するProjectと機械学習の前処理を行うJobをCDKで作成する</title>
      <link>https://www.sambaiz.net/article/381/</link>
      <pubDate>Mon, 27 Sep 2021 16:42:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/381/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/jp/glue/features/databrew/&#34;&gt;Glue DataBrew&lt;/a&gt;は、データを可視化してパラメータ間の相関を見たり、カテゴリー変数のエンコードや、欠損値や外れ値を置換する処理をコードなしで実行できるマネージドサービス。Kaggleの&lt;a href=&#34;https://www.kaggle.com/c/house-prices-advanced-regression-techniques&#34;&gt;House Prices Competiton&lt;/a&gt;の学習データで試してみる。全体のコードは&lt;a href=&#34;https://github.com/sambaiz/cdk-databrew-sample&#34;&gt;GitHub&lt;/a&gt;にある。&lt;/p&gt;</description>
    </item>
    <item>
      <title>GoでAmazon Forecastに時系列データをimportしPredictorを作成して予測結果をS3にexportする</title>
      <link>https://www.sambaiz.net/article/380/</link>
      <pubDate>Mon, 20 Sep 2021 23:26:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/380/</guid>
      <description>&lt;p&gt;以前コンソール上で実行したAmazon Forecastでの時系列データの学習、予測をGoで行う。全体のコードは&lt;a href=&#34;https://github.com/sambaiz/go-aws-forecast-sample&#34;&gt;GitHub&lt;/a&gt;にある。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SageMaker Studioの使っていないKernelを自動でシャットダウンするsagemaker-studio-auto-shutdown-extension</title>
      <link>https://www.sambaiz.net/article/373/</link>
      <pubDate>Sun, 18 Jul 2021 23:09:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/373/</guid>
      <description>&lt;p&gt;SageMaker Studioを使っているとインスタンスを明示的に立ち上げることがないので、シャットダウンするのを忘れて&#xA;無駄なインスタンスコストを発生させ続けてしまうことがある。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Amazon Forecastで時系列データの予測を行う</title>
      <link>https://www.sambaiz.net/article/327/</link>
      <pubDate>Sun, 21 Feb 2021 01:04:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/327/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/jp/forecast/&#34;&gt;Amazon Forecast&lt;/a&gt;は機械学習によって時系列データの予測を行うマネージドサービス。&#xA;ドメインやアルゴリズムを選んでデータを投入すればそれらしい出力が得られる。&#xA;まずこれで予測してみて、その結果をベースラインとしてSageMakerなどで自作したモデルを評価するといった使い方もできる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>EKSにKubeflowをインストールする</title>
      <link>https://www.sambaiz.net/article/316/</link>
      <pubDate>Sun, 29 Nov 2020 23:55:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/316/</guid>
      <description>&lt;p&gt;Kubernetes上で機械学習を行うためのツールキット&lt;a href=&#34;https://www.kubeflow.org/&#34;&gt;Kubeflow&lt;/a&gt;を&#xA;EKSに&lt;a href=&#34;https://www.kubeflow.org/docs/aws/deploy/install-kubeflow/&#34;&gt;インストール&lt;/a&gt;する。&#xA;m5.large * 4のクラスタをCDKで作成した。&lt;/p&gt;</description>
    </item>
    <item>
      <title>時系列データのMAモデルとARモデル、その定常性と反転可能性</title>
      <link>https://www.sambaiz.net/article/285/</link>
      <pubDate>Fri, 14 Aug 2020 19:30:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/285/</guid>
      <description>&lt;p&gt;時系列データにLjung-Box testを行い自己相関があることが分かったら、次はそれをモデルで表現したい。今回は自己相関を表す最も基本的なモデルであるMAモデルとARモデルを見ていく。これらを組み合わせたものがARMAモデルで、階差に対するARMAモデルがARIMAモデル。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SageMakerでTensorFlowのモデルを学習させる</title>
      <link>https://www.sambaiz.net/article/293/</link>
      <pubDate>Mon, 10 Aug 2020 13:39:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/293/</guid>
      <description>&lt;p&gt;以前PyTorchのモデルを学習させたが、そのTensorFlow版。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/287/&#34;&gt;SageMakerでPyTorchのモデルを学習させる - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>TensorFlow2のKeras APIでTitanicのモデルを作る</title>
      <link>https://www.sambaiz.net/article/291/</link>
      <pubDate>Sat, 08 Aug 2020 18:32:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/291/</guid>
      <description>&lt;h2 id=&#34;データセット&#34;&gt;データセット&lt;/h2&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/tensorflow/datasets&#34;&gt;TensorFlow Datasets&lt;/a&gt;の&#xA;&lt;a href=&#34;https://www.tensorflow.org/datasets/catalog/titanic&#34;&gt;Titanic&lt;/a&gt;を使う。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SageMakerで学習したPyTorchのモデルをElastic Inferenceを有効にしてデプロイする</title>
      <link>https://www.sambaiz.net/article/290/</link>
      <pubDate>Sun, 26 Jul 2020 02:43:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/290/</guid>
      <description>&lt;p&gt;学習させたモデルをSageMakerのホスティングサービスにデプロイする。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/287/&#34;&gt;SageMakerでPyTorchのモデルを学習させる - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>SageMakerでPyTorchのモデルを学習させる</title>
      <link>https://www.sambaiz.net/article/287/</link>
      <pubDate>Fri, 24 Jul 2020 22:59:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/287/</guid>
      <description>&lt;p&gt;AWSの機械学習サービス&lt;a href=&#34;https://aws.amazon.com/jp/sagemaker/&#34;&gt;SageMaker&lt;/a&gt;でPyTorchのモデルを学習させる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する</title>
      <link>https://www.sambaiz.net/article/289/</link>
      <pubDate>Sun, 19 Jul 2020 19:34:04 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/289/</guid>
      <description>&lt;p&gt;SageMakerで学習させるモデルを開発するにあたって、Notebooks上ではコードを書きづらいのでVS Codeで書いているのだが、ローカルに依存パッケージをインストールして実行しているため&#xA;エディタ上では警告が出ていなくても、実際の環境にはパッケージがなかったりすることがある。&lt;/p&gt;</description>
    </item>
    <item>
      <title>時系列データの定常性と定常過程、単位根過程</title>
      <link>https://www.sambaiz.net/article/279/</link>
      <pubDate>Sun, 05 Jul 2020 21:26:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/279/</guid>
      <description>&lt;p&gt;時系列データを各時間\(t\)ごとの分布から抽出された確率変数\(R_t\)の列とみなすと、次の性質が定義される。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;弱定常性(weak stationarity): 各分布の平均\(E(R_t) = \mu\)が\(t\)に依らず一定で、\(lag=k\) のデータとの共分散である自己共分散 \(Cov(R_t, R_{t-k}) = E[(R_t - E(R_t))(R_{t-k} - E(R_{t-k}))] = \gamma_k\) がlagのみに依存する(つまり分散 \(\gamma_0\) も一定)&lt;/li&gt;&#xA;&lt;li&gt;強定常性(strict stationarity): 任意の\(t,k\)に対する \((R_t, R_{t+1}, &amp;hellip;, R_{t+k})\) の同時分布が同一&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;つまり弱定常性を持つデータは、一定の平均のまわりの一定の振れ幅の中で、それ以前の値にlagに応じた影響を受けながら推移することになる。単に定常性と言う場合この弱定常性のことを指す。&lt;/p&gt;</description>
    </item>
    <item>
      <title>KaggleのHouse Prices CompetitionをXGBoostで解く</title>
      <link>https://www.sambaiz.net/article/230/</link>
      <pubDate>Tue, 09 Jul 2019 23:55:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/230/</guid>
      <description>&lt;p&gt;以前TitanicをやったXGBoostでHome Prices Competitionに挑戦する。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/168/&#34;&gt;KaggleのTitanicのチュートリアルをXGBoostで解く - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>ColabでKaggleのAPIを呼んで学習データのダウンロードと提出を行う</title>
      <link>https://www.sambaiz.net/article/229/</link>
      <pubDate>Tue, 09 Jul 2019 01:55:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/229/</guid>
      <description>&lt;p&gt;Colabではランタイムがリセットされるたびにファイルが消えてしまうのでその度に学習データをアップロードするのが面倒。&#xA;そこで&lt;a href=&#34;https://github.com/Kaggle/kaggle-api&#34;&gt;Kaggle API&lt;/a&gt;でファイルを持ってきてついでに提出まで行う。&lt;/p&gt;</description>
    </item>
    <item>
      <title>AWS DeepRacerを始める</title>
      <link>https://www.sambaiz.net/article/224/</link>
      <pubDate>Mon, 10 Jun 2019 23:06:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/224/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://aws.amazon.com/jp/deepracer/&#34;&gt;AWS DeepRacer&lt;/a&gt;は自走する1/18スケールのレーシングカーで、&#xA;SageMakerやRoboMaker&lt;a href=&#34;https://docs.aws.amazon.com/ja_jp/deepracer/latest/developerguide/deepracer-dependent-aws-services.html&#34;&gt;など&lt;/a&gt;を使って強化学習し、実機を走らせたりバーチャルのDeepRacerリーグで競うことができる。&#xA;カメラの画像の処理や、強化学習のアルゴリズムの実装の必要はなく、報酬関数だけで動いてくれるので敷居が低い。&lt;/p&gt;</description>
    </item>
    <item>
      <title>カテゴリカル変数を変換するsklearnのLabel/OneHotEncoderとpandasのget_dummies</title>
      <link>https://www.sambaiz.net/article/220/</link>
      <pubDate>Mon, 06 May 2019 15:59:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/220/</guid>
      <description>&lt;p&gt;次のデータを用いる。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;data &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;tokyo&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;berlin&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;paris&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;amsterdam&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;paris&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;amsterdam&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;berlin&amp;#34;&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;partial_data &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; data[:&lt;span style=&#34;color:#bd93f9&#34;&gt;4&lt;/span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;preprocessinglabelencoderhttpsscikit-learnorgstablemodulesgeneratedsklearnpreprocessinglabelencoderhtml&#34;&gt;&lt;a href=&#34;https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html&#34;&gt;preprocessing.LabelEncoder&lt;/a&gt;&lt;/h3&gt;&#xA;&lt;p&gt;カテゴリカル変数を数値のラベルに変換する。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;from&lt;/span&gt; sklearn &lt;span style=&#34;color:#ff79c6&#34;&gt;import&lt;/span&gt; preprocessing&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;le &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; preprocessing&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;LabelEncoder()&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;le&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;fit(data)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;encoded &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; le&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;transform(partial_data)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;print&lt;/span&gt;(encoded) &lt;span style=&#34;color:#6272a4&#34;&gt;# [3 1 2 0]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#8be9fd;font-style:italic&#34;&gt;print&lt;/span&gt;(le&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;inverse_transform(encoded)) &lt;span style=&#34;color:#6272a4&#34;&gt;# [&amp;#39;tokyo&amp;#39; &amp;#39;berlin&amp;#39; &amp;#39;paris&amp;#39; &amp;#39;amsterdam&amp;#39;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pr</description>
    </item>
    <item>
      <title>Box-Cox transformationで非正規分布のデータを正規分布に近づける</title>
      <link>https://www.sambaiz.net/article/218/</link>
      <pubDate>Tue, 30 Apr 2019 17:35:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/218/</guid>
      <description>&lt;p&gt;Box-Cox Transormationは次の式による変換。λ=0のときはlog(x)。&lt;/p&gt;&#xA;&lt;p&gt;&lt;img alt=&#34;式&#34; src=&#34;https://www.sambaiz.net/images/218-1.png&#34;&gt;&lt;/p&gt;&#xA;&lt;p&gt;λが1より大きい場合は小さな値の間隔が圧縮され、小さい場合は大きな値の間隔が圧縮されるように変換される。&lt;/p&gt;</description>
    </item>
    <item>
      <title>KaggleのHouse Prices CompetitionのKernelからデータの探り方を学ぶ</title>
      <link>https://www.sambaiz.net/article/216/</link>
      <pubDate>Mon, 08 Apr 2019 21:01:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/216/</guid>
      <description>&lt;p&gt;Kaggleの家の売値を予測するCompetitionのKernelからデータの探り方を学ぶ。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python&#34;&gt;Comprehensive data exploration with Python&lt;/a&gt;&lt;/p&gt;&#xA;&lt;h2 id=&#34;正規化&#34;&gt;正規化&lt;/h2&gt;&#xA;&lt;p&gt;予測する値であるSalePriceの分布を出すと、やや左に寄った非対称の分布をしている。&lt;/p&gt;</description>
    </item>
    <item>
      <title>HI-VAE(Heterogeneous-Incomple VAE)の論文を読んで処理を追う</title>
      <link>https://www.sambaiz.net/article/214/</link>
      <pubDate>Fri, 22 Mar 2019 20:22:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/214/</guid>
      <description>&lt;p&gt;HI-VAE(Heterogeneous-Incomple VAE)は現実のデータセットにありがちな連続値と離散値が混ざっていたり欠損値を含んでいるものを扱えるようにしたVAE。&lt;/p&gt;&#xA;&lt;p&gt;論文: &lt;a href=&#34;https://arxiv.org/abs/1807.03653&#34;&gt;Alfredo Nazabal, Pablo M. Olmos, Zoubin Ghahramani, Isabel Valera (2018) Handling Incomplete Heterogeneous Data using VAEs&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>VAEでエンコードしたMNISTの潜在空間をt-SNEで可視化する</title>
      <link>https://www.sambaiz.net/article/213/</link>
      <pubDate>Sun, 10 Mar 2019 19:03:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/213/</guid>
      <description>&lt;p&gt;t-SNEは多次元のデータを2,3次元上にマッピングして可視化できるようにする手法の一つで、&#xA;Stochastic Neighbor Embedding(SNE, 確率的近傍埋め込み)という手法をベースに、t分布を用いるなどして改良したもの。&lt;/p&gt;</description>
    </item>
    <item>
      <title>PyTorchでVAEのモデルを実装してMNISTの画像を生成する</title>
      <link>https://www.sambaiz.net/article/212/</link>
      <pubDate>Thu, 07 Mar 2019 19:35:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/212/</guid>
      <description>&lt;p&gt;PyTorchでVAEを実装しMNISTの画像を生成する。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/201/&#34;&gt;生成モデルVAE(Variational Autoencoder) - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>SageMaker NotebookでGitリポジトリにSSHでpush/pullできるようにする</title>
      <link>https://www.sambaiz.net/article/211/</link>
      <pubDate>Mon, 04 Mar 2019 22:00:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/211/</guid>
      <description>&lt;p&gt;Sagemaker NotebookはAWSの機械学習のワークフローを提供する&lt;a href=&#34;https://aws.amazon.com/jp/sagemaker/&#34;&gt;SageMaker&lt;/a&gt;の一部である&#xA;マネージドなJupyter Notebooksで、可視化などはもちろん、ここから複数インタンスでの学習ジョブを実行したりすることができる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>生成モデルGAN(Generative Adversarial Network)</title>
      <link>https://www.sambaiz.net/article/210/</link>
      <pubDate>Fri, 22 Feb 2019 23:38:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/210/</guid>
      <description>&lt;p&gt;GAN(Generative Adversarial Network)は生成器Gと、データが本物かどうか識別する識別器Dを交互に最適化していく生成モデル。&#xA;データの評価は識別器によって行われるので、VAEと異なり分布を仮定して尤度を用いる必要がなく、より良いデータが生成できるが、&#xA;GとDを交互に最適化した結果振動してしまいナッシュ均衡に収束せず、またどちらかが先に最適化されてしまうと&#xA;同じようなデータばかり生成してしまうmode collapseや勾配が消えてしまったりして&#xA;うまく学習できないことがある。&lt;/p&gt;</description>
    </item>
    <item>
      <title>PyTorchでMNISTする</title>
      <link>https://www.sambaiz.net/article/205/</link>
      <pubDate>Sat, 19 Jan 2019 23:35:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/205/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://pytorch.org/&#34;&gt;PyTorch&lt;/a&gt;はFacebookによるOSSの機械学習フレームワーク。&#xA;TensorFlow(v1)よりも簡単に使うことができる。&#xA;TensorFlow 2.0ではPyTorchのようにDefine-by-runなeager executionがデフォルトになるのに加え、パッケージも整理されるようなのでいくらか近くなると思われる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>強化学習とDQN(Deep Q-network)</title>
      <link>https://www.sambaiz.net/article/202/</link>
      <pubDate>Tue, 18 Dec 2018 01:00:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/202/</guid>
      <description>&lt;p&gt;強化学習というのは将来に得られる報酬を最大化するような行動を学習していくもの。&lt;/p&gt;&#xA;&lt;h2 id=&#34;状態価値関数による学習&#34;&gt;状態価値関数による学習&lt;/h2&gt;&#xA;&lt;p&gt;状態sのときに取る行動aを決定する方策(Policy)をπ(s)、次の状態s&amp;rsquo;を予測するモデルをP(s,a,s&amp;rsquo;)、直後に得られる即時報酬r_{t+1}を予測するモデルをR(s,a)とすると、将来得られる報酬の期待値である状態価値関数Vπは次の式で再帰的に表すことができ、この形式をベルマン方程式という。&#xA;同じ報酬なら早くに得られた方が良いという考えから将来の報酬rは1ステップ遅れるたびに割引率γが掛けられる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>生成モデルVAE(Variational Autoencoder)</title>
      <link>https://www.sambaiz.net/article/201/</link>
      <pubDate>Tue, 11 Dec 2018 00:23:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/201/</guid>
      <description>&lt;p&gt;生成モデルというのはデータの分布をモデリングしてそこから新しいデータを生成するもの。&#xA;VAEは入力xに対して何らかの分布を仮定し、例えばガウス分布(正規分布)だとすると平均μと分散σを推論し、&#xA;これを&lt;code&gt;z=μ+(σ・ε) (ε~N(0,1))&lt;/code&gt;の潜在変数に変換して生成モデルへの入力とし、その出力の尤度が最大化するように学習させる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Encoder-Decoder RNNのAttention</title>
      <link>https://www.sambaiz.net/article/200/</link>
      <pubDate>Sat, 01 Dec 2018 23:09:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/200/</guid>
      <description>&lt;p&gt;Encoder-Decoder RNNは入力用のEncoderと出力用のDecoderの2つのLSTMを組み合わせたもので、EncoderのStateはDecoderに繋げる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>TensorFlowのモデルをTPUに対応させてColabで学習し実行時間を計測する</title>
      <link>https://www.sambaiz.net/article/199/</link>
      <pubDate>Tue, 27 Nov 2018 09:57:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/199/</guid>
      <description>&lt;p&gt;TPU(Tensor Processing Unit)は&#xA;Google開発のニューラルネットワークの学習に特化したASIC(Application Specific Integrated Circuit)。&#xA;&lt;a href=&#34;https://cloudplatform-jp.googleblog.com/2017/05/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu.html&#34;&gt;一般的なGPUと比べて15~30倍もの性能が出る&lt;/a&gt;&#xA;らしく検索や翻訳などGoogleのサービスでも使われている。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Deep LearningのBatch Normalizationの効果をTensorFlowで確認する</title>
      <link>https://www.sambaiz.net/article/198/</link>
      <pubDate>Wed, 14 Nov 2018 02:52:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/198/</guid>
      <description>&lt;h2 id=&#34;batch-normalizationとは&#34;&gt;Batch Normalizationとは&lt;/h2&gt;&#xA;&lt;p&gt;Deep Learningでは各層の学習を同時に行うため、前の層の変更によって各層の入力の分布が変わってしまうinternal covariate shiftという現象が起こり、そのためにパラメータの初期化をうまくやる必要があったり、学習率を大きくできず多くのステップを要する。&#xA;以下の論文で発表されたBatch Normalization(BN)は各層の入力を正規化して分布を固定することでこれを解決するというもの。&#xA;画像認識のコンテスト&lt;a href=&#34;http://image-net.org/challenges/LSVRC/2015/results&#34;&gt;ILSVRC 2015&lt;/a&gt;で1位を取った&lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34;&gt;ResNet(Residual Network)&lt;/a&gt;でも使われている。&lt;/p&gt;</description>
    </item>
    <item>
      <title>TensorFlow&#43;numpyでData Augmentationして画像の学習データを増やす</title>
      <link>https://www.sambaiz.net/article/197/</link>
      <pubDate>Sun, 11 Nov 2018 15:10:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/197/</guid>
      <description>&lt;p&gt;Data Augmentationは学習データを加工したものを学習データに加えることで数を増やすというもの。&#xA;加工したデータには通常元のものと同じラベルが付くことになるが、&#xA;例えば画像を反転や回転させても元々のものと同じだと認識されるべきだとしたら妥当だ。&#xA;つまり、なんでもすれば良いわけではなくデータセットに応じた、元のデータと同じラベルが付くような加工をする必要があり、&#xA;裏を返せばそのような違いがあっても同じものであることをモデルに学習させることができる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>MLPと誤差逆伝搬法(Backpropagation)</title>
      <link>https://www.sambaiz.net/article/192/</link>
      <pubDate>Sun, 21 Oct 2018 19:30:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/192/</guid>
      <description>&lt;p&gt;MLP(多層パーセプトロン)は入力層と出力層の間に隠れ層を重ねることによって、&#xA;ロジスティック回帰(単純パーセプトロン)ではできなかった非線形分離をできるようにしたニューラルネットワークモデル。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ロジスティック回帰の尤度と交差エントロピーと勾配降下法</title>
      <link>https://www.sambaiz.net/article/191/</link>
      <pubDate>Sun, 14 Oct 2018 23:28:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/191/</guid>
      <description>&lt;h2 id=&#34;ロジスティック回帰&#34;&gt;ロジスティック回帰&lt;/h2&gt;&#xA;&lt;p&gt;単純パーセプトロンの活性化関数を、0か1の値を取るステップ関数ではなく値域\((0,1)\)のシグモイド関数\(\sigma\)にすることで&#xA;確率を返せるようにしたモデル。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Destributed TensorFlowの流れとSavedModelの出力</title>
      <link>https://www.sambaiz.net/article/179/</link>
      <pubDate>Wed, 25 Jul 2018 23:55:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/179/</guid>
      <description>&lt;h2 id=&#34;distributed-tensorflowhttpswwwtensorfloworgdeploydistributed&#34;&gt;&lt;a href=&#34;https://www.tensorflow.org/deploy/distributed&#34;&gt;Distributed TensorFlow&lt;/a&gt;&lt;/h2&gt;&#xA;&lt;p&gt;クラスタを組んでGraphを分散実行する。&lt;/p&gt;&#xA;&lt;p&gt;クラスタは&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;master: sessionを作成し、workerを制御する&lt;/li&gt;&#xA;&lt;li&gt;worker: 計算を行う&lt;/li&gt;&#xA;&lt;li&gt;ps(parameter server): 変数の値を持ち、更新する&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;のjobからなり、gRPCの&lt;/p&gt;</description>
    </item>
    <item>
      <title>TensorFlowのMonitoredSessionとSessionRunHookとsummaryのエラー</title>
      <link>https://www.sambaiz.net/article/175/</link>
      <pubDate>Sun, 01 Jul 2018 23:50:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/175/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/MonitoredSession&#34;&gt;MonitoredSession&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;deprecatedになった&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/Supervisor&#34;&gt;Supervisor&lt;/a&gt;の後継。&lt;/p&gt;</description>
    </item>
    <item>
      <title>TensorFlowのモデルをsave/loadする</title>
      <link>https://www.sambaiz.net/article/172/</link>
      <pubDate>Fri, 22 Jun 2018 01:48:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/172/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/saved_model/builder/SavedModelBuilder&#34;&gt;SavedModelBuilder&lt;/a&gt;で&#xA;モデルを言語に依存しない&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/protobuf/saved_model.proto&#34;&gt;SavedModel&lt;/a&gt;のprotobufにして保存できる。&#xA;SavedModelには&lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/train/Saver&#34;&gt;Saver&lt;/a&gt;によって&lt;a href=&#34;https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/python/saved_model/builder_impl.py#L418&#34;&gt;出力&lt;/a&gt;される&lt;a href=&#34;https://www.tensorflow.org/get_started/checkpoints&#34;&gt;Checkpoint&lt;/a&gt;を共有する一つ以上の&lt;a href=&#34;https://www.tensohttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directoryrflow.org/api_docs/python/tf/MetaGraphDef&#34;&gt;MetaGraphDef&lt;/a&gt;を&lt;a href=&#34;https://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory&#34;&gt;含む&lt;/a&gt;。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ベイズ最適化でランダムフォレストとXGBoostの良いハイパーパラメータを探す</title>
      <link>https://www.sambaiz.net/article/169/</link>
      <pubDate>Sun, 10 Jun 2018 17:50:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/169/</guid>
      <description>&lt;p&gt;ベイズ最適化で良いハイパーパラメータを総当りのグリッドサーチより効率的に探す。&lt;/p&gt;&#xA;&lt;p&gt;ベイズ最適化はSMBO(Sequential Model-based Global Optimization)と呼ばれる、目的関数を近似するモデルと、ある値を探索すべきか評価する関数を用いて探索を進め、&#xA;実際の目的関数での評価とモデルの修正を行っていく手法の一種。&#xA;ベイズ最適化ではモデルとして、直近探索したパラメータとスコアの組の集合&lt;code&gt;D&lt;/code&gt;の条件付き事後確率分布&lt;code&gt;P(y|x, D)&lt;/code&gt;を用いる。&#xA;確率モデルは、任意の入力&lt;code&gt;(x1, x2, ... , xn)&lt;/code&gt;に対応する出力&lt;code&gt;(y1, y2, ..., yn)&lt;/code&gt;がガウス分布(=正規分布)に従うガウス過程(GP)や、TPE(Tree-structured Parzen Estimator)が仮定される。&lt;/p&gt;</description>
    </item>
    <item>
      <title>KaggleのTitanicのチュートリアルをXGBoostで解く</title>
      <link>https://www.sambaiz.net/article/168/</link>
      <pubDate>Sat, 02 Jun 2018 18:16:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/168/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://xgboost.readthedocs.io/en/latest/&#34;&gt;XGBoost&lt;/a&gt;は高性能なGradient Boostingのライブラリ。&#xA;Boostingというのはアンサンブル学習の種類の一つで、ランダムフォレストのように弱学習器をそれぞれ並列に学習するBaggingに対して、&#xA;順番に前回までの結果を受けながら学習し、結果をまとめる際にそれぞれの重みを掛けるもの。&#xA;XGBoostではランダムフォレストと同様に決定木を弱学習器とする。&lt;/p&gt;</description>
    </item>
    <item>
      <title>KaggleのTitanicのチュートリアルをランダムフォレストで解く</title>
      <link>https://www.sambaiz.net/article/166/</link>
      <pubDate>Tue, 29 May 2018 09:33:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/166/</guid>
      <description>&lt;p&gt;ランダムフォレストはデータや特徴量をランダムにサンプリングして決定木を複数生成し並列に学習するアンサンブル学習のBaggingという種類の手法。&#xA;決定木なので特徴量の影響が分かりやすく、値の大小で分岐するので標準化の必要がないが、複数の変数で表現される特徴を学習しづらい。&#xA;また、単一の決定木と比べて過学習を防ぐことができる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>TensorFlow/RNNで連続的な値の時系列データを予測する</title>
      <link>https://www.sambaiz.net/article/154/</link>
      <pubDate>Sun, 11 Feb 2018 19:49:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/154/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/146/&#34;&gt;TensorFlowのRNN(LSTM)のチュートリアルのコードを読む - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>TensorFlowのRNN(LSTM)のチュートリアルのコードを読む</title>
      <link>https://www.sambaiz.net/article/146/</link>
      <pubDate>Wed, 03 Jan 2018 21:12:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/146/</guid>
      <description>&lt;p&gt;TensorflowのRNN(Recurrent Neural Networks)の&lt;a href=&#34;https://www.tensorflow.org/tutorials/recurrent&#34;&gt;チュートリアル&lt;/a&gt;の&lt;a href=&#34;https://github.com/tensorflow/models/blob/12f279d6f4cb33574bc20109b41eb8a59f40cfd1/tutorials/rnn/ptb/ptb_word_lm.py&#34;&gt;コード&lt;/a&gt;を読む。これは文章のそれまでの単語の履歴から、その次に続く単語を予測することで言語モデルを作るもの。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Lpノルムと正則化</title>
      <link>https://www.sambaiz.net/article/137/</link>
      <pubDate>Thu, 12 Oct 2017 23:48:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/137/</guid>
      <description>&lt;h2 id=&#34;ノルムとは&#34;&gt;ノルムとは&lt;/h2&gt;&#xA;&lt;p&gt;ノルムはベクトル空間の距離を表す、以下の条件を満たす関数p。&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;p(av) = |a| p(v): スケーラブル&lt;/li&gt;&#xA;&lt;li&gt;p(u + v) ≦ p(u) + p(v): 三角不等式を満たす&lt;/li&gt;&#xA;&lt;li&gt;p(v) ≧ 0: 負の値を取らない&lt;/li&gt;&#xA;&lt;li&gt;p(v) = 0 &amp;lt;=&amp;gt; v=0: 距離が0 &amp;lt;=&amp;gt; 零ベクトル&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;以下の式で表されるノルムをLpノルムと呼ぶ。&lt;/p&gt;</description>
    </item>
    <item>
      <title>自己情報量、エントロピー、KL情報量、交差エントロピーと尤度関数</title>
      <link>https://www.sambaiz.net/article/134/</link>
      <pubDate>Mon, 25 Sep 2017 23:50:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/134/</guid>
      <description>&lt;h2 id=&#34;自己情報量&#34;&gt;自己情報量&lt;/h2&gt;&#xA;&lt;p&gt;P(ω)の確率で起きる事象ωの自己情報量は以下の式で定義される。logの底を2にしてbitsで表すのが一般的。&lt;/p&gt;</description>
    </item>
    <item>
      <title>ニューラルネットワークと活性化関数</title>
      <link>https://www.sambaiz.net/article/133/</link>
      <pubDate>Mon, 18 Sep 2017 23:50:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/133/</guid>
      <description>&lt;p&gt;活性化関数というのは各層での重み掛けバイアス足しのあとに適用する非線形の関数。&#xA;これはカーネル法のように空間を変換して線形分離できないデータを線形分離できるようにするはたらきをする。&#xA;線形な関数を使うと層を重ねても結局線形のままで、空間もそのまま伸縮するだけなので目的を果たさない。&lt;/p&gt;</description>
    </item>
    <item>
      <title>DeepMindのTensorFlowライブラリSonnetを使う</title>
      <link>https://www.sambaiz.net/article/124/</link>
      <pubDate>Sun, 06 Aug 2017 23:54:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/124/</guid>
      <description>&lt;p&gt;AlphaGoを開発したGoogle DeepMind社のTensorFlowライブラリ&lt;a href=&#34;https://github.com/deepmind/sonnet&#34;&gt;Sonnet&lt;/a&gt;を使う。&#xA;当初はPython2しか対応していないようだったが、今は3にも対応している。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
