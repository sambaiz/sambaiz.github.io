<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machinelearning on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/machinelearning/</link>
    <description>Recent content in Machinelearning on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Tue, 09 Jul 2019 23:55:00 +0900</lastBuildDate>
    
	<atom:link href="https://www.sambaiz.net/tags/machinelearning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>KaggleのHouse Prices CompetitionをXGBoostで解く</title>
      <link>https://www.sambaiz.net/article/230/</link>
      <pubDate>Tue, 09 Jul 2019 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/230/</guid>
      <description>以前TitanicをやったXGBoostでHome Prices Competitionに挑戦する。
KaggleのTitanicのチュートリアルをXGBoostで解く - sambaiz-net
import pandas as pd df_train = pd.read_csv(&#39;house-prices/train.csv&#39;) df_test= pd.read_csv(&#39;house-prices/test.csv&#39;)  欠損値の処理 以前確認したように欠損値が含まれるので一つずつ見ていって埋めていく。
KaggleのHome Prices CompetitionのKernelからデータの探り方を学ぶ - sambaiz-net
import numpy as np def fillna(df): # PoolQC: Pool quality print(np.unique(df[&#39;PoolQC&#39;].values.tolist())) # [&#39;Ex&#39; &#39;Fa&#39; &#39;Gd&#39; &#39;nan&#39;] df[&amp;quot;PoolQC&amp;quot;] = df[&amp;quot;PoolQC&amp;quot;].fillna(&amp;quot;None&amp;quot;) # MiscFeature: Miscellaneous feature not covered in other categories print(np.unique(df[&#39;MiscFeature&#39;].values.tolist())) # [&#39;Gar2&#39; &#39;Othr&#39; &#39;Shed&#39; &#39;TenC&#39; &#39;nan&#39;] df[&amp;quot;MiscFeature&amp;quot;] = df[&amp;quot;MiscFeature&amp;quot;].fillna(&amp;quot;None&amp;quot;) # Alley: Type of alley access print(np.unique(df[&#39;Alley&#39;].values.tolist())) # [&#39;Grvl&#39; &#39;Pave&#39; &#39;nan&#39;] df[&amp;quot;Alley&amp;quot;] = df[&amp;quot;Alley&amp;quot;].</description>
    </item>
    
    <item>
      <title>ColabでKaggleのAPIを呼んで学習データのダウンロードと提出を行う</title>
      <link>https://www.sambaiz.net/article/229/</link>
      <pubDate>Tue, 09 Jul 2019 01:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/229/</guid>
      <description>Colabではランタイムがリセットされるたびにファイルが消えてしまうのでその度に学習データをアップロードするのが面倒。 そこでKaggle APIでファイルを持ってきてついでに提出まで行う。
Notebookを公開することも考えてなるべく認証情報を直書きしたくないのでGoogle DriveをマウントしてそこからTokenを持ってくることにする。 KaggleのMy AccountからAPI Tokenを発行しGoogle Driveに上げておく。
from google.colab import drive drive.mount(&#39;/content/gdrive&#39;) ! mkdir -p ~/.kaggle ! cp &amp;quot;gdrive/My Drive/kaggle/kaggle.json&amp;quot; ~/.kaggle/ ! pip install kaggle --upgrade ! kaggle config view  competitions downloadでファイルをダウンロードしてくる。
! kaggle competitions download house-prices-advanced-regression-techniques -p house-prices import pandas as pd df_train = pd.read_csv(&#39;house-prices/train.csv&#39;) df_test= pd.read_csv(&#39;house-prices/test.csv&#39;)  competitions submitで提出し、 competitions submissionsで提出履歴とスコアが見える。 リーダーボードはcompetitions leaderboardで取得できる。
! kaggle competitions submit house-prices-advanced-regression-techniques -f submit.csv -m &amp;quot;test submission&amp;quot; ! kaggle competitions submissions house-prices-advanced-regression-techniques !</description>
    </item>
    
    <item>
      <title>AWS DeepRacerを始める</title>
      <link>https://www.sambaiz.net/article/224/</link>
      <pubDate>Mon, 10 Jun 2019 23:06:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/224/</guid>
      <description>AWS DeepRacerは自走する1/18スケールのレーシングカーで、 SageMakerやRoboMakerなどを使って強化学習し、実機を走らせたりバーチャルのDeepRacerリーグで競うことができる。 カメラの画像の処理や、強化学習のアルゴリズムの実装の必要はなく、報酬関数だけで動いてくれるので敷居が低い。
強化学習とDQN(Deep Q-network) - sambaiz-net
設定項目 Action space 取りうるアクションである速度とステアリングの組み合わせのリスト。次の項目から生成される。
 Maximum steering angle (1 - 30) Steering angle granularity (3, 5, 7) Maximum speed (0.8 - 8) Speed granularity (1, 2, 3) Loss type (Mean square error, Huber) Number of experience episodes between each policy-updating iteration (5 - 100)  Reward function 強化学習の報酬関数。次の入力パラメータを用いて実装する。
{ &amp;quot;all_wheels_on_track&amp;quot;: Boolean, # flag to indicate if the vehicle is on the track &amp;quot;x&amp;quot;: float, # vehicle&#39;s x-coordinate in meters &amp;quot;y&amp;quot;: float, # vehicle&#39;s y-coordinate in meters &amp;quot;distance_from_center&amp;quot;: float, # distance in meters from the track center &amp;quot;is_left_of_center&amp;quot;: Boolean, # Flag to indicate if the vehicle is on the left side to the track center or not.</description>
    </item>
    
    <item>
      <title>カテゴリカル変数をLabel/OneHotEncoderやget_dummiesで変換する</title>
      <link>https://www.sambaiz.net/article/220/</link>
      <pubDate>Mon, 06 May 2019 15:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/220/</guid>
      <description>data = [&amp;quot;tokyo&amp;quot;, &amp;quot;berlin&amp;quot;, &amp;quot;tokyo&amp;quot;, &amp;quot;paris&amp;quot;, &amp;quot;amsterdam&amp;quot;, &amp;quot;paris&amp;quot;, &amp;quot;amsterdam&amp;quot;, &amp;quot;berlin&amp;quot;] partial_data = data[:3]  scikit-learnのpreprocessing.LabelEncoderでカテゴリカル変数を数値のラベルに変換できる。
from sklearn import preprocessing le = preprocessing.LabelEncoder() le.fit(data) print(le.classes_) # [&#39;amsterdam&#39; &#39;berlin&#39; &#39;paris&#39; &#39;tokyo&#39;] encoded = le.transform(partial_data) print(encoded) # [3 1 3] print(le.inverse_transform(encoded)) # [&#39;tokyo&#39; &#39;berlin&#39; &#39;tokyo&#39;]  preprocessing.OneHotEncoder でone hot vectorに変換できる。
oh = preprocessing.OneHotEncoder() oh.fit([[d] for d in data]) print(oh.categories_[0]) # [&#39;amsterdam&#39; &#39;berlin&#39; &#39;paris&#39; &#39;tokyo&#39;] encoded = oh.transform([[d] for d in partial_data]).toarray() print(encoded) # [[0.</description>
    </item>
    
    <item>
      <title>Box-Cox transformationで非正規分布のデータを正規分布に近づける</title>
      <link>https://www.sambaiz.net/article/218/</link>
      <pubDate>Tue, 30 Apr 2019 17:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/218/</guid>
      <description>Box-Cox Transormationは次の式による変換。λ=0のときはlog(x)。
λが1より大きい場合は小さな値の間隔が圧縮され、小さい場合は大きな値の間隔が圧縮されるように変換される。
import numpy as np from scipy.special import boxcox1p import matplotlib.pyplot as plt from bokeh.plotting import figure from bokeh.io import output_notebook, show output_notebook() p = figure( title=&amp;quot;Box-Cox Transformations&amp;quot;, x_axis_label=&#39;x&#39;, y_axis_label=&#39;λ&#39;, ) for lam in [-1, 0, 1, 2]: v = np.array([boxcox1p(i, lam) for i in range(10)]) v = v / v.max() p.line(v, lam) p.circle(v, lam, size=8) show(p)  これによって左右非対称な分布を対称(skew=0)な正規分布に近づけることができる。 以前正規分布に近づけるのに対数を取ったが、これはBox-Cox transformationの1ケースだといえる。
KaggleのHome Prices CompetitionのKernelからデータの探り方を学ぶ - sambaiz-net
試しに適当な左右非対称な分布のデータを変換してみる。</description>
    </item>
    
    <item>
      <title>KaggleのHouse Prices CompetitionのKernelからデータの探り方を学ぶ</title>
      <link>https://www.sambaiz.net/article/216/</link>
      <pubDate>Mon, 08 Apr 2019 21:01:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/216/</guid>
      <description>Kaggleの家の売値を予測するCompetitionのKernelからデータの探り方を学ぶ。
Comprehensive data exploration with Python
正規化 予測する値であるSalePriceの分布を出すと、やや左に寄った非対称の分布をしている。
import pandas as pd import seaborn as sns df = pd.read_csv(&#39;train.csv&#39;) df[&#39;SalePrice&#39;].describe()  count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000  scipy.stats.probplot()で プロットしても直線で表されている正規分布から外れていることが分かるが、正規分布であることが望ましい。
from scipy import stats import matplotlib.pyplot as plt res = stats.probplot(df[&#39;SalePrice&#39;], dist=&#39;norm&#39;, plot=plt)  そこで対数をとって正規分布に近づけてやる。
import numpy as np res = stats.probplot(np.log(df[&#39;SalePrice&#39;]), dist=&#39;norm&#39;, plot=plt)  なお、この変換はBox-Cox transformationのλ=0のときにあたる。
Box-Cox transformationで非正規分布のデータを正規分布に近づける - sambaiz-net</description>
    </item>
    
    <item>
      <title>HI-VAE(Heterogeneous-Incomple VAE)の論文を読んで処理を追う</title>
      <link>https://www.sambaiz.net/article/214/</link>
      <pubDate>Fri, 22 Mar 2019 20:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/214/</guid>
      <description>HI-VAE(Heterogeneous-Incomple VAE)は現実のデータセットにありがちな連続値と離散値が混ざっていたり欠損値を含んでいるものを扱えるようにしたVAE。
論文: Alfredo Nazabal, Pablo M. Olmos, Zoubin Ghahramani, Isabel Valera (2018) Handling Incomplete Heterogeneous Data using VAEs
生成モデルVAE(Variational Autoencoder) - sambaiz-net
GitHubにTensorFlow実装が上がっているので論文と合わせて追ってみる。
入力データ 入力データdata.csvと、そのスキーマdata_types.csvが用意されていて、 様々なtypeのデータが含まれる24次元のデータセットであることが分かる。
type,dim,nclass pos,1, cat,3,3 cat,7,7 cat,4,4 count,1, ordinal,11,11 ordinal,11,11 ordinal,11,11 ordinal,11,11 ordinal,11,11 ordinal,11,11 real,1, real,1, real,1, real,1, real,1, real,1, pos,1, pos,1, pos,1, pos,1, pos,1, pos,1, cat,2,2  これに対して、xx%の確率でランダムな次元を欠損値として扱う際に対象とする行と次元を表す Missingxx_y.csvがある。
2,1 3,1 ... 29985,24 29998,24  typeごとのデータの扱い real(実数値) 標準化してencoderに入力し、decoderでは正規分布の平均と分散を 出力し サンプリングしてデータを生成する。
pos(正の実数) 対数を標準化してencoderに入力し、decoderでは正規分布の平均と分散を 出力し サンプリングしたデータをexp()で元のレンジに戻す。</description>
    </item>
    
    <item>
      <title>VAEでエンコードしたMNISTの潜在空間をt-SNEで可視化する</title>
      <link>https://www.sambaiz.net/article/213/</link>
      <pubDate>Sun, 10 Mar 2019 19:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/213/</guid>
      <description>t-SNEは多次元のデータを2,3次元上にマッピングして可視化できるようにする手法の一つで、 Stochastic Neighbor Embedding(SNE, 確率的近傍埋め込み)という手法をベースに、t分布を用いるなどして改良したもの。
Visualizing Data using t-SNE
SNE まず入力データ間の類似度をユークリッド距離を用いた次の条件付き確率p_{j|i}で表す。 これはx_iを中心とした正規分布上で、確率密度に基づいて隣り合うデータを選ぶ場合x_jが選ばれる確率となる。
同様に出力データでも次の条件付き確率q_{j|i}を計算する。σ=1/sqrt(2)とする。
この分布間のKL情報量を勾配降下法で最小化していくことで出力を最適化するのがSME。
自己情報量、エントロピー、KL情報量、交差エントロピーと尤度関数 - sambaiz-net
ロジスティック回帰の尤度と交差エントロピーと勾配降下法 - sambaiz-net
t-SNE t-SNEでは条件付き確率ではなく同時確率を用いる。また、qを自由度1のt分布で表す。
MNISTの潜在空間をt-SNEで可視化した結果 以前作ったVAEのMNISTモデルの潜在空間を scikit-learnのTSNEで可視化する。
PyTorchでVAEのモデルを実装してMNISTの画像を生成する - sambaiz-net
%matplotlib inline import matplotlib.pyplot as plt from sklearn.manifold import TSNE from random import random colors = [&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;orange&amp;quot;, &amp;quot;purple&amp;quot;, &amp;quot;brown&amp;quot;, &amp;quot;fuchsia&amp;quot;, &amp;quot;grey&amp;quot;, &amp;quot;olive&amp;quot;, &amp;quot;lightblue&amp;quot;] def visualize_zs(zs, labels): plt.figure(figsize=(10,10)) points = TSNE(n_components=2, random_state=0).fit_transform(zs) for p, l in zip(points, labels): plt.scatter(p[0], p[1], marker=&amp;quot;${}$&amp;quot;.</description>
    </item>
    
    <item>
      <title>PyTorchでVAEのモデルを実装してMNISTの画像を生成する</title>
      <link>https://www.sambaiz.net/article/212/</link>
      <pubDate>Thu, 07 Mar 2019 19:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/212/</guid>
      <description>PyTorchでVAEを実装しMNISTの画像を生成する。
生成モデルVAE(Variational Autoencoder) - sambaiz-net
学習データ datasetsのMNIST画像を使う。
from torchvision import datasets, transforms transform = transforms.Compose([ transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))]) dataset_train = datasets.MNIST( &#39;~/mnist&#39;, train=True, download=True, transform=transform) dataset_valid = datasets.MNIST( &#39;~/mnist&#39;, train=False, download=True, transform=transform) dataloader_train = utils.data.DataLoader(dataset_train, batch_size=1000, shuffle=True, num_workers=4) dataloader_valid = utils.data.DataLoader(dataset_valid, batch_size=1000, shuffle=True, num_workers=4)  VAE それぞれ3層のEncoderとDecoder。
import torch import torch.nn as nn import torch.nn.functional as F device = &#39;cuda&#39; class VAE(nn.Module): def __init__(self, z_dim): super(VAE, self).__init__() self.dense_enc1 = nn.</description>
    </item>
    
    <item>
      <title>SageMaker NotebookでGitリポジトリにSSHでpush/pullできるようにする</title>
      <link>https://www.sambaiz.net/article/211/</link>
      <pubDate>Mon, 04 Mar 2019 22:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/211/</guid>
      <description>Sagemaker NotebookはAWSの機械学習のワークフローを提供するSageMakerの一部である マネージドなJupyter Notebooksで、可視化などはもちろん、ここから複数インタンスでの学習ジョブを実行したりすることができる。
Git統合 によってノートブック作成時にGitHubなどのリポジトリを指定すると前もって持ってきてくれるようになったが、 今のところHTTPSエンドポイントにしか対応していないようで、ユーザー名・パスワードまたはトークンといった個人に紐づく認証情報が必要になる。 今回はこの機能を使わずに、ライフサイクル設定でssh鍵を置き、これでpush/pullできるようにする。
パスフレーズなしの鍵を作って公開鍵を対象リポジトリのDeployKeyに登録してread/writeできるようにする。
$ mkdir sagemaker-sshkey $ cd sagemaker-sshkey $ ssh-keygen -t rsa -b 4096 -f id_rsa -N &amp;quot;&amp;quot; $ pbcopy &amp;lt; id_rsa.pub  秘密鍵はSSMのParameter Storeに登録する。
AWS Systems Manager (SSM)のParameter Storeに認証情報を置き参照する - sambaiz-net
$ aws ssm put-parameter --name &amp;quot;sagemaker-sshkey&amp;quot; --value &amp;quot;`cat id_rsa`&amp;quot; --type String --overwrite $ aws ssm get-parameters --names &amp;quot;sagemaker-sshkey&amp;quot;  ライフサイクル設定でノートブック開始時に次のスクリプトが実行されるようにする。 このスクリプトはrootで実行される。Parameter Storeが読める権限をNotebookのIAMに付けておく。
#!/bin/bash set -e su - ec2-user &amp;lt;&amp;lt;EOF cd /home/ec2-user aws ssm get-parameters --names &amp;quot;sagemaker-sshkey&amp;quot; | jq -r &amp;quot;.</description>
    </item>
    
    <item>
      <title>生成モデルGAN(Generative Adversarial Network)</title>
      <link>https://www.sambaiz.net/article/210/</link>
      <pubDate>Fri, 22 Feb 2019 23:38:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/210/</guid>
      <description>GAN(Generative Adversarial Network)は生成器Gと、データが本物かどうか識別する識別器Dを交互に最適化していく生成モデル。 データの評価は識別器によって行われるので、VAEと異なり分布を仮定して尤度を用いる必要がなく、より良いデータが生成できるが、 GとDを交互に最適化した結果振動してしまいナッシュ均衡に収束せず、またどちらかが先に最適化されてしまうと 同じようなデータばかり生成してしまうmode collapseや勾配が消えてしまったりして うまく学習できないことがある。
生成モデルVAE(Variational Autoencoder) - sambaiz-net
識別器(Discriminator) 生成されたデータの分布をpg(x)、真のデータの分布をp{data}(x)として、同数のデータにそれぞれy=0, 1のラベルを付ける。 識別器D(x)はyが0か1かの分類モデルで、負の交差エントロピー誤差V(D)を最大化するように学習する。
最適化したあとのDをD^とすると、目的関数V(D^)はJensen-Shannon(JS)ダイバージェンスを使って表せる。これを最小化するのがGANの目的。
生成器(Generator) p_dataとp_gのJSダイバージェンスが小さくなるように学習する。生成器Gを学習するための目的関数は次の通りで、これを最小化する。</description>
    </item>
    
    <item>
      <title>PyTorchでMNISTする</title>
      <link>https://www.sambaiz.net/article/205/</link>
      <pubDate>Sat, 19 Jan 2019 23:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/205/</guid>
      <description>PyTorchはFacebookによるOSSの機械学習フレームワーク。 TensorFlow(v1)よりも簡単に使うことができる。 TensorFlow 2.0ではPyTorchのようにDefine-by-runなeager executionがデフォルトになるのに加え、パッケージも整理されるようなのでいくらか近くなると思われる。
使い方 インストール Colabで動かす。まずpipでインストール。
!pip install torch torchvision  autograd(自動微分) Tensorは自身が作成された関数の参照.grad_fnを持ち、backward()が呼ばれるとbackpropしてrequires_grad=TrueなTensorの勾配を自動で計算し.gradに入れてくれる。
MLPと誤差逆伝搬法(Backpropagation) - sambaiz-net
import torch x = torch.randn(4, 4) y = torch.randn(4, 1) w = torch.randn(4, 1, requires_grad=True) b = torch.randn(1, requires_grad=True) y_pred = torch.matmul(x, w) + b loss = (y_pred - y).pow(2).sum() print(x.grad, w.grad) # None None loss.backward() print(x.grad, w.grad) # None tensor([...]) with torch.no_grad(): y_eval = torch.matmul(x, w) + b print(y_eval.requires_grad) # False  Module nnパッケージにLinearやConv2dといったModuleが実装されていて、次のように呼び出すとforward()が 呼ばれ順伝播する。</description>
    </item>
    
    <item>
      <title>強化学習とDQN(Deep Q-network)</title>
      <link>https://www.sambaiz.net/article/202/</link>
      <pubDate>Tue, 18 Dec 2018 01:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/202/</guid>
      <description>強化学習というのは将来に得られる報酬を最大化するような行動を学習していくもの。
状態価値関数による学習 状態sのときに取る行動aを決定する方策(Policy)をπ(s)、次の状態s&amp;rsquo;を予測するモデルをP(s,a,s&amp;rsquo;)、直後に得られる即時報酬r_{t+1}を予測するモデルをR(s,a)とすると、将来得られる報酬の期待値である状態価値関数Vπは次の式で再帰的に表すことができ、この形式をベルマン方程式という。 同じ報酬なら早くに得られた方が良いという考えから将来の報酬rは1ステップ遅れるたびに割引率γが掛けられる。
どんな状態においても状態価値関数を最大化させる最適方策π*を探すにあたり、定義通り将来の報酬を待つのではなく、即時報酬Rで状態価値関数Vを更新していく。これをTD(Temporal difference)学習という。
取り得る状態数が多いと収束するまでの時間が長くなる問題があって、これを価値関数の近似によって解消するのがDQN。
DQN (Deep Q-Network) DNNでQ学習を行う。Q学習というのは状態sのときに行動aしたときの報酬の期待値である行動価値関数Qを最大化させるように学習させるもので、 最も良かった行動でQを更新していく。 未知の行動を探索するかどうかはバンディットアルゴリズムのε-greedyによって確率的に決定し、学習が進むにつれて確率は下がっていく。
前もってランダムに行動と結果をサンプリングしておき学習の際に使う、ER(Experience Replay)というテクニックが使われる。 これによって実行回数が減るだけではなく、時系列的な相関を減らし効率的に学習させることができる。
またmain-networkとは別に、同じ形式のtarget-networkを作ってQ(s&amp;rsquo;=s_{t+1}, a)の値を出すのに使う。 target-networkのパラメータはmain-networkのものを定期的に同期させる以外では更新しないことで学習を安定させることができる。 これをFixed Target Q-Networkという。
参考 強化学習の基礎
第14回　深層強化学習DQN（Deep Q-Network）の解説｜Tech Book Zone Manatee</description>
    </item>
    
    <item>
      <title>生成モデルVAE(Variational Autoencoder)</title>
      <link>https://www.sambaiz.net/article/201/</link>
      <pubDate>Tue, 11 Dec 2018 00:23:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/201/</guid>
      <description>生成モデルというのはデータの分布をモデリングしてそこから新しいデータを生成するもの。 VAEは入力xに対して何らかの分布を仮定し、例えばガウス分布(正規分布)だとすると平均μと分散σを推論し、 これをz=μ+(σ・ε) (ε~N(0,1))の潜在変数に変換して生成モデルへの入力とし、その出力の尤度が最大化するように学習させる。
Variational Autoencoderという名前はこの分布を推論して生成する流れがAutoencoderの形式と似ているところから来ている。 Autoencoder(自己符号化器)というのはある入力をエンコードしてデコードしたときに入力と同じものを出力するように学習させたもので、 これによって次元削減された潜在変数zが得られる。
推論モデルの確率分布をq、生成モデルの確率分布をpとする。対数尤度log{p}を計算したいが潜在変数zが訓練データにないので周辺化する(1)。 これを変換していくと(2)のようになり、第二項のKL情報量は0以上の値になるので第一項のLを最大化することが対数尤度の最大化につながる。 このLをEvidence Lower Bound (ELBO)といい、推論モデルのパラメータφと生成モデルのパラメータθを交互に最適化して これを最大化させることで尤度の下界を引き上げていく。
自己情報量、エントロピー、KL情報量、交差エントロピーと尤度関数 - sambaiz-net
生成モデルがベルヌーイ分布、p(z)が標準正規分布とするとELBOは次のようになる。
第一項が再生成誤差。第二項によってp(z)とq(z|x)が近づくように学習し正則化され、zの各次元が独立になっていく。 これにβ&amp;gt;1を掛けるとさらに独立性が増し、次元ごとに、人であれば性別や年齢のような、disentangle(もつれを解く)された特徴を持つことができる。これをβ-VAEという。
PyTorchでVAEのモデルを実装してMNISTの画像を生成する - sambaiz-net
VAEは分布を仮定して尤度によって学習するため、真の分布にはないところの生成データが良くない問題がある。 VAE以外の生成モデルとしてGAN(Generative Adversarial Networks)があって、これはデータの分布を仮定せずより近い分布から良いデータを生成するのを目指す。
生成モデルGAN(Generative Adversarial Network) - sambaiz-net
参考 Kerasで学ぶAutoencoder
Carl Doersch (2016) Tutorial on Variational Autoencoders
Variational Autoencoder徹底解説 - Qiita</description>
    </item>
    
    <item>
      <title>Encoder-Decoder RNNのAttention</title>
      <link>https://www.sambaiz.net/article/200/</link>
      <pubDate>Sat, 01 Dec 2018 23:09:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/200/</guid>
      <description>Encoder-Decoder RNNは入力用のEncoderと出力用のDecoderの2つのLSTMを組み合わせたもので、EncoderのStateはDecoderに繋げる。
したがって入力データはDecoderに渡されるStateにまとめられることになるが、 出力ごとに入力時系列の重要な部分は異なるため、特定の部分に注目できるようにすると良い結果が期待できる。 次の論文ではAttention Layerを追加することでこれを行い翻訳の精度を向上させている。
Minh-Thang Luong, Hieu Pham, Christopher D. Manning (2015) Effective Approaches to Attention-based Neural Machine Translation
Attention LayerはEncoderの出力とDecoderの対象の出力からどの部分を重要とするかを表すAlign weights a(t)と Encoderの出力を掛けたものをContext vector c(t)として出力する。 scoreにはそのまま掛けたものや(h_{dec}h_{enc})、重みとDecoderの出力のみを掛ける(Wh_{dec})といったものが使われる。</description>
    </item>
    
    <item>
      <title>TensorFlowのモデルをTPUに対応させてColabで学習し実行時間を計測する</title>
      <link>https://www.sambaiz.net/article/199/</link>
      <pubDate>Tue, 27 Nov 2018 09:57:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/199/</guid>
      <description>TPU(Tensor Processing Unit)は Google開発のニューラルネットワークの学習に特化したASIC(Application Specific Integrated Circuit)。 一般的なGPUと比べて15~30倍もの性能が出る らしく検索や翻訳などGoogleのサービスでも使われている。
TPUを使える環境として、無料で使えるJupyter NotebooksのGoogle Colabと GCPのCloud TPUがある。ColabのTPUも裏側ではCloud TPUが動いている。 Cloud TPUを直に使うとVMから接続して使うことになるので、TPUの料金に加えてVMの料金もかかる。
モデルのTPU対応 CNNのモデルをTPUEstimatorでTPUに対応させる。
EstimatorはTensorFlowの高レベルAPIで、 train()、 evaluate()、 predict()、 export_saved_model() といったモデルの学習から保存まで必要な機能を一通り提供する。
初めは比較的低レベルのAPIを使おうとしていたが、XLA(Accelerated Linear Algebra)によるコンパイルがうまくいかないなど様々な問題にあたって大変だったので使っておくと良いと思う。 それでもトライアンドエラーの繰り返しで、典型的なものはTroubleshootingにあるが、ないものは調べるなりしてなんとかやっていくしかない。
定数などの定義。BATCH_SIZEはCloud TPUのシャード数の8で割り切れる値にする必要がある。
import pandas as pd from sklearn.model_selection import train_test_split import tensorflow as tf import numpy as np flags = tf.app.flags flags.DEFINE_boolean(&#39;use_tpu&#39;, True, &#39;use tpu or not&#39;) tf.app.flags.DEFINE_string(&#39;f&#39;, &#39;&#39;, &#39;kernel&#39;) FLAGS = flags.FLAGS EPOCH_NUM = 100 BATCH_SIZE = 800 # must be divisible by number of replicas 8 EVAL_BATCH_SIZE = 800 SHARD_NUM = 8 # A single Cloud TPU has 8 shards.</description>
    </item>
    
    <item>
      <title>Deep LearningのBatch Normalizationの効果をTensorFlowで確認する</title>
      <link>https://www.sambaiz.net/article/198/</link>
      <pubDate>Wed, 14 Nov 2018 02:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/198/</guid>
      <description>Batch Normalizationとは Deep Learningでは各層の学習を同時に行うため、前の層の変更によって各層の入力の分布が変わってしまうinternal covariate shiftという現象が起こり、そのためにパラメータの初期化をうまくやる必要があったり、学習率を大きくできず多くのステップを要する。 以下の論文で発表されたBatch Normalization(BN)は各層の入力を正規化して分布を固定することでこれを解決するというもの。 画像認識のコンテストILSVRC 2015で1位を取ったResNet(Residual Network)でも使われている。
Sergey Ioffe, Christian Szegedy (2015) Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift
具体的にはWx+bと活性化関数の間にBNの層を入れる。μ、σ^2は入力xの平均と分散。 単に正規化するだけでは表現力が下がってしまうのでγとβでスケールやシフトできるようにする。これらの変数は他のパラメータと同様に学習させる。
TensorFlowでの確認 TensorFlowではbatch_normalization()がすでに実装されているのでこれを使う。
以下のCNNで学習率を高めに設定しBNありなしの結果を比較する。学習データはmnist。MonitoredSessionでcostをsummaryとして出力しTensorBoardで見られるようにしている。
TensorBoardでsummaryやグラフを見る - sambaiz-net
TensorFlowのMonitoredSessionとSessionRunHookとsummaryのエラー - sambaiz-net
import pandas as pd import numpy as np import tensorflow as tf from sklearn.model_selection import train_test_split from sklearn.utils import shuffle from sklearn.metrics import accuracy_score train = pd.read_csv(&#39;./train.csv&#39;) (x_train, x_valid ,y_train, y_valid) = train_test_split( train.</description>
    </item>
    
    <item>
      <title>TensorFlow&#43;numpyでData Augmentationして画像の学習データを増やす</title>
      <link>https://www.sambaiz.net/article/197/</link>
      <pubDate>Sun, 11 Nov 2018 15:10:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/197/</guid>
      <description>Data Augmentationは学習データを加工したものを学習データに加えることで数を増やすというもの。 加工したデータには通常元のものと同じラベルが付くことになるが、 例えば画像を反転や回転させても元々のものと同じだと認識されるべきだとしたら妥当だ。 つまり、なんでもすれば良いわけではなくデータセットに応じた、元のデータと同じラベルが付くような加工をする必要があり、 裏を返せばそのような違いがあっても同じものであることをモデルに学習させることができる。
今回はData Augmentationで行われる加工をTensorFlowやnumpyの関数でおなじみLennaの画像に行う。
必要なパッケージと画像をimportする。Jupyter Notebooksで実行する。
%matplotlib inline from PIL import Image import matplotlib.pyplot as plt import numpy as np import tensorflow as tf im = Image.open(&amp;quot;lenna.png&amp;quot;, &amp;quot;r&amp;quot;)  Flipping  flip_left_right() random_flip_left_right() flip_up_down() random_flip_up_down()  左右と上下の反転。randomは1/2で反転する。
fliph = tf.image.flip_left_right(im) flipv = tf.image.flip_up_down(im) with tf.Session() as sess: results = sess.run([fliph, flipv]) plt.imshow(np.hstack(results))  Rotating  rot90()  反時計周りに90度回転させる。
rot90 = tf.image.rot90(im) with tf.Session() as sess: results = sess.</description>
    </item>
    
    <item>
      <title>MLPと誤差逆伝搬法(Backpropagation)</title>
      <link>https://www.sambaiz.net/article/192/</link>
      <pubDate>Sun, 21 Oct 2018 19:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/192/</guid>
      <description>MLP(多層パーセプトロン)は入力層と出力層の間に隠れ層を重ねることによって、 ロジスティック回帰(単純パーセプトロン)ではできなかった非線形分離をできるようにしたニューラルネットワークモデル。
ロジスティック回帰の尤度と交差エントロピー誤差と勾配降下法 - sambaiz-net
入出力がx、y、層の数がLでl層目での重みとバイアスをW^(l), b^(l)、活性化関数をf^(l)、活性化関数適用前後をu^(l)とh^(l)とし、入力層を0層目とすると各層での演算は以下の式で表される。
活性化関数は非線形で微分可能な関数で、計算速度や勾配消失の面でReLUが最有力。
ニューラルネットワークと活性化関数 - sambaiz-net
各層の最適なWとbを探すのにロジスティック回帰と同様に勾配降下法を使うことができる。 誤差関数は分類の場合は交差エントロピーが、回帰の場合は平均二乗誤差(MSE, Mean Squared Error) または外れ値に引っ張られづらくしたHuber損失などが使われる。
隠れ層の勾配はそれより後ろの層での演算が影響するので、入力から出力への順伝搬に対して 出力から入力への逆伝播で誤差の情報を前の層に伝播させていく。これを誤差逆伝播法(Backpropagation)という。
出力から遠くなればなるほど連鎖律が長くなっていくが、途中までは後ろの層と共通になっている。 ということで順伝搬時のhを保存しておき一つ後ろの層のWと誤差δを渡してやれば必要最小限の演算で済み、実行時間を短くすることができる。
参考 深層学習</description>
    </item>
    
    <item>
      <title>ロジスティック回帰の尤度と交差エントロピーと勾配降下法</title>
      <link>https://www.sambaiz.net/article/191/</link>
      <pubDate>Sun, 14 Oct 2018 23:28:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/191/</guid>
      <description>ロジスティック回帰 単純パーセプトロンの活性化関数を0/1のステップ関数ではなく0~1のシグモイド関数σにしたモデルで、分類の確率を返すことができる。
ニューラルネットワークと活性化関数 - sambaiz-net
線形分離不可能な場合はうまくいかない。入力と出力の間に隠れ層があるMLPでは非線形分離もできる。
MLPと誤差逆伝搬法(Backpropagation) - sambaiz-net
尤度関数と交差エントロピー誤差関数 尤度(likelihood)関数はXという事象が観察されたときにC=tである尤もらしさを表す関数。 例えば、6面ダイスを2回振って両方1の目が出た(X)ときに1の目が出る確率が1/6&amp;copy;である尤度は(1&amp;frasl;6)*(1&amp;frasl;6)=1/36となる。
学習ではモデルのパラメータw,bを、各入力x(x1,x2,&amp;hellip;,xn)に対して正解であるC=tの尤度の和が最大になるように最適化していく。 通常のロジスティック回帰では二値分類を行うので正解データtは{0,1}とし、P(C=1) = 1-P(C=0)となる。
ただ、このままだと積になっていて計算しづらいので、対数を取って和にして、 損失として扱うため負の数にする。これを交差エントロピー誤差関数という。 この値を最小化させるということは尤度を最大化させることになる。
自己情報量、エントロピー、KL情報量、交差エントロピー - sambaiz-net
勾配降下法 誤差をw,bでそれぞれ偏微分したのを引いてパラメータを更新していき、 勾配が0になるような値を探す。
ηは学習率で正の小さな値にする。 大きすぎると収束しないが、小さすぎても収束に必要なステップ数が増え、さらに局所最適解で止まってしまう可能性が高まるので 最初は大きくして徐々に小さくしていったりする。
ほかに局所最適解で止まるのを避ける手法として、サンプル全体ではなく毎回異なる一部を使う(Minibatch)確率的勾配降下法(SGD: Stochastic Gradient Descent)や、SGDに慣性を追加したMomentumなどがある。
多クラスロジスティック回帰 活性化関数をsoftmax関数にすると多クラス分類できる。
多クラスの場合の正解データtは{0,1,2,&amp;hellip;}といったようにはせず 正解のindexだけ1でほかは0のone-hot vectorで表し、尤度関数、交差エントロピー誤差関数は以下のようになる。
偏微分するとこうなる。
あとは同様に勾配降下法でパラメータを更新していく。
参考 詳解 ディープラーニング ~TensorFlow・Kerasによる時系列データ処理~</description>
    </item>
    
    <item>
      <title>Destributed TensorFlowの流れとSavedModelの出力</title>
      <link>https://www.sambaiz.net/article/179/</link>
      <pubDate>Wed, 25 Jul 2018 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/179/</guid>
      <description>Distributed TensorFlow クラスタを組んでGraphを分散実行する。
クラスタは
 master: sessionを作成し、workerを制御する worker: 計算を行う ps(parameter server): 変数の値を持ち、更新する  のjobからなり、gRPCの
 Master Service Worker Service  でやり取りする。
TensorFlow serverを立てる 各jobとURLのmapをClusterSpecにして jobとindexと併せてServerDefを作って Serverを立てる。
{ &amp;quot;master&amp;quot;: [ &amp;quot;check-tf-config-master-34z8-0:2222&amp;quot; ], &amp;quot;ps&amp;quot;: [ &amp;quot;check-tf-config-ps-34z8-0:2222&amp;quot;, &amp;quot;check-tf-config-ps-34z8-1:2222&amp;quot; ], &amp;quot;worker&amp;quot;: [ &amp;quot;check-tf-config-worker-34z8-0:2222&amp;quot;, &amp;quot;check-tf-config-worker-34z8-1:2222&amp;quot; ] }  cluster_spec_object = tf.train.ClusterSpec(cluster_spec) server_def = tf.train.ServerDef( cluster=cluster_spec_object.as_cluster_def(), protocol=&amp;quot;grpc&amp;quot;, job_name=job_name, # worker, master, ps task_index=0) server = tf.train.Server(server_def)  psのjobではserver.join()して待ち構える。
if job_name == &amp;quot;ps&amp;quot;: server.join() else: # build model  WorkerにGraphを割り当てる workerのdeviceにGraphを割り当てる。 deviceは/job:worker/replica:0/task:0/device:GPU:0 のようなフォーマットで表される。</description>
    </item>
    
    <item>
      <title>TensorFlowのMonitoredSessionとSessionRunHookとsummaryのエラー</title>
      <link>https://www.sambaiz.net/article/175/</link>
      <pubDate>Sun, 01 Jul 2018 23:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/175/</guid>
      <description>MonitoredSession
deprecatedになったSupervisorの後継。
MonitoredTrainingSessionで学習用のMonitoredSessionを生成する。 このコンストラクタの引数でcheckpoint_dirを渡すと内部でCheckpointSaverHookが 追加されるようになっていて、restoreしたり指定したタイミングでsaveしたりしてくれる。
なので今回明示的に渡すhooksは 指定したstepに到達したら止めてくれる、StopAtStepHookのみ。
should_stop()がTrueな状態でsession.run()しようとするとRun called even after should_stop requested.のエラーが出るため、 今回は新しいsessionを作ってAccuracyを返しているが、hookでやった方がrestoreする必要がないので良さそうだ。
Destributed TensorFlowの流れとSavedModelの出力 - sambaiz-net
全体のコードはここ。
def train(self, learning_rate, variable_default_stddev, bias_default, last_step=800): test_images = self.images[:500] test_labels = self.labels[:500] train_batch = Batch(self.images[500:], self.labels[500:]) with tf.Graph().as_default(): global_step=tf.train.get_or_create_global_step() g = MNIST_CNN(learning_rate, variable_default_stddev, bias_default).graph() saver = tf.train.Saver() savedir = &#39;./ckpt-{}-{}-{}&#39;.format(learning_rate, variable_default_stddev, bias_default) hooks = [ tf.train.StopAtStepHook(last_step=last_step) ] with tf.train.MonitoredTrainingSession( hooks=hooks, checkpoint_dir=savedir, save_checkpoint_secs = 300, ) as sess: sess.run(global_step) while not sess.should_stop(): # step = sess.</description>
    </item>
    
    <item>
      <title>TensorFlowのモデルをsave/loadする</title>
      <link>https://www.sambaiz.net/article/172/</link>
      <pubDate>Fri, 22 Jun 2018 01:48:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/172/</guid>
      <description>SavedModelBuilderで モデルを言語に依存しないSavedModelのprotobufにして保存できる。 SavedModelにはSaverによって出力されるCheckpointを共有する一つ以上のMetaGraphDefを含む。
import tensorflow as tf def build_signature(signature_inputs, signature_outputs): return tf.saved_model.signature_def_utils.build_signature_def( signature_inputs, signature_outputs, tf.saved_model.signature_constants.REGRESS_METHOD_NAME) def save(sess, export_dir, signature_def_map): builder = tf.saved_model.builder.SavedModelBuilder(export_dir) builder.add_meta_graph_and_variables( sess, [tf.saved_model.tag_constants.SERVING], signature_def_map=signature_def_map ) builder.save() import shutil import os.path export_dir = &amp;quot;./saved_model&amp;quot; if os.path.exists(export_dir): shutil.rmtree(export_dir) with tf.Graph().as_default(): a = tf.placeholder(tf.float32, name=&amp;quot;a&amp;quot;) b = tf.placeholder(tf.float32, name=&amp;quot;b&amp;quot;) c = tf.add(a, b, name=&amp;quot;c&amp;quot;) v = tf.placeholder(tf.float32, name=&amp;quot;v&amp;quot;) w = tf.Variable(0.0, name=&amp;quot;w&amp;quot;) x = w.assign(tf.add(v, w)) sv = tf.train.Supervisor() with sv.managed_session() as sess: print(sess.</description>
    </item>
    
    <item>
      <title>ベイズ最適化でランダムフォレストとXGBoostの良いハイパーパラメータを探す</title>
      <link>https://www.sambaiz.net/article/169/</link>
      <pubDate>Sun, 10 Jun 2018 17:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/169/</guid>
      <description>ベイズ最適化で良いハイパーパラメータを効率的に探す。 他の方法としてscikit-learnにもあるグリッドサーチがあるが、総当りなので時間がかかる。
まず現在の最大値を超える確率や期待値を出力とする獲得関数を決めて、ガウス過程(GP)に従うと仮定する。 ガウス過程は回帰関数の確率モデルで、任意の入力(x1, x2, &amp;hellip; , xn)に対応する出力(y1, y2, &amp;hellip;, yn)がガウス分布(=正規分布)に従うというもの。 これによって予測されるまだ試していない入力での期待値や分散から次に試す値を決める。
今回はKaggleのTitanicのチュートリアルを、チューニングなしのランダムフォレストとXGBoostで解いたときの結果と比較して、ベイズ最適化によるハイパーパラメータで精度が向上するか確認する。
KaggleのTitanicのチュートリアルをランダムフォレストで解く - sambaiz-net
KaggleのTitanicのチュートリアルをXGBoostで解く - sambaiz-net
ランダムフォレスト Pythonのベイズ最適化のライブラリ、BayesianOptimizationを使う。
$ pip install bayesian-optimization  RandomForestClassifierのハイパーパラメータ
 n_estimators: 木の数 min_samples_split: ノードを分割するのに必要な最小サンプル数 max_features: 分割するときに考慮する特徴量の割合  の値を探すため、BayesianOptimizationに最大化したい値(精度)とパラメータの範囲を渡す。
from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier from bayes_opt import BayesianOptimization import pandas as pd def preprocess(df): df[&#39;Fare&#39;] = df[&#39;Fare&#39;].fillna(df[&#39;Fare&#39;].mean()) df[&#39;Age&#39;] = df[&#39;Age&#39;].fillna(df[&#39;Age&#39;].mean()) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].fillna(&#39;Unknown&#39;) df[&#39;Sex&#39;] = df[&#39;Sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].</description>
    </item>
    
    <item>
      <title>KaggleのTitanicのチュートリアルをXGBoostで解く</title>
      <link>https://www.sambaiz.net/article/168/</link>
      <pubDate>Sat, 02 Jun 2018 18:16:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/168/</guid>
      <description>XGBoostは高性能なGradient Boostingのライブラリ。 Boostingというのはアンサンブル学習の種類の一つで、ランダムフォレストのように弱学習器をそれぞれ並列に学習するBaggingに対して、 順番に前回までの結果を受けながら学習し、結果をまとめる際にそれぞれの重みを掛けるもの。 XGBoostではランダムフォレストと同様に決定木を弱学習器とする。
KaggleのTitanicのチュートリアルをランダムフォレストで解く - sambaiz-net
$ pip install xgboost  データの前処理はランダムフォレストと同じようにした。 パラメータの objective(目的関数)には二値分類なのでbinary:logisticを指定し、確率が返るのでroundして出力している。
import pandas as pd import xgboost as xgb from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def preprocess(df): df[&#39;Fare&#39;] = df[&#39;Fare&#39;].fillna(df[&#39;Fare&#39;].mean()) df[&#39;Age&#39;] = df[&#39;Age&#39;].fillna(df[&#39;Age&#39;].mean()) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].fillna(&#39;Unknown&#39;) df[&#39;Sex&#39;] = df[&#39;Sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].map( {&#39;S&#39;: 0, &#39;C&#39;: 1, &#39;Q&#39;: 2, &#39;Unknown&#39;: 3} ).astype(int) df = df.drop([&#39;Cabin&#39;,&#39;Name&#39;,&#39;PassengerId&#39;,&#39;Ticket&#39;],axis=1) return df def train(df): train_x = df.</description>
    </item>
    
    <item>
      <title>KaggleのTitanicのチュートリアルをランダムフォレストで解く</title>
      <link>https://www.sambaiz.net/article/166/</link>
      <pubDate>Tue, 29 May 2018 09:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/166/</guid>
      <description>ランダムフォレストはデータや特徴量をランダムにサンプリングして決定木を複数生成し並列に学習するアンサンブル学習のBaggingという種類の手法。 決定木なので特徴量の影響が分かりやすく、単一の決定木と比べて過学習を防ぐことができる。
KaggleのTitanicのチュートリアルをXGBoostで解く - sambaiz-net
train.csvとtest.csvをKaggleからダウンロードする。 csvにはタイタニックの乗客者リストが含まれ、test.csvには生還したかを表すSurvivedが抜けている。 これを予測するのがこのコンペティションの目的だ。
データのうちFareやAge、Embarkedは入っていないものがあって、これらの欠損値をどう扱うという問題がある。
df = pd.read_csv(&#39;./train.csv&#39;) print(len(df)) print(df.isnull().sum())  891 PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64  連続値をとるFareとAgeは平均を取り、Embarkedは欠損値用の値にしてみた。数値化できないものについては除いている。
def preprocess(df): df[&#39;Fare&#39;] = df[&#39;Fare&#39;].fillna(df[&#39;Fare&#39;].mean()) df[&#39;Age&#39;] = df[&#39;Age&#39;].fillna(df[&#39;Age&#39;].mean()) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].fillna(&#39;Unknown&#39;) df[&#39;Sex&#39;] = df[&#39;Sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].map( {&#39;S&#39;: 0, &#39;C&#39;: 1, &#39;Q&#39;: 2, &#39;Unknown&#39;: 3} ).</description>
    </item>
    
    <item>
      <title>TensorFlow/RNNで連続的な値の時系列データを予測する</title>
      <link>https://www.sambaiz.net/article/154/</link>
      <pubDate>Sun, 11 Feb 2018 19:49:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/154/</guid>
      <description>TensorFlowのRNN(LSTM)のチュートリアルのコードを読む - sambaiz-net
チュートリアルで扱ったのは語彙数分の単語、つまり離散的な値だったが、今回は連続的な値を取る場合のモデルを作る。 全体のコードはここ。
入力 以下の関数によって生成した1次元のデータ列。 これをstrideした最後のデータ、つまり時系列的に次に来るものを予測させる。
def make_time_series_data(size): data = [] for i in range(size): data.append(sin(random.normalvariate(i,0.1)*0.1)) return np.reshape(np.array(data, dtype=np.float32), (size,1)) def make_batch(data, batch_size, num_steps, num_dimensions, name=None): epoch_size = data.size // (batch_size*num_steps*num_dimensions) data = np.lib.stride_tricks.as_strided( data, shape= (epoch_size, batch_size, num_steps+1, num_dimensions), strides=( 4*batch_size*num_steps*num_dimensions, 4*num_steps*num_dimensions, 4*num_dimensions, 4 # bytes ), writeable=False ) return data[:, :, :-1], data[:, :, 1:]  モデル input layerでLSTMのhidden_sizeに合わせて、output layerで予測値を得ている。 lossはMSE(Mean squared error)。OptimizerはGradientDecentOptimizerを使っている。
チュートリアルでは自力で各time_stepの値を入れていたけど、 今回はdynamic_rnn()に任せている。</description>
    </item>
    
    <item>
      <title>TensorFlowのRNN(LSTM)のチュートリアルのコードを読む</title>
      <link>https://www.sambaiz.net/article/146/</link>
      <pubDate>Wed, 03 Jan 2018 21:12:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/146/</guid>
      <description>TensorflowのRNN(Recurrent Neural Networks)のチュートリアルのコードを読む。これは文章のそれまでの単語の履歴から、その次に続く単語を予測することで言語モデルを作るもの。
RNN/LSTMとは RNNは入力に対して出力のほかに情報を次のステップに渡すことで時系列データで学習できるようにするネットワーク。 展開すると同じネットワークに単語を一つずつ入れていくように表現できる。
これを単純にMLPで実装しようとすると逆誤差伝搬する際に過去の層にも伝搬させる(BPTT: Backpropagation through time)必要があり、 時間を遡るほど活性化関数の微分係数が再帰的に繰り返し掛けられるため勾配が消失や爆発しやすくなってしまう。 また、時系列データのうちに発火したいものと発火したくないものが混在している場合、同じ重みにつながっているため更新を打ち消しあってしまう入力/出力重み衝突という問題もある。
これらを解決するのがLSTM(Long Short Term Memory networks)で、 勾配消失は活性化関数がxで重みが単位行列のニューロンのCEC(Constant Error Carousel)によって常に誤差に掛けられる係数を1にすることで防ぎ、 入力/出力重み衝突は必要な入出力を通したり不必要な情報は忘れさせるために値域(0,1)の値を掛けるinput gate、forget gate、output gateによって回避する。gateは入力と前回の出力によって制御される。
TensorflowではいくつかLSTMの実装が用意されていて、CudnnLSTMやBasicLSTMCell、LSTMBlockCellなどがある。 cuDNNというのはNVIDIAのCUDAのDNNライブラリのこと。 LSTMBlockCellはもう少し複雑なLSTMでBasicLSTMCellよりも速い。
動かしてみる $ git clone https://github.com/tensorflow/models.git $ cd models/tutorials/rnn/ptb/ $ wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz $ tar xvf simple-examples.tgz $ python3 -m venv env $ . ./env/bin/activate $ pip install numpy tensorflow  $ python ptb_word_lm.py --data_path=simple-examples/data/ --num_gpus=0 Epoch: 1 Learning rate: 1.000 0.004 perplexity: 5534.452 speed: 894 wps 0.</description>
    </item>
    
    <item>
      <title>Lpノルムと正則化</title>
      <link>https://www.sambaiz.net/article/137/</link>
      <pubDate>Thu, 12 Oct 2017 23:48:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/137/</guid>
      <description>ノルムとは ノルムはベクトル空間の距離を表す、以下の条件を満たす関数p。
 p(av) = |a| p(v): スケーラブル p(u + v) ≦ p(u) + p(v): 三角不等式を満たす p(v) ≧ 0: 負の値を取らない p(v) = 0 &amp;lt;=&amp;gt; v=0: 距離が0 &amp;lt;=&amp;gt; 零ベクトル  以下の式で表されるノルムをLpノルムと呼ぶ。
L1ノルム(マンハッタン距離) 絶対値の和。座標軸方向にしか移動できない縛りでの距離。 StreetとAvenueが格子状になっているマンハッタンではタクシーが移動する距離はL1ノルムのようになる。
L2ノルム(ユークリッド距離) 2乗の和の平方根。普通の距離。
正則化(regularization) 機械学習で過学習を防ぐためのもの。 Lp正則化は重みのLpノルムをp乗してハイパーパラメータΛを掛けたものを正則化項として 素の損失関数に加える。これを最小化するのだから重みがペナルティとしてはたらく。 L1正則化ではΛが大きければいくつかの重みが0になって次元削減できるが、 L2正則化では重みに比例して小さくなるだけ。それぞれLasso回帰、Ridge回帰ともいう。 また、これらを割合で足して使うElasticNetというものもある。
参考 Norm (mathematics) - Wikipedia
RでL1 / L2正則化を実践する - 六本木で働くデータサイエンティストのブログ</description>
    </item>
    
    <item>
      <title>自己情報量、エントロピー、KL情報量、交差エントロピーと尤度関数</title>
      <link>https://www.sambaiz.net/article/134/</link>
      <pubDate>Mon, 25 Sep 2017 23:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/134/</guid>
      <description>自己情報量 P(ω)の確率で起きる事象ωの自己情報量は以下の式で定義される。logの底を2にしてbitsで表すのが一般的。
log(P)+log(Q)=log(P*Q)より加法性がある。 例えば、サイコロで1の目が2回連続で出る(P=1&amp;frasl;36)情報量(5.16bits)はサイコロで1の目が出る(P=1&amp;frasl;6)情報量(2.58bits)の2倍と等しい。 確率が高ければ高いほど自己情報量は小さくなり、P(ω)=1では0bitになる。
エントロピー 確率分布Pに従う確率変数Xのエントロピーは以下の式で定義される。情報量の平均。
これは情報を送る際に必要なビット数の平均の下限になっている。 例えば、Xが1~4の値を(0.8, 0.1, 0.06, 0.04)の確率でとるとする。 4通りなのだからそれぞれ2bits(00, 01, 10, 11)のコードで表すこともできるが、 ほとんど3や4は出ないのだからbit数を偏らせて(0, 10, 110, 111)のコードで表すと 0.8*1 + 0.1*2 + 0.06*3 + 0.04*3 = 1.3bitsまで減らすことができる。 この場合のエントロピーは1.01bitsで、これより小さくすることはできない。
カルバック・ライブラー情報量 離散確率分布PのQに対するカルバック・ライブラー情報量は以下の式で定義される。連続確率分布では積分する。 Qの自己情報量からPの自己情報量を引いて平均を取ったもので、分布間の距離のように考えることができる。非負の値を取る。
交差エントロピー 離散確率分布PとQの交差エントロピーは以下の式で定義される。連続確率分布では積分する。 PのエントロピーにPのQに対するKL情報量を足したもの。
これはQの分布に最適化されたコードでPの分布の確率変数の情報を送ってしまった際に必要なビット数の平均の下限になっている。KL情報量が余分な分。機械学習の損失関数に使われる。
ロジスティック回帰と尤度関数/交差エントロピー誤差と勾配降下法 - sambaiz-net
参考 Self-information - Wikipedia
Kullback–Leibler divergence - Wikipedia
情報理論を視覚的に理解する (3&amp;frasl;4) | コンピュータサイエンス | POSTD</description>
    </item>
    
    <item>
      <title>ニューラルネットワークと活性化関数</title>
      <link>https://www.sambaiz.net/article/133/</link>
      <pubDate>Mon, 18 Sep 2017 23:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/133/</guid>
      <description>活性化関数というのは各層での重み掛けバイアス足しのあとに適用する非線形の関数。 これはカーネル法のように空間を変換して線形分離できないデータを線形分離できるようにするはたらきをする。 線形な関数を使うと層を重ねても結局線形のままで、空間もそのまま伸縮するだけなので目的を果たさない。
バックプロバゲーション(誤差逆伝播法)するために微分できる必要がある。
MLPと誤差逆伝搬法(Backpropagation) - sambaiz-net
Tensorflowでは以下の活性化関数が用意されている。
sigmoid 値域は(0,1)でシグマの語末系ςに似たS字を描く。 微分係数がそれほど大きくないので何層もこの関数を適用すると、バックプロバゲーションで微分係数を掛けていった結果、勾配が消失する問題がありあまり使われない。値域が(-1,1)で似たグラフを描くtanh(Hyperbolic tangent)もある。
softsign tanhと比べて漸近線に近づく速度が遅くなっている。 それほど性能は変わらないが、初期化においてロバストになるはたらきがあるようだ。
softplus ReLUに続く。
ReLU(Rectified Linear Unit) 単純だけど最有力。勾配消失も起きにくい。x=0で微分できないが0か1として扱われる。
def deriv_relu(x): return np.where(x &amp;gt; 0, 1, 0)  softplusと比べてexpやlogを含まない分高速に計算できるので、 膨大で複雑なデータセットに対して多くの層を用いることができる。
0以下は等しく0になるため、学習中に落ちてしまうとニューロンが死んでしまう。 これを避けるため0以下のときy = exp(x) - 1にするELU(Exponential Linear Unit) というのもある。
比較的起きにくいとはいえ、層を深くすると勾配消失する可能性は高まる。 活性化関数ごとに異なる重みの初期値によってこれを緩和でき、ReLUでは入力次元数によるHe Initializationというのが提案されている。
rng = np.random.RandomState(1234) n_in = 10 # 入力次元数 rng.uniform( low=-np.sqrt(6/n_in), high=+np.sqrt(6/n_in), size=5 ) # He Initialization  参考 Activation functions and it’s types-Which is better?</description>
    </item>
    
  </channel>
</rss>