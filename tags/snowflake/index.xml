<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Snowflake on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/snowflake/</link>
    <description>Recent content in Snowflake on sambaiz-net</description>
    <generator>Hugo</generator>
    <language>ja</language>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Mon, 05 May 2025 18:56:00 +0900</lastBuildDate>
    <atom:link href="https://www.sambaiz.net/tags/snowflake/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Snowflake の Spark コネクタでクエリを実行する</title>
      <link>https://www.sambaiz.net/article/534/</link>
      <pubDate>Mon, 05 May 2025 18:56:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/534/</guid>
      <description>&lt;p&gt;アプリケーションから Snowflake のデータを参照する方法として JDBC で接続する &lt;a href=&#34;https://docs.snowflake.com/en/user-guide/spark-connector&#34;&gt;Spark コネクター&lt;/a&gt;や &lt;a href=&#34;https://docs.snowflake.com/en/developer-guide/snowpark/index&#34;&gt;Snowpark API&lt;/a&gt; がある。いずれの場合も基本的には Snowflake のコンピューティングリソースを用いてクエリが遅延評価されるが、Spark の UDF など変換できない処理は &lt;a href=&#34;https://docs.snowflake.com/en/user-guide/spark-connector-use#pushdown&#34;&gt;Spark クラスタ側で実行される&lt;/a&gt;のに対して、Snowpark API は全ての処理が Snowflake 側で行われることが保証されておりクラスタを立てなくてもよいという点で推奨されている。ただ、Snowflake 外に大きなデータがあるなど Spark で処理する場合は Spark の Dataframe を返す Spark コネクターが便利。&lt;/p&gt;</description>
    </item>
    <item>
      <title>dbt で BigQuery tables for Apache Iceberg を作成し Snowflake から読む</title>
      <link>https://www.sambaiz.net/article/531/</link>
      <pubDate>Sun, 20 Apr 2025 23:55:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/531/</guid>
      <description>&lt;p&gt;Snowflake から BigQuery に入っているデータを読む際、直接アクセスすることはできないためSnowflake のテーブルに入れ直すか GCS にエクスポートなどする必要がある。ただ、前者に関しては単一ソースでなくなることによるデータやスキーマの乖離の心配があり、いずれの場合も BigQuery と併用する場合データの保存コストが二重でかかってしまう。後者の場合は BigQuery ストレージに保存するのではなく外部テーブルとして運用する方法もあるが書き込むのに Spark のような外部エンジンが必要になる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Snowflake の Role を Terraform で作成しユーザーにテーブルへのアクセス権限を与える</title>
      <link>https://www.sambaiz.net/article/530/</link>
      <pubDate>Thu, 13 Mar 2025 22:42:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/530/</guid>
      <description>&lt;p&gt;Snowflake でオブジェクトの OWNERSHIP &lt;a href=&#34;https://docs.snowflake.com/ja/user-guide/security-access-control-privileges#all-privileges-alphabetical&#34;&gt;権限&lt;/a&gt;を持たない他のユーザーでもアクセスできるようにするには&lt;a href=&#34;https://docs.snowflake.com/en/user-guide/security-access-control-overview&#34;&gt;ロール&lt;/a&gt;で権限を与える必要がある。システムロールも例外ではなく、ACCOUNTADMIN であっても権限がないと何もできないので階層の最上位のカスタムロールを SYSADMIN に GRANT することが&lt;a href=&#34;https://docs.snowflake.com/ja/user-guide/security-access-control-considerations#managing-custom-roles&#34;&gt;推奨されている&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/524/&#34;&gt;キーペア認証で Terraform を実行したり Snowflake CLI や gosnowflake でクエリを実行する - sambaiz-net&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Iceberg テーブルを Glue Data Catalog に登録して Athena や Snowflake からクエリする</title>
      <link>https://www.sambaiz.net/article/525/</link>
      <pubDate>Thu, 30 Jan 2025 20:31:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/525/</guid>
      <description>&lt;p&gt;Glue Data Catalog に Iceberg テーブルを登録すると、他のテーブルと同様に Athena や EMR などから参照できるほか、Snowflake ではスキーマを渡すことなく &lt;a href=&#34;https://docs.snowflake.com/en/user-guide/tables-iceberg&#34;&gt;ICEBERG TABLE&lt;/a&gt; を作成することができ、メタデータの二重管理を避けることができる。また自動で &lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/enable-compaction.html&#34;&gt;compaction したり&lt;/a&gt;、ジョブの失敗時に生まれるメタデータから参照されていない&lt;a href=&#34;https://docs.aws.amazon.com/glue/latest/dg/enable-orphan-file-deletion.html&#34;&gt;orphan files を掃除したり&lt;/a&gt;する機能まである。従来の Hive パーティションでは Glue Data Catalog にパーティション情報を登録し GetPartitions API で取得するため、パーティションが多いとレイテンシやスロットリングが問題になるが、Iceberg はパーティション情報を manifest ファイルとして S3 に保持するため、この問題を回避できる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>キーペア認証で Terraform を実行したり Snowflake CLI や gosnowflake でクエリを実行する</title>
      <link>https://www.sambaiz.net/article/524/</link>
      <pubDate>Tue, 28 Jan 2025 01:40:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/524/</guid>
      <description>&lt;p&gt;Snowflake CLI はじめ各種クライアントライブラリは &lt;a href=&#34;https://docs.snowflake.com/en/developer-guide/snowflake-cli/connecting/configure-connections&#34;&gt;user_name / password&lt;/a&gt; による認証に加えてキーペアでの認証にも&lt;a href=&#34;https://docs.snowflake.com/en/user-guide/key-pair-auth#supported-snowflake-clients&#34;&gt;対応している&lt;/a&gt;。&lt;/p&gt;&#xA;&lt;p&gt;ユーザーに紐づける鍵は 2048 bits 以上の RSA 鍵である&lt;a href=&#34;https://docs.snowflake.com/ja/user-guide/key-pair-auth#overview&#34;&gt;必要があり&lt;/a&gt; ed25519 には現状対応していない。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spark で Iceberg テーブルを作成しスキーマや write mode を変更してデータを書き込みメタデータの内容を確認する</title>
      <link>https://www.sambaiz.net/article/523/</link>
      <pubDate>Sat, 25 Jan 2025 23:18:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/523/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://iceberg.apache.org/&#34;&gt;Apache Iceberg&lt;/a&gt; は大規模なデータの処理を効率的に行えるようにするオープンなテーブルフォーマットで &lt;a href=&#34;https://trino.io/docs/current/connector/iceberg.html&#34;&gt;Trino&lt;/a&gt;、&lt;a href=&#34;https://docs.snowflake.com/en/user-guide/tables-iceberg&#34;&gt;Snowflake&lt;/a&gt; など様々なシステムから利用できる。ACID なトランザクション、元データを書き換えることなくテーブルのスキーマを変更できる Schema Evolution、タイムトラベルに加え、Hive の day=xxxx/ のような物理構造に依存せず day(event_ts) や month(event_ts) といったテーブルには含まれない論理的な値によって PARTITION BY する &lt;a href=&#34;https://iceberg.apache.org/docs/latest/partitioning/#icebergs-hidden-partitioning&#34;&gt;Hidden Partitioning&lt;/a&gt; によって、パーティションの粒度を意識することなく WHERE event_ts BETWEEN 2025-01-01 AND 2025-03-01 のようにクエリでき、またクエリを壊すことなく粒度を変更する &lt;a href=&#34;https://iceberg.apache.org/docs/latest/evolution/#partition-evolution&#34;&gt;Partition evolution&lt;/a&gt; を行うこともできる。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Streamlit でウィジェットの値を元にデータを描画するアプリケーションを作って公開する</title>
      <link>https://www.sambaiz.net/article/521/</link>
      <pubDate>Sat, 18 Jan 2025 14:25:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/521/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://streamlit.io/&#34;&gt;Streamlit&lt;/a&gt; は Python でデータアプリケーションを開発するためのツール。&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:2;-o-tab-size:2;tab-size:2;-webkit-text-size-adjust:none;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ pip install streamlit&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$ streamlit --version&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Streamlit, version 1.41.1&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;button や slider、radio といった &lt;a href=&#34;https://docs.streamlit.io/develop/api-reference/widgets&#34;&gt;widgets&lt;/a&gt; で入力した値をもとに Dataframe を作って write() や line_chart() で描画できる。widgets の値を変更するたびに&lt;a href=&#34;https://docs.streamlit.io/get-started/fundamentals/main-concepts#data-flow&#34;&gt;更新が走る&lt;/a&gt;ので、必要に応じて session_state に値を保持したり重い処理の結果を&lt;a href=&#34;https://docs.streamlit.io/develop/concepts/architecture/caching&#34;&gt;キャッシュ&lt;/a&gt;したりする。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Snowflake に S3 のデータをコピーしてクエリする</title>
      <link>https://www.sambaiz.net/article/518/</link>
      <pubDate>Tue, 31 Dec 2024 23:57:00 +0900</pubDate>
      <guid>https://www.sambaiz.net/article/518/</guid>
      <description>&lt;p&gt;Snowflake はAWS、GCP、Azure上で動作するフルマネージドなデータプラットフォーム。カラムナフォーマットで&lt;a href=&#34;https://docs.snowflake.com/en/user-guide/intro-key-concepts#database-storage&#34;&gt;データを持ち&lt;/a&gt;、各ノードが共有のデータセットの一部をローカルにキャッシュする&lt;a href=&#34;https://docs.snowflake.com/en/user-guide/intro-key-concepts#snowflake-architecture&#34;&gt;ハイブリッドなシェアードナッシングアーキテクチャ&lt;/a&gt;でネットワークトラフィックを削減したり、自動で&lt;a href=&#34;https://docs.snowflake.com/en/user-guide/tables-clustering-micropartitions&#34;&gt;マイクロパーティション&lt;/a&gt;に切ってきめ細かなプルーニングを行うことで効率的に大規模なクエリを実行できる。&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.sambaiz.net/article/386/&#34;&gt;カラムナフォーマットParquetの構造とReadの最適化 - sambaiz-net&lt;/a&gt;&lt;/p&gt;&#xA;&lt;p&gt;Hadoop クラスタなどのインフラストラクチャの面倒を見る必要がないこと以外にも、BigQuery のような&lt;a href=&#34;https://docs.snowflake.com/en/user-guide/data-time-travel&#34;&gt;タイムトラベル&lt;/a&gt;や、アカウント間で容易に&lt;a href=&#34;https://docs.snowflake.com/en/user-guide/data-sharing-intro&#34;&gt;データを共有&lt;/a&gt;できる特長がある。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
