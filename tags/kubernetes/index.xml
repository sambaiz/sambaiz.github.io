<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/kubernetes/</link>
    <description>Recent content in kubernetes on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Fri, 04 Dec 2020 12:30:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>IstioのSidecarでmTLS認証を行いServiceAccountによるアクセス制御を行う</title>
      <link>https://www.sambaiz.net/article/317/</link>
      <pubDate>Fri, 04 Dec 2020 12:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/317/</guid>
      <description>mTLS (Mutual TLS)認証は、RFC8705に記載されている、TLSハンドシェイクの際にサーバーからだけではなくクライアントからも証明書を送ることで相互に認証を行う手法。クライアントは事前に自身の秘密鍵で生成したCSRをサーバーに送り、サーバーがルート認証局となって発行した証明書を取得しておく。 KubernetesではIstioのSidecarを通すことで透過的にmTLS認証を行うことができる。 実際に試してみる。なお、証明書はEnvoy SDS(secret discovery service) APIのリクエストによってCSRが定期的にistiodに送られて発行、更新されるらしい。
Istio OperatorでIstioをインストールする。 以前はHelmで入れていたが今はこれがベストプラクティスのようだ。
IstioをHelmでインストールしてRoutingとTelemetryを行いJaeger/Kialiで確認する - sambaiz-net
$ curl -L https://istio.io/downloadIstio | sh - $ istio-1.8.0/bin/istioctl version no running Istio pods in &amp;#34;istio-system&amp;#34; 1.8.0 $ istioctl operator init Installing operator controller in namespace: istio-operator using image: docker.io/istio/operator:1.8.0 Operator controller will watch namespaces: istio-system ✔ Istio operator installed ✔ Installation complete $ kubectl get pods --namespace istio-operator NAME READY STATUS RESTARTS AGE istio-operator-76766bc79-lfm49 1/1 Running 0 2m32s $ kubectl create ns istio-system $ kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: install.</description>
    </item>
    
    <item>
      <title>EKSにKubeflowをインストールする</title>
      <link>https://www.sambaiz.net/article/316/</link>
      <pubDate>Sun, 29 Nov 2020 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/316/</guid>
      <description>Kubernetes上で機械学習を行うためのツールキットKubeflowを EKSにインストールする。 m5.large * 4のクラスタをCDKで作成した。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
まずKubeflowのCLIツールkfctlをインストールする。 内部でeksctlが呼ばれるがフラグでprofileを指定できないので環境変数に入れておく。
$ curl -L https://github.com/kubeflow/kfctl/releases/download/v1.2.0/kfctl_v1.2.0-0-gbc038f9_darwin.tar.gz &amp;gt; kfctl.tar.gz $ tar -xvf kfctl.tar.gz $ ./kfctl version kfctl v1.2.0-0-gbc038f9 $ eksctl version 0.32.0 $ export AWS_PROFILE=*** AWS用の設定ファイルを持ってきて、RegionやRole、Cognito UserPoolあるいはusernameとpasswordによる認証の設定をKfAwsPluginに書く。UserPoolの作成もCDKで行える。それとACMで証明書を発行しておく。
CDKでCognito UserPoolとClientを作成しトリガーやFederationを設定する - sambaiz-net
AWSのサービスにアクセスするのにServiceAccountに関連づけられたRoleを用いる場合はenablePodIamPolicy: trueにして、ワーカーノードのRoleを用いる場合はrolesにそのロール名を書く。 KfDefにはManifestごとのKustomizeに関する設定が並んでいるがそのままで問題ない。
kustomizeでkubernetesのmanifestを環境ごとに生成する - sambaiz-net
$ mkdir &amp;lt;cluster_name&amp;gt; &amp;amp;&amp;amp; cd &amp;lt;cluster_name&amp;gt; $ curl -L https://raw.githubusercontent.com/kubeflow/manifests/v1.1-branch/kfdef/kfctl_aws_cognito.v1.2.0.yaml &amp;gt; kfctl_aws.yaml $ cat kfctl_aws.yaml apiVersion: kfdef.apps.kubeflow.org/v1 kind: KfDef metadata: namespace: kubeflow spec: applications: - kustomizeConfig: repoRef: name: manifests path: namespaces/base name: namespaces .</description>
    </item>
    
    <item>
      <title>kustomizeでkubernetesのmanifestを環境ごとに生成する</title>
      <link>https://www.sambaiz.net/article/311/</link>
      <pubDate>Sun, 22 Nov 2020 19:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/311/</guid>
      <description>Kustomizeはk8sのmanifestを生成するツールでkubectlに統合されている。Helmやksonnetのようにテンプレートからではなく、kusomization.yaml以外は素のmanifestをソースとするのでツール独自の記法を覚えたり変換したりする必要がない。
KubernetesのパッケージマネージャーHelmを使う - sambaiz-net
ksonnetでkubernetesのmanifestを環境ごとに生成/applyする - sambaiz-net
kustomization.yamlには対象とするresources、共通のnamespace, label, prefixやパッチなどを記述する。
$ tree . |-- deployment.yaml `-- kustomization.yaml $ cat kustomization.yaml resources: - deployment.yaml commonLabels: foo: bar $ cat deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: app: nginx name: nginx-deployment spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - image: nginx:1.14.2 name: nginx ports: - containerPort: 80 kubectl kustomize でkustomize適用後のyamlをstdoutに出力し、kubectl apply -kで直接applyできる。 出力を見ると commonLabels の foo: bar labelが追加されている。</description>
    </item>
    
    <item>
      <title>EKS上のLocustから負荷をかける際のリソースの割り当てやインスタンスタイプの調整</title>
      <link>https://www.sambaiz.net/article/299/</link>
      <pubDate>Sun, 20 Sep 2020 16:39:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/299/</guid>
      <description>EKS上にLocustをインストールしたのだが、ユーザーを増やしてもRPSが大して伸びない。リソースを調整してなるべく効率的に負荷をかけられるようにする。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
前提 実行するシナリオは次のGETリクエストを送るだけのもの。Chartの都合で0.x系のlocustfileになっている。
from locust import HttpLocust, TaskSet, task class MyTaskSet(TaskSet): @task def index(self): self.client.get(&amp;#34;/&amp;#34;) class MyUser(HttpLocust): task_set = MyTaskSet min_wait = 5 max_wait = 15 ちなみに負荷をかける対象はECS+Fargateに立ち上げたAPIサーバーで、こちらが問題にならないよう余裕を持って動かしている。 スケールするからといってAPI Gatewayなどに向けるとリクエスト数による多額の課金が発生し得るので注意だ。
ECSでアプリケーションを動かすBoilerplateを作った - sambaiz-net
なお、ファイルディスクリプタの数は元から十分大きかったため特に変更していない。
ファイルディスクリプタの上限を増やす - sambaiz-net
$ kubectl exec tryeksstackclusterchartlocustchart1abdd876-worker-d5c7b85cbq6sh -- /bin/sh -c &amp;#34;ulimit -n&amp;#34; 1048576 ワーカー数とリソースの割り当て m5.large (2vCPU, メモリ10GiB)の2ノードに5ワーカーを立ち上げ負荷をかけたところ230RPSあたりで頭打ちになってしまった。
Container Insightsのメトリクスを見るとワーカーPodのCPUの使用率が100%に張り付いていることが分かる。
CloudWatch Container InsightsでEKSのメトリクスを取得する - sambaiz-net
ノードのCPUは40%ほどしかリクエストされておらず同量のlimitsがかかっているので、ワーカーのリクエストCPUを増やすか数を増やせば簡単にRPSを増やせそうだ。
まずはリクエストCPUを100mから500mに増やした。2.5倍ではないのは40%の中にはkube-systemやContainer InsightsのDaemonSetが含まれているため。リクエスト率は90%ほどになった。
$ kubectl describe nodes ... Non-terminated Pods: (9 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- amazon-cloudwatch cloudwatch-agent-sfz5l 200m (10%) 200m (10%) 200Mi (6%) 200Mi (6%) 5m49s amazon-cloudwatch fluentd-cloudwatch-5vnhg 100m (5%) 0 (0%) 200Mi (6%) 400Mi (13%) 5m49s default tryeksstackclusterchartlocustchart1abdd876-master-595c44cczwmcm 100m (5%) 100m (5%) 128Mi (4%) 128Mi (4%) 4m54s default tryeksstackclusterchartlocustchart1abdd876-worker-fddd54db4d7xr 500m (25%) 500m (25%) 128Mi (4%) 128Mi (4%) 4m54s default tryeksstackclusterchartlocustchart1abdd876-worker-fddd54dbz2kpj 500m (25%) 500m (25%) 128Mi (4%) 128Mi (4%) 4m54s kube-system aws-node-875rb 10m (0%) 0 (0%) 0 (0%) 0 (0%) 6m39s kube-system coredns-75b44cb5b4-7qpfl 100m (5%) 0 (0%) 70Mi (2%) 170Mi (5%) 4m54s kube-system coredns-75b44cb5b4-gc6mv 100m (5%) 0 (0%) 70Mi (2%) 170Mi (5%) 4m54s kube-system kube-proxy-wj8fk 100m (5%) 0 (0%) 0 (0%) 0 (0%) 6m39s worker: { replicaCount: 5, config: { configmapName: configmap.</description>
    </item>
    
    <item>
      <title>CloudWatch Container InsightsでEKSのメトリクスを取得する</title>
      <link>https://www.sambaiz.net/article/300/</link>
      <pubDate>Fri, 18 Sep 2020 19:58:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/300/</guid>
      <description>CloudWatch Container Insightsは EKS/EC2上で動くK8sクラスタおよびECSのメトリクスを取得する機能。今回はEKSで使う。
CDKでクラスタを作成する場合、ECSではcontainerInsightsをtrueにすることでセットアップされるが、EKSにはまだ存在しないため手動で行う。PRは上がっている。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
まずCloudWatchにログとメトリクスを送れるようにするため、ワーカーノードのIAMロールか、 PodのServiceAccountに関連づけられたIAMロールに CloudWatchAgentServerPolicyをアタッチする。今回はCDKで先にクラスタやロールを作るため前者の方法を取る。
cluster.defaultNodegroup?.role.addManagedPolicy(ManagedPolicy.fromAwsManagedPolicyName(&amp;#39;CloudWatchAgentServerPolicy&amp;#39;)) セットアップは次のコマンドの実行で完了し、amazon-cloudwatchネームスペースにCloudWatchメトリクスを送信するエージェントとFluentdのDaemonSetや、 各リソースを取得するClusterRoleやServiceAccountなどが作成される。cluster-nameとregion-nameの部分は書き換える。
$ curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed &amp;#34;s/{{cluster_name}}/cluster-name/;s/{{region_name}}/cluster-region/&amp;#34; | kubectl apply -f - $ kubectl get daemonset --namespace amazon-cloudwatch NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE cloudwatch-agent 2 2 2 2 2 &amp;lt;none&amp;gt; 60s fluentd-cloudwatch 2 2 2 2 2 &amp;lt;none&amp;gt; 57s メトリクスはCloudWatchのContainerInsightsネームスペースに送られる。 Podに割り当てられたCPUとメモリの使用率を出してみたところ、負荷をかけた際にCPUが100%に張り付いたので正しく送られていそうだ。</description>
    </item>
    
    <item>
      <title>CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う</title>
      <link>https://www.sambaiz.net/article/297/</link>
      <pubDate>Wed, 16 Sep 2020 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/297/</guid>
      <description>以前、Helmでlocustをインストールしたが、今回はEKSにCDKでインストールする。CDKだとクラスタの作成からできるのでcdk deployで一気に環境が整う。
KubernetesにHelmでLocustによる分散負荷試験環境を立てる - sambaiz-net
$ npm run cdk -- --version 1.62.0 (build 8c2d7fc) まずVPCとClusterを作成する。mastersRoleをユーザーもassumeできるようPrincipalにAWSアカウントも入れている。
AWSのAssumeRole - sambaiz-net
その後、実行するタスクを記述したスクリプトlocustfileのConfigMapを作成し、 これをChartのworker.config.configmapNameで参照する。キー名を間違えがち。 ChartのリポジトリはHelm Hubのもの。
 追記 (2020-12-21): 以前はHelm Hubの https://kubernetes-charts.storage.googleapis.com/ を参照していたが、helm/charts リポジトリがdeprecated になり削除されてしまったので、archiveを参照するようにした。
 import * as cdk from &amp;#39;@aws-cdk/core&amp;#39;; import { Cluster, KubernetesVersion, DefaultCapacityType } from &amp;#39;@aws-cdk/aws-eks&amp;#39; import { Vpc, SubnetType, InstanceType } from &amp;#39;@aws-cdk/aws-ec2&amp;#39; import { Role, ManagedPolicy, ServicePrincipal, AccountPrincipal, CompositePrincipal } from &amp;#39;@aws-cdk/aws-iam&amp;#39; import * as fs from &amp;#39;fs&amp;#39;; export class TryEksStack extends cdk.</description>
    </item>
    
    <item>
      <title>K8s上でElastic APMを動かして外部のGo製APIサーバーのリクエストをトレースする</title>
      <link>https://www.sambaiz.net/article/241/</link>
      <pubDate>Tue, 01 Oct 2019 23:25:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/241/</guid>
      <description>Elastic Cloud on Kubernetes (ECK)で Kubernetesクラスタ上にElasticsearch, KibanaとAPM Serverを立ち上げ、外部のGo製APIサーバーのリクエストをトレースする。 クラスタはGKEで作成し、ノードプールはn2-highmem-4(2vCPU, 13GB)の3台にした。
インストール ElasticSearchやKibana, APM ServerのCRDやelastic-operatorなどをインストールする。
KubernetesのCustom Resource Definition(CRD)とCustom Controller - sambaiz-net
$ kubectl apply -f https://download.elastic.co/downloads/eck/0.9.0/all-in-one.yaml customresourcedefinition.apiextensions.k8s.io/apmservers.apm.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/elasticsearches.elasticsearch.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/trustrelationships.elasticsearch.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co created clusterrole.rbac.authorization.k8s.io/elastic-operator created clusterrolebinding.rbac.authorization.k8s.io/elastic-operator created namespace/elastic-system created statefulset.apps/elastic-operator created secret/webhook-server-secret created serviceaccount/elastic-operator created $ kubectl get pod -n elastic-system NAME READY STATUS RESTARTS AGE elastic-operator-0 1/1 Running 1 91s $ kubectl -n elastic-system logs -f statefulset.apps/elastic-operator ... {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1569230702.0881083,&amp;#34;logger&amp;#34;:&amp;#34;kubebuilder.controller&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;Starting workers&amp;#34;,&amp;#34;controller&amp;#34;:&amp;#34;elasticsearch-controller&amp;#34;,&amp;#34;worker count&amp;#34;:1} {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1569230702.</description>
    </item>
    
    <item>
      <title>Kubernetesでliveness/readinessProbeのexec commandが実行される流れ</title>
      <link>https://www.sambaiz.net/article/190/</link>
      <pubDate>Sat, 06 Oct 2018 16:32:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/190/</guid>
      <description>Kubernetesのliveness/readinessProbe はPodが生きているか/リクエストを受けられるかの判定で、 後者はアプリケーションの起動に時間がかかる場合などに使われる。 ヘルスチェックのようなエンドポイントを叩くのはhttpGetでできるが、任意のcommandを実行することもできる。
livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome Manifestに書かれたProbeは、 各ノードで動いているkubeletが Podが追加されたときにworkerを 作って runProbe()で実行させている。
if p.Exec != nil { glog.V(4).Infof(&amp;#34;Exec-Probe Pod: %v, Container: %v, Command: %v&amp;#34;, pod, container, p.Exec.Command) command := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env) return pb.exec.Probe(pb.newExecInContainer(container, containerID, command, timeout)) } まずcommandに含まれる$( )で囲まれた文字列を Expand() で存在すればcontainerのenvの値に置き換える。
その後、 RunInContainer()で、 コンテナランタイムがK8s標準のCRI(Container Runtime Interface)に対応している場合はそのAPIの、 対応していない場合は~shimパッケージのExecSync()が呼ばれ、コンテナ内でcommandを実行させて結果を受け取り、終了コードが0でなければエラーとする。
$( )でenvの値が使えることを確認する。
$ kubectl config use-context docker-for-desktop $ cat test.yaml --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: &amp;#34;test&amp;#34; spec: replicas: 1 template: metadata: labels: app: &amp;#34;test&amp;#34; spec: containers: - name: &amp;#34;test&amp;#34; image: &amp;#34;alpine&amp;#34; command: [&amp;#34;top&amp;#34;] env: - name: TEST value: &amp;#34;foobar&amp;#34; ports: - containerPort: 5000 name: grpc readinessProbe: exec: command: - test - $(TEST) - = - foobar initialDelaySeconds: 0 timeoutSeconds: 1 $ kubectl apply test.</description>
    </item>
    
    <item>
      <title>cert-managerで生成した証明書をIstioのGatewayに設定してHTTPS対応する</title>
      <link>https://www.sambaiz.net/article/186/</link>
      <pubDate>Thu, 13 Sep 2018 21:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/186/</guid>
      <description>cert-managerはTLSの証明書を自動で生成し管理するK8sのアドオン。 Istioにも含まれていて、これを使ってLet&amp;rsquo;s Encryptで証明書を生成しGatewayに設定することでHTTPS対応することができる。
デフォルトではcert-managerは入らないのでenabled=trueにしてインストールする。 最初に入るLet&amp;rsquo;s EncryptのClusterIssuerはエラーになったので消す。
IstioをHelmでインストールしてRoutingとTelemetryを行いJaeger/Kialiで確認する - sambaiz-net
$ helm install install/kubernetes/helm/istio --name istio --namespace istio-system --set certmanager.enabled=true $ kubectl delete ClusterIssuer --all 確認用にBookInfoを動かす。
$ kubectl label namespace default istio-injection=enabled $ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml $ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml Let&amp;rsquo;s Encryptで使われているACMEプロトコルのドメイン認証(Challenge)には /.well-known/acme-challenge/{token}でHTTPレスポンスを返すHTTP Challenge(http-01)と DNSのTXTレコードに書き込むDNS Challenge(dns-01)がある。 HTTP Challengeは手軽に達成できる一方、CAからアクセスできるようにする必要がある。今回はDNS Challengeでやる。 cert-managerはCloud DNSやRoute53などに対応していて、今回はCloudflareを使う。
DNSに書き込めるようにするためCloudflareのMy ProfileからGlobal API Keyを持ってきてBase64デコードしSecretに入れる。 改行コードが含まれないように-nを付ける。
$ echo -n ***** | base64 apiVersion: v1 kind: Secret metadata: name: cloudflare-api-key namespace: istio-system type: Opaque data: api-key: ***** Let&amp;rsquo;s EncryptのClusterIssuerと証明書を生成するドメインのCertificateを作成する。 serverのURLはStatusのページから確認できる。 本番のURLはレート制限があるので、まずはFakeの証明書が生成されるstgで試すとよい。</description>
    </item>
    
    <item>
      <title>IstioをHelmでインストールしてRoutingとTelemetryを行いJaeger/Kialiで確認する</title>
      <link>https://www.sambaiz.net/article/185/</link>
      <pubDate>Sun, 02 Sep 2018 23:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/185/</guid>
      <description>IstioはEnvoyというProxyをSidecarとしてPodに入れてトラフィックを通すことでマイクロサービスのRoutingやTelemetryをサービスのコードに手を入れずに行うことができるサービスメッシュ。 もともとEnvoy自体は単体で、コネクションを張りっぱなしのgRPC(HTTP/2)をK8sのServiceのL4ロードバランサーでは分散できない問題の解決方法の一つとして 各PodのIPの一覧を返すHeadless Serviceと使われていたが、各Manifestに入れたりConfigMapを編集したりする必要があり少し面倒だった。 Istioにするとそれらが省けて、さらに賢いRoutingやモニタリングの仕組みまで付いてくるのでとても便利だ。
インストール IstioをダウンロードしてきてHelmでインストールする。Istioには様々なコンポーネントが含まれているが、パラメータでインストールするものを選択することができる。
KubernetesのパッケージマネージャーHelmを使う - sambaiz-net
今回はデフォルトではfalseになっているGrafana/Jaeger/Kialiをtrueにしてほぼ全て入るようにしている。
RBACが有効な場合はServiceAccountを作ってcluster-adminあるいは必要なRoleをBindしておく。
RBACが有効なGKEでHelmを使う - sambaiz-net
$ curl -L https://git.io/getLatestIstio | sh - $ cd istio-1.0.1/ # helm init --service-account tiller $ helm install install/kubernetes/helm/istio --name istio --namespace istio-system --set grafana.enabled=true --set grafana.persist=true --set grafana.storageClassName=standard --set tracing.enabled=true --set kiali.enabled=true $ kubectl get pod -n istio-system NAME READY STATUS RESTARTS AGE grafana-598678cbb-bglbq 1/1 Running 0 3m istio-citadel-6f9887d776-tvdg8 1/1 Running 0 3m istio-egressgateway-84d78d84bf-zpxrq 1/1 Running 0 3m istio-galley-556f5558f5-hk2r8 1/1 Running 0 3m istio-ingressgateway-78cccbddbb-gh2xl 1/1 Running 0 3m istio-pilot-799845f56d-l777d 2/2 Running 0 3m istio-policy-7666fcd574-nbx8s 2/2 Running 0 3m istio-sidecar-injector-7b6589c9-m7x77 1/1 Running 0 3m istio-statsd-prom-bridge-55965ff9c8-s6dmj 1/1 Running 0 3m istio-telemetry-844c8d6bff-9trcf 2/2 Running 0 3m istio-tracing-77f9f94b98-g7v6f 1/1 Running 0 3m kiali-bdf7fdc78-9lpd4 1/1 Running 0 3m prometheus-7456f56c96-drhlq 1/1 Running 0 3m default namespaceにラベルを貼って自動でEnvoyが各PodにInjectionされるようにする。</description>
    </item>
    
    <item>
      <title>CircleCI 2.0でDocker imageをbuildしてタグを付けてContainer Registryに上げる</title>
      <link>https://www.sambaiz.net/article/183/</link>
      <pubDate>Wed, 22 Aug 2018 23:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/183/</guid>
      <description>(追記: 2019-04-13) 2.1からのOrbを使うと自分でjobを書かなくてもよくなる CircleCI 2.1からのOrbでdocker buildしてECRにpushし、Slackに通知させる - sambaiz-net
 masterにpushしたときと、リリースタグを切ったときにビルドされるようにする。
version: 2 jobs: build: docker: - image: google/cloud-sdk environment: GCP_PROJECT: &amp;lt;project_name&amp;gt; IMAGE_NAME: &amp;lt;image_name&amp;gt; steps: - checkout - setup_remote_docker: version: 18.05.0-ce - run: name: gcloud auth command: | echo $GCLOUD_SERVICE_KEY | base64 --decode &amp;gt; ${HOME}/gcloud-service-key.json gcloud auth activate-service-account --key-file ${HOME}/gcloud-service-key.json gcloud --quiet auth configure-docker - run: name: docker build &amp;amp; push command: | docker build -t asia.gcr.io/${GCP_PROJECT}/${IMAGE_NAME}:${CIRCLE_BUILD_NUM} . docker tag asia.</description>
    </item>
    
    <item>
      <title>KubernetesのCustom Resource Definition(CRD)とCustom Controller</title>
      <link>https://www.sambaiz.net/article/182/</link>
      <pubDate>Thu, 09 Aug 2018 23:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/182/</guid>
      <description>K8sではDeploymentを作成したときにReplicaSetも作成されるようにしたり、 Load Balancer Serviceを作成したときにGCPなどその環境に応じたLoad Balancerも作成されるようにしたりするため、Controllerがそれらを監視してAPIを呼んでいる。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
Controllerは単なるAPIを呼ぶアプリケーションなので自分でCustom Controllerを作成してDeploymentとしてデプロイすることもできる。 また、監視する対象もpodsやdeploymentsといった標準のAPIだけではなく、 Custom Resource で拡張したものを使うことができる。
特定のアプリケーションのためのControllerはOperatorとも呼ばれる。
CustomResourceDefinition(CRD) Custom Resourceを定義する。
apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: # REST APIで使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt; group: stable.example.com version: v1 # Namespaced か Cluster scope: Namespaced names: # 複数形 URLに使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;/&amp;lt;plural&amp;gt; plural: crontabs # 単数形 CLIなどで使われる singular: crontab # manifestで使う kind: CronTab shortNames: - ct $ kubectl create -f crd.yaml $ kubectl get crd NAME AGE crontabs.</description>
    </item>
    
    <item>
      <title>KubernetesのNetworkPolicy Resource</title>
      <link>https://www.sambaiz.net/article/181/</link>
      <pubDate>Mon, 30 Jul 2018 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/181/</guid>
      <description>Network Policies - Kubernetes
PodのトラフィックをラベルやIPアドレスで許可するためのResource。AWSのセキュリティグループやGCPのファイアウォールルールのようなもの。 GKEでは今のところデフォルトでオフになっているので--enable-network-policyを付けてクラスタを作成する必要がある。
以前作成したmulti podのアプリケーションで挙動を確認する。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
$ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE clusterip-app ClusterIP 10.23.247.54 &amp;lt;none&amp;gt; 80/TCP 48m loadbalancer-app LoadBalancer 10.23.244.137 35.224.130.196 80:31508/TCP 48m nodeport-app NodePort 10.23.246.215 &amp;lt;none&amp;gt; 80:32181/TCP 48m ... $ curl -d &amp;#39;{&amp;#34;url&amp;#34;: &amp;#34;http://nodeport-app&amp;#34;}&amp;#39; http://35.224.130.196/ 200 作成するNetworkPolicyは以下の二つで、いずれも対象はapp: nodeport-appのラベルが付いたPod。 一つ目は対象Podへのリクエストを一旦全て拒否する。 二つ目はnodeport-access: &amp;quot;true&amp;quot;のラベルが付いたPodから対象Podへの8080ポートのリクエストを許可するもの。 今回は設定しないがegressも設定できる。
$ cat networkPolicies.yaml --- # Default deny all ingress traffic apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: matchLabels: app: nodeport-app --- apiVersion: networking.</description>
    </item>
    
    <item>
      <title>GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress</title>
      <link>https://www.sambaiz.net/article/173/</link>
      <pubDate>Sat, 23 Jun 2018 15:02:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/173/</guid>
      <description>疎通確認用アプリケーション GETでは200を返し、POSTではURLにGETリクエストを送ってステータスコードを返す。
package main import ( &amp;#34;encoding/json&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;io/ioutil&amp;#34; &amp;#34;net/http&amp;#34; ) type PostBody struct { URL string `json:&amp;#34;url&amp;#34;` } func handler(w http.ResponseWriter, r *http.Request) { if r.Method == http.MethodGet { fmt.Fprintln(w, &amp;#34;ok&amp;#34;) } else if r.Method == http.MethodPost { data, err := ioutil.ReadAll(r.Body) if err != nil { w.WriteHeader(http.StatusInternalServerError) fmt.Fprintln(w, err.Error()) return } p := PostBody{} if err := json.Unmarshal(data, &amp;amp;p); err != nil { w.WriteHeader(http.StatusBadRequest) fmt.Fprintln(w, err.Error()) return } resp, err := http.</description>
    </item>
    
    <item>
      <title>ksonnetでkubernetesのmanifestを環境ごとに生成/applyする</title>
      <link>https://www.sambaiz.net/article/171/</link>
      <pubDate>Wed, 20 Jun 2018 01:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/171/</guid>
      <description>ksonnetはJSONのテンプレートエンジンjsonnetからk8sのmanifestを環境ごとに生成してapplyするツール。kubeflowでも使われている。
 追記(2020-11-22): 既に開発が終了しリポジトリもアーカイブされている。 kubeflowはkustomizeに移行した。
kustomizeでkubernetesのmanifestを環境ごとに生成する - sambaiz-net
 $ brew install ksonnet/tap/ks $ ks version ksonnet version: 0.11.0 jsonnet version: v0.10.0 client-go version: init まずks initしてディレクトリを作成する。
$ kubectl config current-context minikube $ ks init kstest $ cd kstest $ ls app.yaml	components	environments	lib	vendor $ cat app.yaml apiVersion: 0.1.0 environments: default: destination: namespace: default server: https://192.168.99.100:8443 k8sVersion: v1.10.0 path: default kind: ksonnet.io/app name: kstest registries: incubator: gitVersion: commitSha: 40285d8a14f1ac5787e405e1023cf0c07f6aa28c refSpec: master protocol: github uri: github.</description>
    </item>
    
    <item>
      <title>Istio v0.7でEnvoy Proxyを付けるまで</title>
      <link>https://www.sambaiz.net/article/167/</link>
      <pubDate>Tue, 29 May 2018 22:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/167/</guid>
      <description>追記(2018-09-01) v1.0となりHelmでのインストールも問題なくできるようになった。Istio-AuthがCitadelという名前になっていたりDeprecatedになってるAPIもある
IstioをHelmでインストールしてRoutingとTelemetryを行いJaeger/Kialiで確認する - sambaiz-net
 Istioとは マイクロサービス間のネットワークの、ロードバランシングや認証、モニタリングなどを担うサービスメッシュのOSS。 概念は抽象化されていて、Kubernetes以外でもサポートされている。 通信をコントロールするdata-planeのEnvoyと、Envoyを管理するcontrol-planeのPilot, Mixer, Istio-Authからなる。
Envoy Sidecarとしてデプロイされる、サービスメッシュでの全ての通信を通すプロキシ。 アプリケーションのコードに手を入れる必要がないので言語に縛られない。 CNCFのプロジェクトの一つで、 Istio用にオリジナルから拡張されている。 ロードバランシングやヘルスチェックなどを行い、メトリクスを取る。
Mixer サービスメッシュ全体のアクセスコントロールや、Envoyからデータを集めてログに出したりモニタリングしたりする。 プラグインよってAWSやGCPといったインフラバックエンドの差異が吸収される。
Pilot サービスディスカバリしてEnvoyのトラフィックを制御する。A/Bテストやカナリアリリースをする場合や、障害に対応して適切にルーティングを行うことができる。
Istio-Auth サービスやエンドユーザーの認証を行い、ポリシーに従ってアクセス制御する。
Istioのインストール ローカルのminikubeに環境を作る。 apiserver.Admission.PluginNamesでは立ち上がらなかったので代わりに apiserver.admission-controlを指定している。
$ minikube version minikube version: v0.27.0 $ minikube start \ --extra-config=apiserver.admission-control=&amp;#34;NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota&amp;#34; \ --kubernetes-version=v1.9.0 $ kubectl config current-context minikube istioを持ってきてapplyする。 Helmも用意されていて将来的にそっちで入れるのが推奨になりそうだ。
$ curl -L https://git.io/getLatestIstio | sh - $ cd istio-0.7.1/ $ kubectl apply -f install/kubernetes/istio-auth.yaml 作成されたserviceとpodはこんな感じ。
$ kubectl get svc -o name -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingress LoadBalancer 10.</description>
    </item>
    
    <item>
      <title>TerraformでGKEクラスタとBigQueryを立てる</title>
      <link>https://www.sambaiz.net/article/165/</link>
      <pubDate>Tue, 29 May 2018 02:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/165/</guid>
      <description>GKEクラスタからBigQueryを読み書きすることを想定している。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る - sambaiz-net
GKE  google_container_cluster  oauth_scopeにbigqueryを付けている。
resource &amp;#34;google_container_cluster&amp;#34; &amp;#34;sample&amp;#34; { name = &amp;#34;${var.cluster_name}&amp;#34; description = &amp;#34;sample k8s cluster&amp;#34; zone = &amp;#34;${var.gcp_zone}&amp;#34; initial_node_count = &amp;#34;${var.initial_node_count}&amp;#34; master_auth { username = &amp;#34;${var.master_username}&amp;#34; password = &amp;#34;${var.master_password}&amp;#34; } node_config { machine_type = &amp;#34;${var.node_machine_type}&amp;#34; disk_size_gb = &amp;#34;${var.node_disk_size}&amp;#34; oauth_scopes = [ &amp;#34;https://www.googleapis.com/auth/compute&amp;#34;, &amp;#34;https://www.googleapis.com/auth/devstorage.read_only&amp;#34;, &amp;#34;https://www.googleapis.com/auth/logging.write&amp;#34;, &amp;#34;https://www.googleapis.com/auth/monitoring&amp;#34;, &amp;#34;https://www.googleapis.com/auth/bigquery&amp;#34;, ] } } variable &amp;#34;env&amp;#34; { description = &amp;#34;system env&amp;#34; } variable &amp;#34;gcp_zone&amp;#34; { description = &amp;#34;GCP zone, e.</description>
    </item>
    
    <item>
      <title>KubernetesにHelmでLocustによる分散負荷試験環境を立てる</title>
      <link>https://www.sambaiz.net/article/161/</link>
      <pubDate>Sun, 18 Mar 2018 22:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/161/</guid>
      <description>OSSの負荷試験ツールLocustをK8sクラスタに立てる。 K8sならworkerの増減も簡単だし、HelmのChartもあるので立てるのも楽。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
LocustはPython製で、以下のようなコードで処理を書くことができる。
@task(10)のように括弧の中に数字を書いて実行される割合を偏らせることもできる。 異なるTaskSetに対応するユーザーを複数作ることもできて、こちらもweightで重みを付けられる。 ユーザー数はあとでWeb上から入力する。
$ mkdir tasks $ cat tasks/tasks.py from locust import HttpLocust, TaskSet, task class ElbTasks(TaskSet): @task def task1(self): with self.client.get(&amp;#34;/&amp;#34;, catch_response=True) as response: if response.content != &amp;#34;Success&amp;#34;: response.failure(&amp;#34;Got wrong response&amp;#34;) class ElbWarmer(HttpLocust): task_set = ElbTasks min_wait = 1000 max_wait = 3000 stableにChartはあるが、今のところtasksの中を書き換えなくてはいけないようなので、forkしてきてtasksの中を書き換え、package して、helm repo index でこれを参照するindex.yamlを生成した。
 追記(2020-03-11): 今はConfigmapを自分で作成し --set worker.config.configmapName=*** することでforkしなくてもよくなった kubectl create configmap locust-worker-configs --from-file tasks/tasks.py
 $ helm package .</description>
    </item>
    
    <item>
      <title>RBACが有効なGKEでHelmを使う</title>
      <link>https://www.sambaiz.net/article/160/</link>
      <pubDate>Sun, 18 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/160/</guid>
      <description>k8sのパッケージマネージャーHelmを使う - sambaiz-net
$ helm version Client: &amp;amp;version.Version{SemVer:&amp;#34;v2.8.2&amp;#34;, GitCommit:&amp;#34;a80231648a1473929271764b920a8e346f6de844&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} Server: &amp;amp;version.Version{SemVer:&amp;#34;v2.8.2&amp;#34;, GitCommit:&amp;#34;a80231648a1473929271764b920a8e346f6de844&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} GKEでhelm initしてhelm installしたところ以下のエラーが返ってきた。
Error: release my-locust failed: namespaces &amp;#34;default&amp;#34; is forbidden: User &amp;#34;system:serviceaccount:kube-system:default&amp;#34; cannot get namespaces in the namespace &amp;#34;default&amp;#34;: Unknown user &amp;#34;system:serviceaccount:kube-system:default&amp;#34; GKEではデフォルトでK8sのRBAC(Role-Based Access Control)が有効になっているため、Tillerインスタンスに権限を与える必要がある。
ということでTiller用にnamespaceを切って、その中では好きにできるRoleと、Tillerが使うServiceAccountを作成し、RoleBindingで紐づける。
kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-manager namespace: tiller-world rules: - apiGroups: [&amp;#34;&amp;#34;, &amp;#34;extensions&amp;#34;, &amp;#34;apps&amp;#34;] resources: [&amp;#34;*&amp;#34;] verbs: [&amp;#34;*&amp;#34;] --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: tiller-world --- kind: RoleBinding apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る</title>
      <link>https://www.sambaiz.net/article/159/</link>
      <pubDate>Tue, 13 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/159/</guid>
      <description>Logging AgentをNodeレベルのDaemonSetとして動かすのではなく、Podの中にSidecar Containerとして動かす。その分リソースは食うけど、独立した設定で動かせる。
アプリケーション https://github.com/sambaiz/go-logging-sample
Goで定期的にログを出すサンプルコードを書いたのでこれを使う。 viperで設定を持ち、 zapでログを出力する。 あとSIGINTを拾ってSync()してGraceful Shutdownするようにしている。
Golangの高速なロガーzapとlumberjackでログを出力してrotateさせる - sambaiz-net
multistage-buildして、GKEで動かすのでContainer Registryに上げる。
$ docker build -t go-logging-sample . $ docker tag go-logging-sample gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample:v1 $ gcloud docker -- push gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample Fluentdの設定 fluent-plugin-bigqueryプラグインを使う。
projectとdataset、パーティションの日付分割テーブルに入れる場合は、auto_create_tableできないのでtableも作成しておく。
fluentdの設定はConfigMapで持つ。
apiVersion: v1 kind: ConfigMap metadata: name: fluentd-config data: fluent.conf: | &amp;lt;source&amp;gt; @type tail format json path /var/log/app.log pos_file /var/log/app.log.pos tag bigquery &amp;lt;/source&amp;gt; &amp;lt;match bigquery&amp;gt; @type bigquery method load &amp;lt;buffer time&amp;gt; @type file path /var/log/bigquery.*.buffer timekey 1d flush_at_shutdown true &amp;lt;/buffer&amp;gt; auth_method	compute_engine project &amp;lt;project-name&amp;gt; dataset &amp;lt;dataset-name&amp;gt; table &amp;lt;table-name&amp;gt;$%Y%m%d fetch_schema true ignore_unknown_values true	&amp;lt;/match&amp;gt; プラグイン入りのfluentdイメージもビルドして上げる。</description>
    </item>
    
    <item>
      <title>ローカルでビルドしたimageをminikubeで使う</title>
      <link>https://www.sambaiz.net/article/151/</link>
      <pubDate>Thu, 01 Feb 2018 22:49:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/151/</guid>
      <description>$ minikube version minikube version: v0.25.0 $ kubectl version Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;9&amp;#34;, GitVersion:&amp;#34;v1.9.2&amp;#34;, GitCommit:&amp;#34;5fa2db2bd46ac79e5e00a4e6ed24191080aa463b&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2018-01-18T21:11:08Z&amp;#34;, GoVersion:&amp;#34;go1.9.2&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;darwin/amd64&amp;#34;} $ kubectl config current-context minikube $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100 dockerコマンドがminikube VM内で動いているdocker daemonを参照するようにする。
$ minikube docker-env export DOCKER_TLS_VERIFY=&amp;#34;1&amp;#34; export DOCKER_HOST=&amp;#34;tcp://192.168.99.100:2376&amp;#34; export DOCKER_CERT_PATH=&amp;#34;/Users/sambaiz/.minikube/certs&amp;#34; $ eval $(minikube docker-env) $ docker info --format &amp;#39;{{json .Name}}&amp;#39; &amp;#34;minikube&amp;#34; ビルドするDockerfile。nginxが立ち上がるだけ。
FROMnginx 何もタグを付けない(:latest)とcreate時にDockerレジストリからpullしにいって失敗してしまうため、タグ付きでビルドする。
$ docker build -t my/myapp:1.</description>
    </item>
    
    <item>
      <title>KubernetesのパッケージマネージャーHelmを使う</title>
      <link>https://www.sambaiz.net/article/122/</link>
      <pubDate>Wed, 26 Jul 2017 01:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/122/</guid>
      <description>Kubernatesが操舵手なのに対して、Helmは舵。 パッケージはChart(海図)と呼ばれている。
ChartにはデフォルトでGoのtemplateで書かれたManifestが含まれ、values.yamlの値を-f values.yamlや--set key=valueフラグで上書きして適用しインストールすることができる。
Helmコマンドをインストールする。 今回はminikubeに入れるので立ち上げる。
$ brew install kubernetes-helm $ helm version Client: &amp;amp;version.Version{SemVer:&amp;#34;v2.5.0&amp;#34;, GitCommit:&amp;#34;012cb0ac1a1b2f888144ef5a67b8dab6c2d45be6&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} # brew cask install virtualbox minikube $ minikube version minikube version: v0.20.0 $ minikube start Kubectl is now configured to use the cluster. $ kubectl version Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;7&amp;#34;, GitVersion:&amp;#34;v1.7.2&amp;#34;, GitCommit:&amp;#34;922a86cfcd65915a9b2f69f3f193b8907d741d9c&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;, BuildDate:&amp;#34;2017-07-21T19:06:19Z&amp;#34;, GoVersion:&amp;#34;go1.8.3&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;darwin/amd64&amp;#34;} Server Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;6&amp;#34;, GitVersion:&amp;#34;v1.6.4&amp;#34;, GitCommit:&amp;#34;d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae&amp;#34;, GitTreeState:&amp;#34;dirty&amp;#34;, BuildDate:&amp;#34;2017-06-22T04:31:09Z&amp;#34;, GoVersion:&amp;#34;go1.7.5&amp;#34;, Compiler:&amp;#34;gc&amp;#34;, Platform:&amp;#34;linux/amd64&amp;#34;} $ kubectl config current-context minikube まずk8sクラスタ上にHelmの管理サーバーTillerをインストールする必要がある。</description>
    </item>
    
    <item>
      <title>gcloudのアカウント切り替えとkubectlのcontext変更</title>
      <link>https://www.sambaiz.net/article/28/</link>
      <pubDate>Tue, 25 Oct 2016 20:29:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/28/</guid>
      <description>いつも迷うのでまとめた。
gcloudのアカウント一覧と切り替え $ gcloud auth list $ gcloud config set account `ACCOUNT` configにprojectなども設定している場合はconfig自体を作成して切り替えた方が楽。
$ gcloud config configurations create &amp;lt;name&amp;gt; $ gcloud config configurations activate &amp;lt;name&amp;gt; $ gcloud config list ... Your active configuration is: [&amp;lt;name&amp;gt;] $ gcloud config set account &amp;lt;accout&amp;gt; $ gcloud config set project &amp;lt;project&amp;gt; kubectlのcontext変更 $ kubectl config current-context $ kubectl config view # contexts $ kubectl config use-context minikube </description>
    </item>
    
    <item>
      <title>GKEで複数コンテナのアプリケーションを動かす</title>
      <link>https://www.sambaiz.net/article/18/</link>
      <pubDate>Fri, 26 Aug 2016 21:57:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/18/</guid>
      <description>前回は単一コンテナのアプリケーションを動かしたが、今回はコンテナ間でやり取りが発生するものを動かす。 流れとしては、クライアントからのリクエストをGATEWAYサーバーで受け取り、SERVICEサーバーにリクエストし、その結果を返すまで。
プログラムは以下の通り、環境変数TYPEの値によって挙動を変えていて、同じイメージを使い回す。コードはここ。
var http = require(&amp;#39;http&amp;#39;); var handleRequest = function(request, response) { if(process.env.TYPE == &amp;#34;GATEWAY&amp;#34;){ console.log(&amp;#39;Passed.&amp;#39;); var options = { host: &amp;#39;service&amp;#39;, port: 8080, method: &amp;#39;GET&amp;#39; }; var req = http.request(options, function(res) { data = &amp;#34;&amp;#34; res.on(&amp;#39;data&amp;#39;, function (chunk) { data+=chunk; }); res.on(&amp;#39;end&amp;#39;, () =&amp;gt; { response.writeHead(200); response.end(data); }); }); req.on(&amp;#39;error&amp;#39;, function(e) { response.writeHead(500) response.end(e.message); }); req.end(); }else{ console.log(&amp;#39;Received.&amp;#39;); response.writeHead(200); response.end(&amp;#39;ok&amp;#39;); } }; var www = http.createServer(handleRequest); www.listen(8080); これをContainer RegistryにPushする。</description>
    </item>
    
    <item>
      <title>Google Container Engine(GKE)で単一コンテナのアプリケーションを動かす</title>
      <link>https://www.sambaiz.net/article/17/</link>
      <pubDate>Sun, 21 Aug 2016 23:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/17/</guid>
      <description>Kubernetes - Hello World Walkthrough
CloudSDKとkubectlのインストール Cloud SDKをインストールしてgloudコマンドを使えるようにする。
$ gcloud --version Google Cloud SDK 122.0.0 $ gcloud components install kubectl Google Container RegistryにPush $ export PROJECT_ID=&amp;#34;******&amp;#34; $ docker build -t gcr.io/$PROJECT_ID/test:v1 . $ gcloud docker push gcr.io/$PROJECT_ID/test:v1 プロジェクトの課金を有効にしていないとこんなエラーメッセージが出る。
denied: Unable to create the repository, please check that you have access to do so. Clusterの作成 $ gcloud config set core/project $PROJECT_ID $ gcloud config set compute/zone asia-east1-b $ gcloud container clusters create test-cluster $ gcloud config set container/cluster test-cluster Container Engine APIが有効になっていない場合はこうなる。 一度コンソールからContainer Engineを選ぶと、サービスの準備が始まって有効になる。</description>
    </item>
    
    <item>
      <title>Kubernetesのチュートリアルをたどる</title>
      <link>https://www.sambaiz.net/article/9/</link>
      <pubDate>Mon, 18 Jul 2016 22:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/9/</guid>
      <description>Kubernetesとは Kubernetes(発音はkoo-ber-nay&#39;-tace。 ギリシャ語で操舵手。)はGoogleによって開発が始められた、アプリケーションコンテナにおける自動デプロイ、スケーリング、操作を 自動化するOSS。K8sと略される。
Minikube K8sをローカルで試すために、MinikubeというVMの中で単一ノードのK8sクラスターを動かすツールを入れる。
v0.6.0
curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.6.0/minikube-darwin-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo mv minikube /usr/local/bin/ $ minikube start Starting local Kubernetes cluster... ... $ kubectl version Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;2&amp;#34;, GitVersion:&amp;#34;v1.2.4&amp;#34;, GitCommit:&amp;#34;3eed1e3be6848b877ff80a93da3785d9034d0a4f&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} Server Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;3&amp;#34;, GitVersion:&amp;#34;v1.3.0&amp;#34;, GitCommit:&amp;#34;283137936a498aed572ee22af6774b6fb6e9fd94&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} Pods K8sではコンテナのグループをpodと呼ぶ。pod中のコンテナは共にデプロイされ、起動し、停止する。 また、グループとして複製される。
Podの定義は次のようにyamlで書かれる。
apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 Podの定義に望ましい状態を記述すると、Kubernatesはそれを見て現在の状態が一致しているかどうか確認する。 例えば、Podが作られたときに、コンテナがその中で動いている状態が望ましい状態だとすると、 コンテナが動かなくなったときに、Kubernatesは新しいものを再作成することで望ましい状態にする。</description>
    </item>
    
  </channel>
</rss>
