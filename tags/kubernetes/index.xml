<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Sat, 06 Oct 2018 16:32:00 +0900</lastBuildDate>
    
	<atom:link href="https://www.sambaiz.net/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Kubernetesでliveness/readinessProbeのexec commandが実行される流れ</title>
      <link>https://www.sambaiz.net/article/190/</link>
      <pubDate>Sat, 06 Oct 2018 16:32:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/190/</guid>
      <description>Kubernetesのliveness/readinessProbe はPodが生きているか/リクエストを受けられるかの判定で、 後者はアプリケーションの起動に時間がかかる場合などに使われる。 ヘルスチェックのようなエンドポイントを叩くのはhttpGetでできるが、任意のcommandを実行することもできる。
livenessProbe: httpGet: path: /healthz port: 8080 httpHeaders: - name: X-Custom-Header value: Awesome  Manifestに書かれたProbeは、 各ノードで動いているkubeletが Podが追加されたときにworkerを 作って runProbe()で実行させている。
if p.Exec != nil { glog.V(4).Infof(&amp;quot;Exec-Probe Pod: %v, Container: %v, Command: %v&amp;quot;, pod, container, p.Exec.Command) command := kubecontainer.ExpandContainerCommandOnlyStatic(p.Exec.Command, container.Env) return pb.exec.Probe(pb.newExecInContainer(container, containerID, command, timeout)) }  まずcommandに含まれる$( )で囲まれた文字列を Expand() で存在すればcontainerのenvの値に置き換える。
その後、 RunInContainer()で、 コンテナランタイムがK8s標準のCRI(Container Runtime Interface)に対応している場合はそのAPIの、 対応していない場合は~shimパッケージのExecSync()が呼ばれ、コンテナ内でcommandを実行させて結果を受け取り、終了コードが0でなければエラーとする。
$( )でenvの値が使えることを確認する。
$ kubectl config use-context docker-for-desktop $ cat test.yaml --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: &amp;quot;test&amp;quot; spec: replicas: 1 template: metadata: labels: app: &amp;quot;test&amp;quot; spec: containers: - name: &amp;quot;test&amp;quot; image: &amp;quot;alpine&amp;quot; command: [&amp;quot;top&amp;quot;] env: - name: TEST value: &amp;quot;foobar&amp;quot; ports: - containerPort: 5000 name: grpc readinessProbe: exec: command: - test - $(TEST) - = - foobar initialDelaySeconds: 0 timeoutSeconds: 1 $ kubectl apply test.</description>
    </item>
    
    <item>
      <title>cert-managerで生成した証明書をIstioのGatewayに設定してHTTPS対応する</title>
      <link>https://www.sambaiz.net/article/186/</link>
      <pubDate>Thu, 13 Sep 2018 21:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/186/</guid>
      <description>cert-managerはTLSの証明書を自動で生成し管理するK8sのアドオン。 Istioにも含まれていて、これを使ってLet&amp;rsquo;s Encryptで証明書を生成しGatewayに設定することでHTTPS対応することができる。
デフォルトではcert-managerは入らないのでenabled=trueにしてインストールする。 最初に入るLet&amp;rsquo;s EncryptのClusterIssuerはエラーになったので消す。
IstioをHelmでインストールしてRoutingとTelemetryを行いJaeger/Kialiで確認する - sambaiz-net
$ helm install install/kubernetes/helm/istio --name istio --namespace istio-system --set certmanager.enabled=true $ kubectl delete ClusterIssuer --all  確認用にBookInfoを動かす。
$ kubectl label namespace default istio-injection=enabled $ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml $ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml  Let&amp;rsquo;s Encryptで使われているACMEプロトコルのドメイン認証(Challenge)には /.well-known/acme-challenge/{token}でHTTPレスポンスを返すHTTP Challenge(http-01)と DNSのTXTレコードに書き込むDNS Challenge(dns-01)がある。 HTTP Challengeは手軽に達成できる一方、CAからアクセスできるようにする必要がある。今回はDNS Challengeでやる。 cert-managerはCloud DNSやRoute53などに対応していて、今回はCloudflareを使う。
DNSに書き込めるようにするためCloudflareのMy ProfileからGlobal API Keyを持ってきてBase64デコードしSecretに入れる。 改行コードが含まれないように-nを付ける。
$ echo -n ***** | base64  apiVersion: v1 kind: Secret metadata: name: cloudflare-api-key namespace: istio-system type: Opaque data: api-key: *****  Let&amp;rsquo;s EncryptのClusterIssuerと証明書を生成するドメインのCertificateを作成する。 serverのURLはStatusのページから確認できる。 本番のURLはレート制限があるので、まずはFakeの証明書が生成されるstgで試すとよい。</description>
    </item>
    
    <item>
      <title>IstioをHelmでインストールしてRoutingとTelemetryを行いJaeger/Kialiで確認する</title>
      <link>https://www.sambaiz.net/article/185/</link>
      <pubDate>Sun, 02 Sep 2018 23:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/185/</guid>
      <description>IstioはEnvoyというProxyをSidecarとしてPodに入れてトラフィックを通すことでマイクロサービスのRoutingやTelemetryをサービスのコードに手を入れずに行うことができるサービスメッシュ。 もともとEnvoy自体は単体で、コネクションを張りっぱなしのgRPC(HTTP/2)をK8sのServiceのL4ロードバランサーでは分散できない問題の解決方法の一つとして 各PodのIPの一覧を返すHeadless Serviceと使われていたが、各Manifestに入れたりConfigMapを編集したりする必要があり少し面倒だった。 Istioにするとそれらが省けて、さらに賢いRoutingやモニタリングの仕組みまで付いてくるのでとても便利だ。
インストール IstioをダウンロードしてきてHelmでインストールする。Istioには様々なコンポーネントが含まれているが、パラメータでインストールするものを選択することができる。
KubernetesのパッケージマネージャーHelmを使う - sambaiz-net
今回はデフォルトではfalseになっているGrafana/Jaeger/Kialiをtrueにしてほぼ全て入るようにしている。
RBACが有効な場合はServiceAccountを作ってcluster-adminあるいは必要なRoleをBindしておく。
RBACが有効なGKEでHelmを使う - sambaiz-net
$ curl -L https://git.io/getLatestIstio | sh - $ cd istio-1.0.1/ # helm init --service-account tiller $ helm install install/kubernetes/helm/istio --name istio --namespace istio-system --set grafana.enabled=true --set grafana.persist=true --set grafana.storageClassName=standard --set tracing.enabled=true --set kiali.enabled=true $ kubectl get pod -n istio-system NAME READY STATUS RESTARTS AGE grafana-598678cbb-bglbq 1/1 Running 0 3m istio-citadel-6f9887d776-tvdg8 1/1 Running 0 3m istio-egressgateway-84d78d84bf-zpxrq 1/1 Running 0 3m istio-galley-556f5558f5-hk2r8 1/1 Running 0 3m istio-ingressgateway-78cccbddbb-gh2xl 1/1 Running 0 3m istio-pilot-799845f56d-l777d 2/2 Running 0 3m istio-policy-7666fcd574-nbx8s 2/2 Running 0 3m istio-sidecar-injector-7b6589c9-m7x77 1/1 Running 0 3m istio-statsd-prom-bridge-55965ff9c8-s6dmj 1/1 Running 0 3m istio-telemetry-844c8d6bff-9trcf 2/2 Running 0 3m istio-tracing-77f9f94b98-g7v6f 1/1 Running 0 3m kiali-bdf7fdc78-9lpd4 1/1 Running 0 3m prometheus-7456f56c96-drhlq 1/1 Running 0 3m  default namespaceにラベルを貼って自動でEnvoyが各PodにInjectionされるようにする。</description>
    </item>
    
    <item>
      <title>CircleCI 2.0でDocker imageをbuildしてタグを付けてContainer Registryに上げる</title>
      <link>https://www.sambaiz.net/article/183/</link>
      <pubDate>Wed, 22 Aug 2018 23:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/183/</guid>
      <description>masterにpushしたときと、リリースタグを切ったときにビルドされるようにする。
version: 2 jobs: build: docker: - image: google/cloud-sdk environment: GCP_PROJECT: &amp;lt;project_name&amp;gt; IMAGE_NAME: &amp;lt;image_name&amp;gt; steps: - checkout - setup_remote_docker: version: 18.05.0-ce - run: name: gcloud auth command: | echo $GCLOUD_SERVICE_KEY | base64 --decode &amp;gt; ${HOME}/gcloud-service-key.json gcloud auth activate-service-account --key-file ${HOME}/gcloud-service-key.json gcloud --quiet auth configure-docker - run: name: docker build &amp;amp; push command: | docker build -t asia.gcr.io/${GCP_PROJECT}/${IMAGE_NAME}:${CIRCLE_BUILD_NUM} . docker tag asia.gcr.io/${GCP_PROJECT}/${IMAGE_NAME}:${CIRCLE_BUILD_NUM} asia.gcr.io/${GCP_PROJECT}/${IMAGE_NAME}:latest if [ -n &amp;quot;${CIRCLE_TAG}&amp;quot; ]; then docker tag asia.</description>
    </item>
    
    <item>
      <title>KubernetesのCustom Resource Definition(CRD)とCustom Controller</title>
      <link>https://www.sambaiz.net/article/182/</link>
      <pubDate>Thu, 09 Aug 2018 23:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/182/</guid>
      <description>K8sではDeploymentを作成したときにReplicaSetも作成されるようにしたり、 Load Balancer Serviceを作成したときにGCPなどその環境に応じたLoad Balancerも作成されるようにしたりするため、Controllerがそれらを監視してAPIを呼んでいる。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
Controllerは単なるAPIを呼ぶアプリケーションなので自分でCustom Controllerを作成してDeploymentとしてデプロイすることもできる。 また、監視する対象もpodsやdeploymentsといった標準のAPIだけではなく、 Custom Resource で拡張したものを使うことができる。
特定のアプリケーションのためのControllerはOperatorとも呼ばれる。
CustomResourceDefinition(CRD) Custom Resourceを定義する。
apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: crontabs.stable.example.com spec: # REST APIで使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt; group: stable.example.com version: v1 # Namespaced か Cluster scope: Namespaced names: # 複数形 URLに使われる /apis/&amp;lt;group&amp;gt;/&amp;lt;version&amp;gt;/&amp;lt;plural&amp;gt; plural: crontabs # 単数形 CLIなどで使われる singular: crontab # manifestで使う kind: CronTab shortNames: - ct  $ kubectl create -f crd.yaml $ kubectl get crd NAME AGE crontabs.</description>
    </item>
    
    <item>
      <title>KubernetesのNetworkPolicy Resource</title>
      <link>https://www.sambaiz.net/article/181/</link>
      <pubDate>Mon, 30 Jul 2018 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/181/</guid>
      <description>Network Policies - Kubernetes
PodのトラフィックをラベルやIPアドレスで許可するためのResource。AWSのセキュリティグループやGCPのファイアウォールルールのようなもの。 GKEでは今のところデフォルトでオフになっているので--enable-network-policyを付けてクラスタを作成する必要がある。
以前作成したmulti podのアプリケーションで挙動を確認する。
GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress - sambaiz-net
$ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE clusterip-app ClusterIP 10.23.247.54 &amp;lt;none&amp;gt; 80/TCP 48m loadbalancer-app LoadBalancer 10.23.244.137 35.224.130.196 80:31508/TCP 48m nodeport-app NodePort 10.23.246.215 &amp;lt;none&amp;gt; 80:32181/TCP 48m ... $ curl -d &#39;{&amp;quot;url&amp;quot;: &amp;quot;http://nodeport-app&amp;quot;}&#39; http://35.224.130.196/ 200  作成するNetworkPolicyは以下の二つで、いずれも対象はapp: nodeport-appのラベルが付いたPod。 一つ目は対象Podへのリクエストを一旦全て拒否する。 二つ目はnodeport-access: &amp;quot;true&amp;quot;のラベルが付いたPodから対象Podへの8080ポートのリクエストを許可するもの。 今回は設定しないがegressも設定できる。
$ cat networkPolicies.yaml --- # Default deny all ingress traffic apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny spec: podSelector: matchLabels: app: nodeport-app --- apiVersion: networking.</description>
    </item>
    
    <item>
      <title>GKEでのService(ClusterIP/NodePort/LoadBalancer)とIngress</title>
      <link>https://www.sambaiz.net/article/173/</link>
      <pubDate>Sat, 23 Jun 2018 15:02:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/173/</guid>
      <description>疎通確認用アプリケーション GETでは200を返し、POSTではURLにGETリクエストを送ってステータスコードを返す。
package main import ( &amp;quot;encoding/json&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;io/ioutil&amp;quot; &amp;quot;net/http&amp;quot; ) type PostBody struct { URL string `json:&amp;quot;url&amp;quot;` } func handler(w http.ResponseWriter, r *http.Request) { if r.Method == http.MethodGet { fmt.Fprintln(w, &amp;quot;ok&amp;quot;) } else if r.Method == http.MethodPost { data, err := ioutil.ReadAll(r.Body) if err != nil { w.WriteHeader(http.StatusInternalServerError) fmt.Fprintln(w, err.Error()) return } p := PostBody{} if err := json.Unmarshal(data, &amp;amp;p); err != nil { w.WriteHeader(http.StatusBadRequest) fmt.Fprintln(w, err.Error()) return } resp, err := http.</description>
    </item>
    
    <item>
      <title>ksonnetでkubernetesのmanifestを環境ごとに生成/applyする</title>
      <link>https://www.sambaiz.net/article/171/</link>
      <pubDate>Wed, 20 Jun 2018 01:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/171/</guid>
      <description>ksonnetはJSONのテンプレートエンジンjsonnetからk8sのmanifestを環境ごとに生成してapplyするツール。kubeflowでも使われている。
$ brew install ksonnet/tap/ks $ ks version ksonnet version: 0.11.0 jsonnet version: v0.10.0 client-go version:  init まずks initしてディレクトリを作成する。
$ kubectl config current-context minikube $ ks init kstest $ cd kstest $ ls app.yaml	components	environments	lib	vendor $ cat app.yaml apiVersion: 0.1.0 environments: default: destination: namespace: default server: https://192.168.99.100:8443 k8sVersion: v1.10.0 path: default kind: ksonnet.io/app name: kstest registries: incubator: gitVersion: commitSha: 40285d8a14f1ac5787e405e1023cf0c07f6aa28c refSpec: master protocol: github uri: github.</description>
    </item>
    
    <item>
      <title>Istio v0.7でEnvoy Proxyを付けるまで</title>
      <link>https://www.sambaiz.net/article/167/</link>
      <pubDate>Tue, 29 May 2018 22:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/167/</guid>
      <description>追記(2018-09-01) v1.0となりHelmでのインストールも問題なくできるようになった。Istio-AuthがCitadelという名前になっていたりDeprecatedになってるAPIもある
IstioをHelmでインストールしてRoutingとTelemetryを行いJaeger/Kialiで確認する - sambaiz-net
 Istioとは マイクロサービス間のネットワークの、ロードバランシングや認証、モニタリングなどを担うサービスメッシュのOSS。 概念は抽象化されていて、Kubernetes以外でもサポートされている。 通信をコントロールするdata-planeのEnvoyと、Envoyを管理するcontrol-planeのPilot, Mixer, Istio-Authからなる。
Envoy Sidecarとしてデプロイされる、サービスメッシュでの全ての通信を通すプロキシ。 アプリケーションのコードに手を入れる必要がないので言語に縛られない。 CNCFのプロジェクトの一つで、 Istio用にオリジナルから拡張されている。 ロードバランシングやヘルスチェックなどを行い、メトリクスを取る。
Mixer サービスメッシュ全体のアクセスコントロールや、Envoyからデータを集めてログに出したりモニタリングしたりする。 プラグインよってAWSやGCPといったインフラバックエンドの差異が吸収される。
Pilot サービスディスカバリしてEnvoyのトラフィックを制御する。A/Bテストやカナリアリリースをする場合や、障害に対応して適切にルーティングを行うことができる。
Istio-Auth サービスやエンドユーザーの認証を行い、ポリシーに従ってアクセス制御する。
Istioのインストール ローカルのminikubeに環境を作る。 apiserver.Admission.PluginNamesでは立ち上がらなかったので代わりに apiserver.admission-controlを指定している。
$ minikube version minikube version: v0.27.0 $ minikube start \ --extra-config=apiserver.admission-control=&amp;quot;NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota&amp;quot; \ --kubernetes-version=v1.9.0 $ kubectl config current-context minikube  istioを持ってきてapplyする。 Helmも用意されていて将来的にそっちで入れるのが推奨になりそうだ。
$ curl -L https://git.io/getLatestIstio | sh - $ cd istio-0.7.1/ $ kubectl apply -f install/kubernetes/istio-auth.yaml  作成されたserviceとpodはこんな感じ。
$ kubectl get svc -o name -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingress LoadBalancer 10.</description>
    </item>
    
    <item>
      <title>TerraformでGKEクラスタとBigQueryを立てる</title>
      <link>https://www.sambaiz.net/article/165/</link>
      <pubDate>Tue, 29 May 2018 02:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/165/</guid>
      <description>GKEクラスタからBigQueryを読み書きすることを想定している。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る - sambaiz-net
GKE  google_container_cluster  oauth_scopeにbigqueryを付けている。
resource &amp;quot;google_container_cluster&amp;quot; &amp;quot;sample&amp;quot; { name = &amp;quot;${var.cluster_name}&amp;quot; description = &amp;quot;sample k8s cluster&amp;quot; zone = &amp;quot;${var.gcp_zone}&amp;quot; initial_node_count = &amp;quot;${var.initial_node_count}&amp;quot; master_auth { username = &amp;quot;${var.master_username}&amp;quot; password = &amp;quot;${var.master_password}&amp;quot; } node_config { machine_type = &amp;quot;${var.node_machine_type}&amp;quot; disk_size_gb = &amp;quot;${var.node_disk_size}&amp;quot; oauth_scopes = [ &amp;quot;https://www.googleapis.com/auth/compute&amp;quot;, &amp;quot;https://www.googleapis.com/auth/devstorage.read_only&amp;quot;, &amp;quot;https://www.googleapis.com/auth/logging.write&amp;quot;, &amp;quot;https://www.googleapis.com/auth/monitoring&amp;quot;, &amp;quot;https://www.googleapis.com/auth/bigquery&amp;quot;, ] } }  variable &amp;quot;env&amp;quot; { description = &amp;quot;system env&amp;quot; } variable &amp;quot;gcp_zone&amp;quot; { description = &amp;quot;GCP zone, e.</description>
    </item>
    
    <item>
      <title>Kubernetes,Helmで負荷試験ツールLocustを立てる</title>
      <link>https://www.sambaiz.net/article/161/</link>
      <pubDate>Sun, 18 Mar 2018 22:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/161/</guid>
      <description>OSSの負荷試験ツールLocustをK8sクラスタに立てる。 K8sならworkerの増減も簡単だし、HelmのChartもあるので立てるのも楽。
LocustはPython製で、以下のようなコードで処理を書くことができる。
@task(10)のように括弧の中に数字を書いて実行される割合を偏らせることもできる。 異なるTaskSetに対応するユーザーを複数作ることもできて、こちらもweightで重みを付けられる。 ユーザー数はあとでWeb上から入力する。
$ mkdir tasks $ cat tasks/tasks.py from locust import HttpLocust, TaskSet, task class ElbTasks(TaskSet): @task def task1(self): with client.get(&amp;quot;/&amp;quot;, catch_response=True) as response: if response.content != &amp;quot;Success&amp;quot;: response.failure(&amp;quot;Got wrong response&amp;quot;) class ElbWarmer(HttpLocust): task_set = ElbTasks min_wait = 1000 max_wait = 3000  stableにChartはあるが、今のところtasksの中を書き換えなくてはいけないようなので、forkしてきてtasksの中を書き換え、helm repo addするためにpackageして、これを参照するindex.yamlを生成した。
$ helm package . $ helm repo index . $ ls locust-0.1.2.tgz index.yaml index.yaml	locust-0.1.2.tgz $ cat index.yaml apiVersion: v1 entries: locust: .</description>
    </item>
    
    <item>
      <title>RBACが有効なGKEでHelmを使う</title>
      <link>https://www.sambaiz.net/article/160/</link>
      <pubDate>Sun, 18 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/160/</guid>
      <description>k8sのパッケージマネージャーHelmを使う - sambaiz-net
$ helm version Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;} Server: &amp;amp;version.Version{SemVer:&amp;quot;v2.8.2&amp;quot;, GitCommit:&amp;quot;a80231648a1473929271764b920a8e346f6de844&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}  GKEでhelm initしてhelm installしたところ以下のエラーが返ってきた。
Error: release my-locust failed: namespaces &amp;quot;default&amp;quot; is forbidden: User &amp;quot;system:serviceaccount:kube-system:default&amp;quot; cannot get namespaces in the namespace &amp;quot;default&amp;quot;: Unknown user &amp;quot;system:serviceaccount:kube-system:default&amp;quot;  GKEではデフォルトでK8sのRBAC(Role-Based Access Control)が有効になっているため、Tillerインスタンスに権限を与える必要がある。
ということでTiller用にnamespaceを切って、その中では好きにできるRoleと、Tillerが使うServiceAccountを作成し、RoleBindingで紐づける。
kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-manager namespace: tiller-world rules: - apiGroups: [&amp;quot;&amp;quot;, &amp;quot;extensions&amp;quot;, &amp;quot;apps&amp;quot;] resources: [&amp;quot;*&amp;quot;] verbs: [&amp;quot;*&amp;quot;] --- apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: tiller-world --- kind: RoleBinding apiVersion: rbac.</description>
    </item>
    
    <item>
      <title>Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る</title>
      <link>https://www.sambaiz.net/article/159/</link>
      <pubDate>Tue, 13 Mar 2018 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/159/</guid>
      <description>Logging AgentをNodeレベルのDaemonSetとして動かすのではなく、Podの中にSidecar Containerとして動かす。その分リソースは食うけど、独自の設定で動かせる。
アプリケーション https://github.com/sambaiz/go-logging-sample
Goで定期的にログを出すサンプルコードを書いたのでこれを使う。 viperで設定を持ち、 zapでログを出力する。 あとSIGINTを拾ってSync()してGraceful Shutdownするようにしている。
Golangの高速なロガーzapとlumberjackでログを出力してrotateさせる - sambaiz-net
multistage-buildでビルドして、GKEで動かすのでContainer Registryに上げる。
$ docker build -t go-logging-sample . $ docker tag go-logging-sample gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample:v1 $ gcloud docker -- push gcr.io/&amp;lt;project_id&amp;gt;/go-logging-sample  Fluentdの設定 fluent-plugin-bigqueryプラグインを使う。
projectとdataset、パーティションの日付分割テーブルに入れる場合は、auto_create_tableできないのでtableも作成しておく。
fluentdの設定はConfigMapで持つ。
apiVersion: v1 kind: ConfigMap metadata: name: fluentd-config data: fluent.conf: | &amp;lt;source&amp;gt; @type tail format json path /var/log/app.log pos_file /var/log/app.log.pos tag bigquery &amp;lt;/source&amp;gt; &amp;lt;match bigquery&amp;gt; @type bigquery method load &amp;lt;buffer time&amp;gt; @type file path /var/log/bigquery.</description>
    </item>
    
    <item>
      <title>ローカルでビルドしたimageをminikubeで使う</title>
      <link>https://www.sambaiz.net/article/151/</link>
      <pubDate>Thu, 01 Feb 2018 22:49:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/151/</guid>
      <description>$ minikube version minikube version: v0.25.0 $ kubectl version Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;9&amp;quot;, GitVersion:&amp;quot;v1.9.2&amp;quot;, GitCommit:&amp;quot;5fa2db2bd46ac79e5e00a4e6ed24191080aa463b&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-01-18T21:11:08Z&amp;quot;, GoVersion:&amp;quot;go1.9.2&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;darwin/amd64&amp;quot;} $ kubectl config current-context minikube $ minikube status minikube: Running cluster: Running kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100  dockerコマンドがminikube VM内で動いているdocker daemonを参照するようにする。
$ minikube docker-env export DOCKER_TLS_VERIFY=&amp;quot;1&amp;quot; export DOCKER_HOST=&amp;quot;tcp://192.168.99.100:2376&amp;quot; export DOCKER_CERT_PATH=&amp;quot;/Users/sambaiz/.minikube/certs&amp;quot; $ eval $(minikube docker-env) $ docker info --format &#39;{{json .Name}}&#39; &amp;quot;minikube&amp;quot;  ビルドするDockerfile。nginxが立ち上がるだけ。
FROM nginx  何もタグを付けない(:latest)とcreate時にDockerレジストリからpullしにいって失敗してしまうため、タグ付きでビルドする。</description>
    </item>
    
    <item>
      <title>KubernetesのパッケージマネージャーHelmを使う</title>
      <link>https://www.sambaiz.net/article/122/</link>
      <pubDate>Wed, 26 Jul 2017 01:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/122/</guid>
      <description>Kubernatesが操舵手なのに対して、Helmは舵。 パッケージはChart(海図)と呼ばれている。
ChartにはデフォルトでGoのtemplateで書かれたManifestが含まれ、values.yamlの値を-f values.yamlや--set key=valueフラグで上書きして適用しインストールすることができる。
Helmコマンドをインストールする。 今回はminikubeに入れるので立ち上げる。
$ brew install kubernetes-helm $ helm version Client: &amp;amp;version.Version{SemVer:&amp;quot;v2.5.0&amp;quot;, GitCommit:&amp;quot;012cb0ac1a1b2f888144ef5a67b8dab6c2d45be6&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;} # brew cask install virtualbox minikube $ minikube version minikube version: v0.20.0 $ minikube start Kubectl is now configured to use the cluster. $ kubectl version Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;7&amp;quot;, GitVersion:&amp;quot;v1.7.2&amp;quot;, GitCommit:&amp;quot;922a86cfcd65915a9b2f69f3f193b8907d741d9c&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2017-07-21T19:06:19Z&amp;quot;, GoVersion:&amp;quot;go1.8.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;darwin/amd64&amp;quot;} Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;6&amp;quot;, GitVersion:&amp;quot;v1.6.4&amp;quot;, GitCommit:&amp;quot;d6f433224538d4f9ca2f7ae19b252e6fcb66a3ae&amp;quot;, GitTreeState:&amp;quot;dirty&amp;quot;, BuildDate:&amp;quot;2017-06-22T04:31:09Z&amp;quot;, GoVersion:&amp;quot;go1.7.5&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;} $ kubectl config current-context minikube  まずk8sクラスタ上にHelmの管理サーバーTillerをインストールする必要がある。 ついでにリポジトリをupdateする。</description>
    </item>
    
    <item>
      <title>gcloudのアカウント切り替えとkubectlのcontext変更</title>
      <link>https://www.sambaiz.net/article/28/</link>
      <pubDate>Tue, 25 Oct 2016 20:29:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/28/</guid>
      <description> いつも迷うのでまとめた。
gcloudのアカウント一覧と切り替え $ gcloud auth list $ gcloud config set account `ACCOUNT`  configにprojectなども設定している場合はconfig自体を作成して切り替えた方が楽。
$ gcloud config configurations create &amp;lt;name&amp;gt; $ gcloud config configurations activate &amp;lt;name&amp;gt; $ gcloud config list ... Your active configuration is: [&amp;lt;name&amp;gt;] $ gcloud config set account &amp;lt;accout&amp;gt; $ gcloud config set project &amp;lt;project&amp;gt;  kubectlのcontext変更 $ kubectl config current-context $ kubectl config view # contexts $ kubectl config use-context minikube  </description>
    </item>
    
    <item>
      <title>GKEで複数コンテナのアプリケーションを動かす</title>
      <link>https://www.sambaiz.net/article/18/</link>
      <pubDate>Fri, 26 Aug 2016 21:57:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/18/</guid>
      <description>前回は単一コンテナのアプリケーションを動かしたが、今回はコンテナ間でやり取りが発生するものを動かす。 流れとしては、クライアントからのリクエストをGATEWAYサーバーで受け取り、SERVICEサーバーにリクエストし、その結果を返すまで。
プログラムは以下の通り、環境変数TYPEの値によって挙動を変えていて、同じイメージを使い回す。コードはここ。
var http = require(&#39;http&#39;); var handleRequest = function(request, response) { if(process.env.TYPE == &amp;quot;GATEWAY&amp;quot;){ console.log(&#39;Passed.&#39;); var options = { host: &#39;service&#39;, port: 8080, method: &#39;GET&#39; }; var req = http.request(options, function(res) { data = &amp;quot;&amp;quot; res.on(&#39;data&#39;, function (chunk) { data+=chunk; }); res.on(&#39;end&#39;, () =&amp;gt; { response.writeHead(200); response.end(data); }); }); req.on(&#39;error&#39;, function(e) { response.writeHead(500) response.end(e.message); }); req.end(); }else{ console.log(&#39;Received.&#39;); response.writeHead(200); response.end(&#39;ok&#39;); } }; var www = http.createServer(handleRequest); www.listen(8080);  これをContainer RegistryにPushする。</description>
    </item>
    
    <item>
      <title>Google Container Engine(GKE)で単一コンテナのアプリケーションを動かす</title>
      <link>https://www.sambaiz.net/article/17/</link>
      <pubDate>Sun, 21 Aug 2016 23:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/17/</guid>
      <description>Kubernetes - Hello World Walkthrough
CloudSDKとkubectlのインストール Cloud SDKをインストールしてgloudコマンドを使えるようにする。
$ gcloud --version Google Cloud SDK 122.0.0 $ gcloud components install kubectl  Google Container RegistryにPush $ export PROJECT_ID=&amp;quot;******&amp;quot; $ docker build -t gcr.io/$PROJECT_ID/test:v1 . $ gcloud docker push gcr.io/$PROJECT_ID/test:v1  プロジェクトの課金を有効にしていないとこんなエラーメッセージが出る。
denied: Unable to create the repository, please check that you have access to do so.  Clusterの作成 $ gcloud config set core/project $PROJECT_ID $ gcloud config set compute/zone asia-east1-b $ gcloud container clusters create test-cluster $ gcloud config set container/cluster test-cluster  Container Engine APIが有効になっていない場合はこうなる。 一度コンソールからContainer Engineを選ぶと、サービスの準備が始まって有効になる。</description>
    </item>
    
    <item>
      <title>Kubernetesのチュートリアルをたどる</title>
      <link>https://www.sambaiz.net/article/9/</link>
      <pubDate>Mon, 18 Jul 2016 22:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/9/</guid>
      <description>Kubernetesとは Kubernetes(発音はkoo-ber-nay&amp;rsquo;-tace。 ギリシャ語で操舵手。)はGoogleによって開発が始められた、アプリケーションコンテナにおける自動デプロイ、スケーリング、操作を 自動化するOSS。K8sと略される。
Minikube K8sをローカルで試すために、MinikubeというVMの中で単一ノードのK8sクラスターを動かすツールを入れる。
v0.6.0
curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.6.0/minikube-darwin-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo mv minikube /usr/local/bin/  $ minikube start Starting local Kubernetes cluster... ... $ kubectl version Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;2&amp;quot;, GitVersion:&amp;quot;v1.2.4&amp;quot;, GitCommit:&amp;quot;3eed1e3be6848b877ff80a93da3785d9034d0a4f&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;} Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;3&amp;quot;, GitVersion:&amp;quot;v1.3.0&amp;quot;, GitCommit:&amp;quot;283137936a498aed572ee22af6774b6fb6e9fd94&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;}  Pods K8sではコンテナのグループをpodと呼ぶ。pod中のコンテナは共にデプロイされ、起動し、停止する。 また、グループとして複製される。
Podの定義は次のようにyamlで書かれる。
apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80  Podの定義に望ましい状態を記述すると、Kubernatesはそれを見て現在の状態が一致しているかどうか確認する。 例えば、Podが作られたときに、コンテナがその中で動いている状態が望ましい状態だとすると、 コンテナが動かなくなったときに、Kubernatesは新しいものを再作成することで望ましい状態にする。</description>
    </item>
    
  </channel>
</rss>