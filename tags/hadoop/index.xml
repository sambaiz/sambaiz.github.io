<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>hadoop on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/hadoop/</link>
    <description>Recent content in hadoop on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Fri, 03 Dec 2021 20:00:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/hadoop/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>カラムナフォーマットParquetの構造とReadの最適化</title>
      <link>https://www.sambaiz.net/article/386/</link>
      <pubDate>Fri, 03 Dec 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/386/</guid>
      <description>Parquetは主にHadoopのエコシステムで使われるカラムナフォーマット。 CSVのようなrow-basedのフォーマットと比べて、必要ないデータを読まずに済むため効率的にクエリを実行することができる。
構造 データはいくつかのRow Groupに水平分割される。カラムはいくつかのColumn Chunkに切られ、圧縮やエンコーディングされる単位であるPageに分けられる。
ファイルの構造は次のようになっていて、カラムごとに1つのColumn Chunkを持ついくつかのRow Groupとそのメタデータが順に並んでいる。
 ネストしたスキーマにも対応していて、 子要素をフラットにした後、Repetiton LevelとDefinition Levelで繰り返しや階層を表している。
Readの最適化 ColumnMetadataにレコード数や最小最大値、圧縮コーデックやサイズが含まれているため、Readerは必要なものだけを展開して読むことができる。 したがって取得するカラムを絞ったり、フィルタに用いるカラムをソートしておくと読むPageを減らすことができる。
参考 What is Apache Parquet? - Databricks</description>
    </item>
    
    <item>
      <title>Cloudera Docker ImageでHiveの実行環境を立ち上げてJSONのログにクエリを実行する</title>
      <link>https://www.sambaiz.net/article/128/</link>
      <pubDate>Thu, 24 Aug 2017 09:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/128/</guid>
      <description>Hiveとは Hadoop上で動くデータウェアハウスソフトウェア。 SQLを拡張したHiveQLを書くとデータを処理するMapReduceやSpark、Tezのジョブが生成される。 クエリの実行に時間がかかり、耐障害性があるのでDailyやHourlyのバッチで使われる。
ちなみにAthenaにも使われているPresto はタスクを並列に実行し、中間データをメモリ上に持つことで数分以内に結果が得られるので ダッシュボードなどの用途でアドホックに使える。中間データが大きいと時間がかかったり失敗する。
Impalaはさらに速いがメモリの消費が激しいらしい。
Cloudera Docker Imageを起動する Cloudera Docker Imageには
 CDH: Clouderaのディストリビューション。Hadoop、Hive、SparkなどのOSSで構成されている。 Cloudera Manager: CDHクラスタを管理する。無料のExpressと有料のEnterpriseで使える機能に差がある。  が含まれていて、これを起動すると諸々立ち上がる。CDHクラスタを組むのはサポートされていないようなのでテスト用らしい。
$ docker pull cloudera/quickstart:latest $ docker run --hostname=quickstart.cloudera --privileged=true -itd -p 8888 -p 7180 -p 80 cloudera/quickstart /usr/bin/docker-quickstart 80がチュートリアルで、8888がHadoopのWeb UIのHue、7180がCloudera Manager。Dockerに割り当てるメモリが2GBだとFailed to contact an active Resource Managerになってしまったので4GBにした。
Hiveのテーブルを作成して実行する チュートリアルではSqoopを使ってDBから取り込んでいるんだが、 今回はjsonのログのテーブルを作成する。
$ sqoop import-all-tables \ -m 1 \ --connect jdbc:mysql://localhost:3306/retail_db \ --username=retail_dba \ --password=cloudera \ --compression-codec=snappy \ --as-parquetfile \ --warehouse-dir=/user/hive/warehouse \ --hive-import JSONを扱うにはStringからLATERAL VIEW json_tuple(json_str, &amp;quot;field1&amp;quot;, &amp;quot;field2&amp;quot;) j AS field1, field2のように実行時にパースする方法と、JSON SerDeで最初から別カラムにいれる方法があるが、今回はSerDeでやる。</description>
    </item>
    
    <item>
      <title>HDFS(Hadoop Distributed File System)とは</title>
      <link>https://www.sambaiz.net/article/126/</link>
      <pubDate>Mon, 14 Aug 2017 22:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/126/</guid>
      <description>HDFSとは Hadoopの分散ファイルシステム。 Hadoopの抽象化されたファイルシステム実装の一つで、他の実装にはLocal fileやS3などがある。 データのサイズが大きい場合に特に問題になるディスクI/Oを分散させ、 読み書きする最小の単位であるブロックサイズを大きくしシークのコストを減らすことで スループットを高めている。 ディスクI/Oがどれくらい遅いかというと、 シークがデータセンター内での往復の通信の20倍(10ms)、 1MBの読み込みが40倍の時間(20ms)かかる。
一方、小さなデータに低レイテンシでアクセスするというのは得意でなく、 また、1ファイルあたり150バイトのメタデータがNameNodeのメモリ上に乗っかるため大量のファイルを扱うのは大変。 あとデータは追記しかできない。
NameNodeとDataNode クラスタの中にはおおよそ2種類のノードがあって、 ブロックがあるいくらかのDataNodeと、
 ファイルの階層とメタデータ どのDataNodeにそのファイルのブロックがあるか  の情報が含まれる
 fsimage(メタデータのスナップショット) edit log(fsimageに含まれていない変更ログ)  を保存する、名前空間に単一のNameNodeがある。 もう一つSecondary NameNodeというのがあって、これはedit logが大きくならないよう 定期的にedit logをfsimageにマージするもの。
NameNodeが機能停止すると読み書きできなくなってしまうので、 新しいNameNodeを立てる必要がある。 その際fsimageにedit logを適用して状態を復元するため これらのファイルを別のファイルシステムにバックアップなどして失われないようにする。
巨大なクラスタだとNameNodeを立ち上げるのに30分以上かかることもあるため、 Secondary NameNodeの代わりにStandby NameNodeを立ててHigh Availabilityにすることもできる。 Standby NameNodeはNameNodeと共有ストレージでedit logを共有し、最新の状態がメモリ上に乗っているので NameNodeが死んだと判断されてから数十秒ほどで切り替えることができる。
書き込みと読み込み 書き込み ファイルシステムがNameNodeとやりとりして、ファイルが存在しているか、パーミッションがあるかを確認し、問題なければFSDataOutputStreamをクライアントに返す。 書き込むデータはdata queueにまず入って、 どのDataNodeに書き込むかはDataStreamerというのがNameNodeとやりとりして決める。 レプリカの設定数分DataNodeが選ばれ、順に書き込まれるようDataNode間でパイプラインを作る。 正常に書き込まれたDetaNodeからはack packetが返ってくるのでこれはack queryに入れて 全て正しく書き込まれたことが確認できたら消す。 失敗したDataNodeがあったら、そこに中途半端に書き込まれた分を復活したら消すようにして、パイプラインから除き 新しいパイプラインを作る。
読み込み ファイルシステムがNameNodeにファイルのブロックがどこにあるかを聞く。 NameNodeは、ブロックがあるDataNodeをクライアントからネットワークトポロジ的に近い順にソートしてアドレスを返す。 ファイルシステムはそのアドレスが含まれたFSDataInputStreamをクライアントに返して、クライアントが順にreadしていく。
SingleNode Clusterで動かす $ yum --enablerepo=epel -y install pdsh $ echo $JAVA_HOME /usr/lib/jvm/jre $ wget http://ftp.</description>
    </item>
    
  </channel>
</rss>
