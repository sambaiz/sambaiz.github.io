<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/hadoop/index.xml</link>
    <language>ja</language>
    <author>sambaiz</author>
    <rights>(C) 2017</rights>
    <updated>0001-01-01 00:00:00 &#43;0000 UTC</updated>

    
      
        <item>
          <title>Pythonのインタラクティブな可視化ライブラリBokehでグラフを描く</title>
          <link>https://www.sambaiz.net/article/129/</link>
          <pubDate>Sat, 26 Aug 2017 18:02:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/129/</guid>
          <description>

&lt;p&gt;Pythonの可視化というと&lt;a href=&#34;https://github.com/matplotlib/matplotlib&#34;&gt;matplotlib&lt;/a&gt;や、
そのラッパーの&lt;a href=&#34;https://github.com/mwaskom/seaborn&#34;&gt;seaborn&lt;/a&gt;、
データ解析ライブラリの&lt;a href=&#34;https://github.com/pandas-dev/pandas&#34;&gt;Pandas&lt;/a&gt;にもそういう機能があるけど、
これらが表示するのが静止画なのに対して、&lt;a href=&#34;https://github.com/bokeh/bokeh&#34;&gt;Bokeh&lt;/a&gt;はD3.jsで描画し、
拡大したりスクロールしたり、動的に何か表示することができる。Bokehはカメラのボケ。
似たようなのに&lt;a href=&#34;https://github.com/plotly/plotly.py&#34;&gt;Plotly&lt;/a&gt;というのもあるけど、
こちらはPandasと同じpydata.orgドメインで、スターが多い。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://hub.docker.com/r/jupyter/datascience-notebook/&#34;&gt;jupyter/datascience-notebook&lt;/a&gt;イメージにもBokehがインストールされている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker run -d -p 8888:8888 jupyter/datascience-notebook start-notebook.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;簡単なグラフを描く&#34;&gt;簡単なグラフを描く&lt;/h2&gt;

&lt;p&gt;&lt;code&gt;output_notebook&lt;/code&gt;でJupytor Notebokに出力する。ファイルに出力する場合は&lt;code&gt;ouput_file&lt;/code&gt;を呼ぶ。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;from bokeh.plotting import figure
from bokeh.io import output_notebook, show
output_notebook()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://bokeh.pydata.org/en/latest/docs/reference/plotting.html#bokeh.plotting.figure.figure&#34;&gt;figure()&lt;/a&gt;でplotするFigureオブジェクトを作成する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p = figure(
    title=&amp;quot;Hoge&amp;quot;, 
    x_axis_label=&#39;x&#39;, 
    y_axis_label=&#39;y&#39;,
    y_axis_type=&amp;quot;log&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;http://bokeh.pydata.org/en/latest/docs/reference/plotting.html#bokeh.plotting.figure.Figure.line&#34;&gt;line()&lt;/a&gt;で線をつないで&lt;a href=&#34;http://bokeh.pydata.org/en/latest/docs/reference/plotting.html#bokeh.plotting.figure.Figure.circle&#34;&gt;circle()&lt;/a&gt;で円を描く。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]
y0 = [i**2 for i in x]
y1 = [10**i for i in x]
y2 = [10**(i**2) for i in x]

p.line(x, x, legend=&amp;quot;y=x&amp;quot;)
p.circle(x, x, legend=&amp;quot;y=x&amp;quot;, fill_color=&amp;quot;white&amp;quot;, size=8)

p.line(x, y0, legend=&amp;quot;y=x^2&amp;quot;, line_width=3)

p.line(x, y1, legend=&amp;quot;y=10^x&amp;quot;, line_color=&amp;quot;red&amp;quot;)
p.circle(x, y1, legend=&amp;quot;y=10^x&amp;quot;, fill_color=&amp;quot;red&amp;quot;, line_color=&amp;quot;red&amp;quot;, size=6)

p.line(x, y2, legend=&amp;quot;y=10^x^2&amp;quot;, line_color=&amp;quot;orange&amp;quot;, line_dash=&amp;quot;4 4&amp;quot;)

show(p)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/129.png&#34; alt=&#34;折れ線グラフ&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://bokeh.pydata.org/en/latest/docs/reference/plotting.html#bokeh.plotting.figure.Figure.vbar&#34;&gt;vbar()&lt;/a&gt;で
縦棒を描く。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]

p.vbar(x, top=x, width=0.2, bottom=0, color=&amp;quot;#CAB2D6&amp;quot;)

show(p)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/129-vbar.png&#34; alt=&#34;棒グラフ&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://bokeh.pydata.org/en/latest/docs/reference/plotting.html#bokeh.plotting.figure.Figure.annular_wedge&#34;&gt;annular_wedge()&lt;/a&gt;で弧を描く。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import math
from collections import namedtuple

Data = namedtuple(&#39;Data&#39;, (&#39;name&#39;, &#39;value&#39;, &#39;color&#39;))
rates = [Data(&amp;quot;A&amp;quot;, 0.6, &amp;quot;#7FC97F&amp;quot;), Data(&amp;quot;B&amp;quot;, 0.4, &amp;quot;#DD1C77&amp;quot;)]

start_angle = 0

for rate in rates:
    p.annular_wedge(
            x=0, 
            y=0,
            inner_radius=0.2, 
            outer_radius=0.5, 
            start_angle=math.pi * 2 * start_angle, 
            end_angle=math.pi * 2 * (start_angle + rate.value),
            color=rate.color,
            legend=rate.name
    )
    start_angle += rate.value

show(p)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/129-c.png&#34; alt=&#34;円グラフ&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;複数のグラフを連動させる&#34;&gt;複数のグラフを連動させる&lt;/h2&gt;

&lt;p&gt;複数のfigureでrangeを合わせると連動する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;x = [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0]

left = figure(
    title=&amp;quot;Left&amp;quot;, 
    width=400,
    y_axis_type=&amp;quot;log&amp;quot;, 
    x_axis_label=&#39;x&#39;, 
    y_axis_label=&#39;y&#39;
)

right = figure(
    title=&amp;quot;Right&amp;quot;, 
    width=400,
    x_range=left.x_range,
    x_axis_label=&#39;x&#39;, 
    y_axis_label=&#39;y&#39;
)

left.line(x, x, legend=&amp;quot;y=x&amp;quot;)
right.line(x, x, legend=&amp;quot;y=x&amp;quot;)

p = gridplot([[left, right]])

show(p)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/129.gif&#34; alt=&#34;連動するグラフ&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;ホバーで情報を表示する&#34;&gt;ホバーで情報を表示する&lt;/h2&gt;

&lt;p&gt;sourceに&lt;code&gt;CoulumnDataSource&lt;/code&gt;を渡して、以下のように&lt;code&gt;select_one(HoverTool)&lt;/code&gt;するとホバーで情報を表示できる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import math
from bokeh.models import HoverTool, ColumnDataSource
from collections import namedtuple

Data = namedtuple(&#39;Data&#39;, (&#39;name&#39;, &#39;value&#39;, &#39;color&#39;))
rates = [Data(&amp;quot;A&amp;quot;, 0.6, &amp;quot;#7FC97F&amp;quot;), Data(&amp;quot;B&amp;quot;, 0.4, &amp;quot;#DD1C77&amp;quot;)]

start_angle = 0

for rate in rates:
    
    source = ColumnDataSource(
        data=dict(
            value=[rate.value],
        )
    )
    
    p.annular_wedge(
            x=0, 
            y=0,
            inner_radius=0.2, 
            outer_radius=0.5, 
            start_angle=math.pi * 2 * start_angle, 
            end_angle=math.pi * 2 * (start_angle + rate.value),
            color=rate.color,
            legend=rate.name,
            source=source
    )
    start_angle += rate.value

p.select_one(HoverTool).tooltips = [
    (&amp;quot;value&amp;quot;, &amp;quot;@value&amp;quot;)
]

show(p)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;https://www.sambaiz.net/images/129-c2.png&#34; alt=&#34;Tooltips&#34; /&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>Cloudera Docker ImageでHiveの実行環境を立ち上げてJSONのログにクエリを実行する</title>
          <link>https://www.sambaiz.net/article/128/</link>
          <pubDate>Thu, 24 Aug 2017 09:22:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/128/</guid>
          <description>

&lt;h2 id=&#34;hiveとは-https-cwiki-apache-org-confluence-display-hive-home-home-apachehive&#34;&gt;&lt;a href=&#34;https://cwiki.apache.org/confluence/display/Hive/Home#Home-ApacheHive&#34;&gt;Hiveとは&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Hadoop上で動くデータウェアハウスソフトウェア。
SQLを拡張したHiveQLを書くとデータを処理するMapReduceやSpark、Tezのジョブが生成される。
クエリの実行に時間がかかり、耐障害性があるのでDailyやHourlyのバッチで使われる。&lt;/p&gt;

&lt;p&gt;ちなみにAthenaにも使われている&lt;a href=&#34;https://prestodb.io/&#34;&gt;Presto&lt;/a&gt;
はタスクを並列に実行し、中間データをメモリ上に持つことで数分以内に結果が得られるので
ダッシュボードなどの用途でアドホックに使える。中間データが大きいと&lt;a href=&#34;https://aws.amazon.com/jp/blogs/news/top-10-performance-tuning-tips-for-amazon-athena/&#34;&gt;時間がかかったり失敗する&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.cloudera.com/documentation/enterprise/5-5-x/topics/impala.html&#34;&gt;Impala&lt;/a&gt;はさらに速いけどメモリの消費が激しいらしい。&lt;/p&gt;

&lt;h2 id=&#34;cloudera-docker-imageを起動する-https-www-cloudera-com-documentation-enterprise-latest-topics-quickstart-docker-container-html&#34;&gt;&lt;a href=&#34;https://www.cloudera.com/documentation/enterprise/latest/topics/quickstart_docker_container.html&#34;&gt;Cloudera Docker Imageを起動する&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Cloudera Docker Imageには&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudera.com/products/open-source/apache-hadoop/key-cdh-components.html&#34;&gt;CDH&lt;/a&gt;: Clouderaのディストリビューション。Hadoop、Hive、SparkなどのOSSで構成されている。&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.cloudera.com/documentation/enterprise/latest/topics/cloudera_manager.html&#34;&gt;Cloudera Manager&lt;/a&gt;: CDHクラスタを管理する。無料のExpressと有料のEnterpriseで使える機能に&lt;a href=&#34;https://www.cloudera.com/content/dam/www/static/documents/datasheets/cloudera-enterprise-datasheet.pdf&#34;&gt;差がある&lt;/a&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;が含まれていて、これを起動すると諸々立ち上がる。CDHクラスタを組むのはサポートされていないようなのでテスト用らしい。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ docker pull cloudera/quickstart:latest
$ docker run --hostname=quickstart.cloudera --privileged=true -itd -p 8888 -p 7180 -p 80 cloudera/quickstart /usr/bin/docker-quickstart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;80がチュートリアルで、8888がHadoopのWeb UIの&lt;a href=&#34;https://github.com/cloudera/hue&#34;&gt;Hue&lt;/a&gt;、7180がCloudera Manager。Dockerに割り当てるメモリが2GBだと&lt;code&gt;Failed to contact an active Resource Manager&lt;/code&gt;になってしまったので4GBにした。&lt;/p&gt;

&lt;h2 id=&#34;hiveのテーブルを作成して実行する&#34;&gt;Hiveのテーブルを作成して実行する&lt;/h2&gt;

&lt;p&gt;チュートリアルでは&lt;a href=&#34;http://sqoop.apache.org/&#34;&gt;Sqoop&lt;/a&gt;を使ってDBから取り込んでいるんだけど、
今回はjsonのログのテーブルを作成する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sqoop import-all-tables \
    -m 1 \
    --connect jdbc:mysql://localhost:3306/retail_db \
    --username=retail_dba \
    --password=cloudera \
    --compression-codec=snappy \
    --as-parquetfile \
    --warehouse-dir=/user/hive/warehouse \
    --hive-import
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;JSONを扱うにはStringから&lt;code&gt;LATERAL VIEW json_tuple(json_str, &amp;quot;field1&amp;quot;, &amp;quot;field2&amp;quot;) j AS field1, field2&lt;/code&gt;のように実行時にパースする方法と、&lt;a href=&#34;https://github.com/rcongiu/Hive-JSON-Serde&#34;&gt;JSON SerDe&lt;/a&gt;で最初から別カラムにいれる方法があるが、今回はSerDeでやる。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;json_tupleを使う場合、配列はStringになってしまうのでこれをArrayにするには便利なUDF(User-Defined Functions)をまとめた&lt;a href=&#34;https://github.com/klout/brickhouse&#34;&gt;brickhouse&lt;/a&gt;のjson_splitが使える。例えば、&lt;code&gt;SELECT col1 FROM table LATERAL VIEW explode(json_split(&#39;[&amp;quot;a&amp;quot;,&amp;quot;b&amp;quot;,&amp;quot;c&amp;quot; ]&#39;)) a as ja&lt;/code&gt;のようにするとArrayになったStringがexplodeしてtableの各列に3種のカラムjaが並ぶ。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;left&#34;&gt;col1&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ja&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;x&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;x&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;b&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;x&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;c&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;y&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;a&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;y&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;b&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;left&#34;&gt;y&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;c&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Arrayが空の場合にexplodeすると消滅してしまうので、LEFT JOINのように残すにはarray(null)に加工してやるとよい。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;CASE WHEN size(json_split(arr)) &amp;gt; 0 THEN json_split(arr) ELSE array(null) END AS arr
&lt;/code&gt;&lt;/pre&gt;

&lt;hr /&gt;

&lt;p&gt;JSON SerDeのjarを持ってくる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ curl http://www.congiu.net/hive-json-serde/1.3.8/cdh5/json-serde-1.3.8-jar-with-dependencies.jar &amp;gt; /usr/lib/hive/lib/json-serde-1.3.8-jar-with-dependencies.jar
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;クエリはHueから実行できて、初期ユーザー名とパスワードはどちらもcloudera。&lt;/p&gt;

&lt;p&gt;CREATE TABLEはこんな感じ。スキーマ情報はmetastoreに入る。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ADD JAR /usr/lib/hive/lib/json-serde-1.3.8-jar-with-dependencies.jar;

CREATE EXTERNAL TABLE jinrou (
        participant ARRAY&amp;lt;STRUCT&amp;lt;user_id:INT,role:STRING,team:STRING&amp;gt;&amp;gt;,
        win_team    STRING,
        ts          STRING
      )
      ROW FORMAT SERDE &#39;org.openx.data.jsonserde.JsonSerDe&#39;
      WITH SERDEPROPERTIES ( &amp;quot;mapping.ts&amp;quot; = &amp;quot;timestamp&amp;quot; )
      LOCATION &#39;/user/cloudera/jinrou&#39;;
      
ADD JAR /usr/lib/hive/lib/hive-contrib.jar;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;指定したLOCATIONにログをアップロードする。これもHueからできる。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{&amp;quot;participant&amp;quot;:[{&amp;quot;user_id&amp;quot;:1,&amp;quot;role&amp;quot;:&amp;quot;villager&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;villager&amp;quot;},{&amp;quot;user_id&amp;quot;:2,&amp;quot;role&amp;quot;:&amp;quot;wolf&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;wolf&amp;quot;},{&amp;quot;user_id&amp;quot;:3,&amp;quot;role&amp;quot;:&amp;quot;villager&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;villager&amp;quot;},{&amp;quot;user_id&amp;quot;:4,&amp;quot;role&amp;quot;:&amp;quot;medium&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;villager&amp;quot;},{&amp;quot;user_id&amp;quot;:5,&amp;quot;role&amp;quot;:&amp;quot;villager&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;villager&amp;quot;},{&amp;quot;user_id&amp;quot;:6,&amp;quot;role&amp;quot;:&amp;quot;fortune-teller&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;villager&amp;quot;}],&amp;quot;win_team&amp;quot;:&amp;quot;wolf&amp;quot;,&amp;quot;timestamp&amp;quot;:&amp;quot;2017-08-21T01:23:45.678+0900&amp;quot;}
{&amp;quot;participant&amp;quot;:[{&amp;quot;user_id&amp;quot;:3,&amp;quot;role&amp;quot;:&amp;quot;villager&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;villager&amp;quot;},{&amp;quot;user_id&amp;quot;:4,&amp;quot;role&amp;quot;:&amp;quot;wolf&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;wolf&amp;quot;},{&amp;quot;user_id&amp;quot;:1,&amp;quot;role&amp;quot;:&amp;quot;villager&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;villager&amp;quot;},{&amp;quot;user_id&amp;quot;:2,&amp;quot;role&amp;quot;:&amp;quot;medium&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;villager&amp;quot;},{&amp;quot;user_id&amp;quot;:6,&amp;quot;role&amp;quot;:&amp;quot;villager&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;villager&amp;quot;},{&amp;quot;user_id&amp;quot;:5,&amp;quot;role&amp;quot;:&amp;quot;fortune-teller&amp;quot;,&amp;quot;team&amp;quot;:&amp;quot;villager&amp;quot;}],&amp;quot;win_team&amp;quot;:&amp;quot;villager&amp;quot;,&amp;quot;timestamp&amp;quot;:&amp;quot;2017-08-21T02:34:56.789+0900&amp;quot;}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SELECT文を実行すると&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT user_id, role, SUM(is_win)/COUNT(1) AS wp FROM (
  SELECT 
    par.user_id,
    par.role, 
    CASE WHEN par.role = win_team THEN 1 ELSE 0 END AS is_win
    FROM jinrou
  LATERAL VIEW explode(participant) p AS par
) j GROUP BY user_id, role;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参照できている。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;user_id,role,wp
1,villager,0.5
2,medium,0.0
2,wolf,1.0
3,villager,0.5
4,medium,0.0
4,wolf,0.0
5,fortune-teller,0.0
5,villager,0.0
6,fortune-teller,0.0
6,villager,1.0
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;テーブルの定義よりjsonのフィールドが多いと無視されて、ないものはNULLになる。&lt;/p&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://blog-jp.treasuredata.com/entry/2014/07/10/150250&#34;&gt;『Prestoとは何か，Prestoで何ができるか』 - トレジャーデータ（Treasure Data）公式ブログ&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://recruit.gmo.jp/engineer/jisedai/blog/presto_spark_hive/&#34;&gt;スケールアウト可能なSQLエンジンのベンチマークテスト：Presto vs Spark SQL vs Hive on Tez | GMOインターネット 次世代システム研究室&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    
      
        <item>
          <title>HDFS(Hadoop Distributed File System)とは</title>
          <link>https://www.sambaiz.net/article/126/</link>
          <pubDate>Mon, 14 Aug 2017 22:52:00 &#43;0900</pubDate>
          <author></author>
          <guid>https://www.sambaiz.net/article/126/</guid>
          <description>

&lt;h2 id=&#34;hdfsとは&#34;&gt;HDFSとは&lt;/h2&gt;

&lt;p&gt;Hadoopの分散ファイルシステム。
Hadoopの抽象化されたファイルシステム実装の一つで、他の実装にはLocal fileやS3などがある。
データのサイズが大きい場合に特に問題になるディスクI/Oを分散させ、
読み書きする最小の単位であるブロックサイズを大きくしシークのコストを減らすことで
スループットを高めている。
ディスクI/Oがどれくらい遅いかというと、
シークがデータセンター内での往復の通信の20倍(10ms)、
1MBの読み込みが40倍の時間(20ms)&lt;a href=&#34;https://gist.github.com/jboner/2841832&#34;&gt;かかる&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;一方、小さなデータに低レイテンシでアクセスするというのは得意でなく、
また、1ファイルあたり150バイトのメタデータがNameNodeのメモリ上に乗っかるため大量のファイルを扱うのは大変。
あとデータは追記しかできない。&lt;/p&gt;

&lt;h3 id=&#34;namenodeとdatanode&#34;&gt;NameNodeとDataNode&lt;/h3&gt;

&lt;p&gt;クラスタの中にはおおよそ2種類のノードがあって、
ブロックがあるいくらかのDataNodeと、&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ファイルの階層とメタデータ&lt;/li&gt;
&lt;li&gt;どのDataNodeにそのファイルのブロックがあるか&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;の情報が含まれる&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;fsimage(メタデータのスナップショット)&lt;/li&gt;
&lt;li&gt;edit log(fsimageに含まれていない変更ログ)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;を保存する、名前空間に単一のNameNodeがある。
もう一つSecondary NameNodeというのがあって、これはedit logが大きくならないよう
定期的にedit logをfsimageにマージするもの。&lt;/p&gt;

&lt;p&gt;NameNodeが機能停止すると読み書きできなくなってしまうので、
新しいNameNodeを立てる必要がある。
その際fsimageにedit logを適用して状態を復元するため
これらのファイルを別のファイルシステムにバックアップなどして失われないようにする。&lt;/p&gt;

&lt;p&gt;巨大なクラスタだとNameNodeを立ち上げるのに30分以上かかることもあるため、
Secondary NameNodeの代わりにStandby NameNodeを立ててHigh Availabilityにすることもできる。
Standby NameNodeはNameNodeと共有ストレージでedit logを共有し、最新の状態がメモリ上に乗っているので
NameNodeが死んだと判断されてから数十秒ほどで切り替えることができる。&lt;/p&gt;

&lt;h2 id=&#34;書き込みと読み込み&#34;&gt;書き込みと読み込み&lt;/h2&gt;

&lt;h3 id=&#34;書き込み&#34;&gt;書き込み&lt;/h3&gt;

&lt;p&gt;ファイルシステムがNameNodeとやりとりして、ファイルが存在しているか、パーミッションがあるかを確認し、問題なければFSDataOutputStreamをクライアントに返す。
書き込むデータはdata queueにまず入って、
どのDataNodeに書き込むかはDataStreamerというのがNameNodeとやりとりして決める。
レプリカの設定数分DataNodeが選ばれ、順に書き込まれるようDataNode間でパイプラインを作る。
正常に書き込まれたDetaNodeからはack packetが返ってくるのでこれはack queryに入れて
全て正しく書き込まれたことが確認できたら消す。
失敗したDataNodeがあったら、そこに中途半端に書き込まれた分を復活したら消すようにして、パイプラインから除き
新しいパイプラインを作る。&lt;/p&gt;

&lt;h3 id=&#34;読み込み&#34;&gt;読み込み&lt;/h3&gt;

&lt;p&gt;ファイルシステムがNameNodeにファイルのブロックがどこにあるかを聞く。
NameNodeは、ブロックがあるDataNodeをクライアントからネットワークトポロジ的に近い順にソートしてアドレスを返す。
ファイルシステムはそのアドレスが含まれたFSDataInputStreamをクライアントに返して、クライアントが順にreadしていく。&lt;/p&gt;

&lt;h2 id=&#34;singlenode-clusterで動かす-http-hadoop-apache-org-docs-current-hadoop-project-dist-hadoop-common-singlecluster-html&#34;&gt;&lt;a href=&#34;http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/SingleCluster.html&#34;&gt;SingleNode Clusterで動かす&lt;/a&gt;&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;$ yum --enablerepo=epel -y install pdsh
$ echo $JAVA_HOME
/usr/lib/jvm/jre
$ wget http://ftp.jaist.ac.jp/pub/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz
$ tar xvzf hadoop-2.7.3.tar.gz 
$ cd hadoop-2.7.3
$ bin/hadoop version
Hadoop 2.7.3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;デフォルトのファイルシステムをHDFSにしてレプリカを1にする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ vi etc/hadoop/core-site.xml
&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;?xml-stylesheet type=&amp;quot;text/xsl&amp;quot; href=&amp;quot;configuration.xsl&amp;quot;?&amp;gt;
...
&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;hdfs://localhost:9000&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;

$ vi etc/hadoop/hdfs-site.xml
&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;
&amp;lt;?xml-stylesheet type=&amp;quot;text/xsl&amp;quot; href=&amp;quot;configuration.xsl&amp;quot;?&amp;gt;
...
&amp;lt;configuration&amp;gt;
    &amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt;
    &amp;lt;/property&amp;gt;
&amp;lt;/configuration&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Hadoopデーモンを起動/終了させるためにsshできるようにする。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ssh-keygen -t rsa -P &#39;&#39; -f ~/.ssh/id_rsa
$ cat ~/.ssh/id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
$ chmod 0600 ~/.ssh/authorized_keys
$ ssh localhost
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;起動。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/hdfs namenode -format
$ sbin/start-dfs.sh
localhost: starting namenode, ...
localhost: starting datanode, ...
0.0.0.0: starting secondarynamenode, ...
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ディレクトリやファイルを作成して参照する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/hdfs dfs -mkdir /home
$ bin/hdfs dfs -mkdir /user/ec2-user
$ echo &#39;aaaaa&#39; &amp;gt; hoge
$ bin/hdfs dfs -put hoge ./
$ bin/hdfs dfs -put hoge ./
put: `hoge&#39;: File exists

$ bin/hdfs dfs -appendToFile hoge hoge
$ bin/hdfs dfs -ls ./
Found 1 items
-rw-r--r--   1 ec2-user supergroup         12 2017-08-14 13:44 hoge

$ bin/hdfs dfs -cat hoge
aaaaa
aaaaa
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Filesystem check utilityを実行する。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bin/hdfs fsck ./ -files -blocks
Connecting to namenode via http://localhost:50070/fsck?ugi=ec2-user&amp;amp;files=1&amp;amp;blocks=1&amp;amp;path=%2Fuser%2Fec2-user
FSCK started by ec2-user (auth:SIMPLE) from /127.0.0.1 for path /user/ec2-user at Mon Aug 14 13:44:48 UTC 2017
/user/ec2-user &amp;lt;dir&amp;gt;
/user/ec2-user/hoge 12 bytes, 1 block(s):  OK
0. BP-478671077-172.31.3.159-1502715364675:blk_1073741825_1002 len=12 repl=1

Status: HEALTHY
 Total size:	12 B
 Total dirs:	1
 Total files:	1
 Total symlinks:		0
 Total blocks (validated):	1 (avg. block size 12 B)
 Minimally replicated blocks:	1 (100.0 %)
 Over-replicated blocks:	0 (0.0 %)
 Under-replicated blocks:	0 (0.0 %)
 Mis-replicated blocks:		0 (0.0 %)
 Default replication factor:	1
 Average block replication:	1.0
 Corrupt blocks:		0
 Missing replicas:		0 (0.0 %)
 Number of data-nodes:		1
 Number of racks:		1
FSCK ended at Mon Aug 14 13:44:48 UTC 2017 in 2 milliseconds
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;参考&#34;&gt;参考&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://shop.oreilly.com/product/0636920033448.do&#34;&gt;Hadoop: The Definitive Guide, 4th Edition&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.cloudera.com/blog/2014/03/a-guide-to-checkpointing-in-hadoop/&#34;&gt;A Guide to Checkpointing in Hadoop – Cloudera Engineering Blog&lt;/a&gt;&lt;/p&gt;
</description>
        </item>
      
    

  </channel>
</rss>
