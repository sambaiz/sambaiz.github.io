<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>read_paper on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/read_paper/</link>
    <description>Recent content in read_paper on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Thu, 07 Oct 2021 03:14:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/read_paper/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Adaptive Replacement Cache (ARC) とは</title>
      <link>https://www.sambaiz.net/article/383/</link>
      <pubDate>Thu, 07 Oct 2021 03:14:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/383/</guid>
      <description>Adaptive Replacement Cache (ARC) は4つのLRU Cacheを用いることでチューニングを自動で行いヒット率を向上させたメモリのページ置換のアルゴリズム。 PostgreSQLでバッファ管理のため一時期用いられていたが、IBMの特許を回避するため今は使われていない。
ARC: A SELF-TUNING, LOW OVERHEAD REPLACEMENT CACHE, Nimrod Megiddo and Dharmendra S. Modha
それぞれ次のデータを保持する。
 T1: 最近アクセスしたもの T2: 2回以上参照したもの B1: T1から追い出されたもののキー B2: T2から追い出されたもののキー  次の図では中央が各LRU Cacheの先頭で、!がT1の先頭、^がT1のサイズを表す。 新規データは!の左に格納され、既にT1またはB1に存在するデータは!の右に格納される。
...(B1)...[...(T1)...!...^...(T2)...]...(B2)... ghost listと呼ばれるB1とB2によってrecencyとfrequencyに応じたリソースの調整が自動で行われる。 B1でヒットした場合^を右に動かしてT1のサイズを増やし、T2の末尾のデータはB2に追い出される。 B2でヒットした場合^を左に動かしてT2のサイズを増やし、T1の末尾のデータはB1に追い出される。 いずれもヒットしなかった場合は!を^に近づける。
...(B1)...[...(T1)..!...→^.(T2)...]...(B2)... ...(B1)...[...(T1)..!..^←..(T2)...]...(B2)... ...(B1)...[...(T1)..→!..^..(T2)...]...(B2)... 参考 Adaptive replacement cache - Wikipedia
IBM patent sparks open source code rewrite | ZDNet</description>
    </item>
    
    <item>
      <title>HI-VAE(Heterogeneous-Incomple VAE)の論文を読んで処理を追う</title>
      <link>https://www.sambaiz.net/article/214/</link>
      <pubDate>Fri, 22 Mar 2019 20:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/214/</guid>
      <description>HI-VAE(Heterogeneous-Incomple VAE)は現実のデータセットにありがちな連続値と離散値が混ざっていたり欠損値を含んでいるものを扱えるようにしたVAE。
論文: Alfredo Nazabal, Pablo M. Olmos, Zoubin Ghahramani, Isabel Valera (2018) Handling Incomplete Heterogeneous Data using VAEs
生成モデルVAE(Variational Autoencoder) - sambaiz-net
GitHubにTensorFlow実装が上がっているので論文と合わせて追ってみる。
入力データ 入力データdata.csvと、そのスキーマdata_types.csvが用意されていて、 様々なtypeのデータが含まれる24次元のデータセットであることが分かる。
type,dim,nclass pos,1, cat,3,3 cat,7,7 cat,4,4 count,1, ordinal,11,11 ordinal,11,11 ordinal,11,11 ordinal,11,11 ordinal,11,11 ordinal,11,11 real,1, real,1, real,1, real,1, real,1, real,1, pos,1, pos,1, pos,1, pos,1, pos,1, pos,1, cat,2,2 これに対して、xx%の確率でランダムな次元を欠損値として扱う際に対象とする行と次元を表す Missingxx_y.csvがある。
2,1 3,1 ... 29985,24 29998,24 typeごとのデータの扱い real(実数値) 標準化してencoderに入力し、decoderでは正規分布の平均と分散を 出力し サンプリングしてデータを生成する。
pos(正の実数) 対数を標準化してencoderに入力し、decoderでは正規分布の平均と分散を 出力し サンプリングしたデータをexp()で元のレンジに戻す。
count 対数を取ってencoderに入力し、decoderではポアソン分布の平均λを 出力し サンプリングしてデータを生成する。</description>
    </item>
    
  </channel>
</rss>
