<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/python/</link>
    <description>Recent content in python on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Wed, 13 Oct 2021 02:30:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Sparkでstructをmapとして扱いexplodeで複数行に展開できるようにする</title>
      <link>https://www.sambaiz.net/article/384/</link>
      <pubDate>Wed, 13 Oct 2021 02:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/384/</guid>
      <description>Sparkでschemaを指定せずjsonなどを読み込むと次のように入力データから自動で決定される。
# {&amp;#34;aaa&amp;#34;:123,&amp;#34;ccc&amp;#34;:[123],&amp;#34;eee&amp;#34;:{&amp;#34;fff&amp;#34;:123},&amp;#34;hhh&amp;#34;:null} df = spark.read.json(&amp;#34;s3://hogefuga/testjson/&amp;#34;) df.printSchema() &amp;#39;&amp;#39;&amp;#39; root |-- aaa: long (nullable = true) |-- ccc: array (nullable = true) | |-- element: long (containsNull = true) |-- eee: struct (nullable = true) | |-- fff: long (nullable = true) |-- hhh: string (nullable = true) &amp;#39;&amp;#39;&amp;#39; これは大抵の場合うまくはたらくが、mapを想定したフィールドがstruct判定されたり、フィールドにnullしか含まれてないためにstring判定されたりすると、関数の入力に合わず処理に失敗することがある。explode()はarrayやmapを複数行に展開する関数。
df.createOrReplaceTempView(&amp;#34;tbl&amp;#34;) print(spark.sql(&amp;#34;SELECT v FROM tbl LATERAL VIEW explode(ccc) as v&amp;#34;).head()) # Row(v=123) print(spark.sql(&amp;#34;SELECT k, v FROM tbl LATERAL VIEW explode(eee) as k, v&amp;#34;).</description>
    </item>
    
    <item>
      <title>AWS GlueのJobのBookmarkを有効にして前回の続きから処理を行う</title>
      <link>https://www.sambaiz.net/article/333/</link>
      <pubDate>Fri, 16 Apr 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/333/</guid>
      <description>GlueのJobのBookmarkは どこまで処理したかを記録し、次回はその続きから実行できるようにする機能。 1.0以前は対応していなかったParquetやORCも今は対応している。
AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する - sambaiz-net
Job bookamarkをEnableにして、DynamicFrameのメソッドを呼ぶ際にtranscation_ctxを渡し、job.commit()するとBookmarkされる。
例えば、S3のjsonをソースとするテーブルをカウントして出力する次のjobは、Bookmarkを有効にすると既にカウントしたものが読まれなくなるため、 トータルの件数ではなく前回との増分が出力される。 S3の結果整合性による、参照できるようになるまでのラグも考慮されていて、 単純に保存時間のみによって対象を選ぶのではなく、リストを持って対象の時間の少し前から見るようになっている。
import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, [&amp;#39;JOB_NAME&amp;#39;]) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[&amp;#39;JOB_NAME&amp;#39;], args) source = glueContext.create_dynamic_frame.from_catalog( database=&amp;#34;test&amp;#34;, table_name=&amp;#34;test&amp;#34;, transformation_ctx=&amp;#34;source&amp;#34;) source.toDF().createOrReplaceTempView(&amp;#34;t&amp;#34;) df = spark.sql(&amp;#34;SELECT COUNT(1) as cnt FROM t&amp;#34;) df.write.mode(&amp;#34;append&amp;#34;).csv( &amp;#34;s3://*****/test-table-summary&amp;#34;, compression=&amp;#34;gzip&amp;#34;) job.</description>
    </item>
    
    <item>
      <title>PythonのProtocolによるstructural subtypingでインタフェースを記述する</title>
      <link>https://www.sambaiz.net/article/325/</link>
      <pubDate>Fri, 12 Feb 2021 02:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/325/</guid>
      <description>interfaceが文法に存在しないPythonで関数が呼べることを保証する方法の一つに 組み込み関数hasattr()によるチェックがあるが、 都度処理を挟む必要があるのと、実行してみないと分からない問題があった。
class ImplClass(): def foo(self): print(&amp;#34;ok&amp;#34;) class NoImplClass(): pass def call(d): assert hasattr(d, &amp;#39;foo&amp;#39;) d.foo() if __name__ == &amp;#34;__main__&amp;#34;: call(ImplClass()) # =&amp;gt; ok call(NoImplClass()) # =&amp;gt; AssertionError Python 3.5で実装されたType Hintsで共通の基底クラスを型として取れば実行前にmypyによる静的解析で検知できるが、サブクラスでの実装は強制できない。
PythonのType Hintsとmypy - sambaiz-net
class BaseClass: def foo(self): print(&amp;#34;please implement this&amp;#34;) class ImplClass(BaseClass): def foo(self): print(&amp;#34;ok&amp;#34;) class NoImplClass(BaseClass): pass def call(d: BaseClass): d.foo() if __name__ == &amp;#34;__main__&amp;#34;: call(BaseClass()) # =&amp;gt; please implement this call(ImplClass()) # =&amp;gt; ok call(NoImplClass()) # =&amp;gt; please implement this それを解決するのがPEP3119のAbstract Base Classesで、 次のようにabc.</description>
    </item>
    
    <item>
      <title>Pythonのmoduleとpackage</title>
      <link>https://www.sambaiz.net/article/323/</link>
      <pubDate>Sat, 23 Jan 2021 19:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/323/</guid>
      <description>6. モジュール — Python 3.9.1 ドキュメント
Pythonではファイル名がそのままimportするmodule名となり、グローバル変数__name__にその名前が入る。 エントリーポイントではこれが__main__となるので、if __name__ == &amp;quot;__main__&amp;quot;:のようにしてimportした場合は実行されないコードが書ける。
moduleをコンパイルしたpycファイルは__py_cache__にキャッシュされ、ソースの変更日時を見て再利用される。
$ cat foo.py def bar(): print(&amp;#34;hello&amp;#34;) def _bar(): print(&amp;#34;world&amp;#34;) print(&amp;#34;aaaa&amp;#34;) $ python Python 3.9.0 (default, Nov 21 2020, 14:01:50) [Clang 12.0.0 (clang-1200.0.32.27)] on darwin Type &amp;#34;help&amp;#34;, &amp;#34;copyright&amp;#34;, &amp;#34;credits&amp;#34; or &amp;#34;license&amp;#34; for more information. &amp;gt;&amp;gt;&amp;gt; import foo as f aaaa &amp;gt;&amp;gt;&amp;gt; f.__name__ foo &amp;gt;&amp;gt;&amp;gt; f.bar() hello &amp;gt;&amp;gt;&amp;gt; f._bar() world from module import *でimportすると関数外のコードは実行されず、_から始まる関数はimportされない。
&amp;gt;&amp;gt;&amp;gt; from foo import * &amp;gt;&amp;gt;&amp;gt; bar() hello &amp;gt;&amp;gt;&amp;gt; _bar() Traceback (most recent call last): File &amp;#34;&amp;lt;stdin&amp;gt;&amp;#34;, line 1, in &amp;lt;module&amp;gt; NameError: name &amp;#39;_bar&amp;#39; is not defined &amp;gt;&amp;gt;&amp;gt; from foo import _bar &amp;gt;&amp;gt;&amp;gt; _bar() world packageはmoduleをまとめたもので、そのディレクトリ名の名前空間を作る。</description>
    </item>
    
    <item>
      <title>VSCodeのDocker開発コンテナでJupyter Notebookを開いてAthenaのクエリを実行し可視化する</title>
      <link>https://www.sambaiz.net/article/294/</link>
      <pubDate>Fri, 04 Sep 2020 19:34:04 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/294/</guid>
      <description>ローカルでJupyter Notebookを動かすために以前はjupyter/datascience-notebookのイメージを立ち上げていた。 Notebookはエディタとしての機能に乏しいため通常のコードを書くのが大変だったのだが、 VSCodeのPython extensionにはJupyter notebookサポートが入っていてそのまま開けて実行できるのを知ったので移行することにした。 今回はVSCodeのDocker開発コンテナからNotebookを開いてAthenaのクエリを実行し可視化する。
VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する - sambaiz-net
環境構築 ~/.awsをマウントする。Dockerfileは上記の記事のと同じ。
{ &amp;#34;name&amp;#34;: &amp;#34;Python&amp;#34;, &amp;#34;build&amp;#34;: { &amp;#34;dockerfile&amp;#34;: &amp;#34;Dockerfile&amp;#34; }, &amp;#34;settings&amp;#34;: { &amp;#34;terminal.integrated.shell.linux&amp;#34;: &amp;#34;/bin/bash&amp;#34;, &amp;#34;python.pythonPath&amp;#34;: &amp;#34;/usr/local/bin/python&amp;#34;, &amp;#34;python.linting.enabled&amp;#34;: true, &amp;#34;python.linting.pylintEnabled&amp;#34;: false, &amp;#34;python.linting.flake8Enabled&amp;#34;: true, &amp;#34;python.linting.flake8Path&amp;#34;: &amp;#34;/usr/local/bin/flake8&amp;#34;, &amp;#34;editor.formatOnSave&amp;#34;: true, &amp;#34;python.formatting.provider&amp;#34;: &amp;#34;yapf&amp;#34;, &amp;#34;python.formatting.yapfPath&amp;#34;: &amp;#34;/usr/local/bin/yapf&amp;#34;, &amp;#34;python.linting.mypyEnabled&amp;#34;: true, &amp;#34;python.linting.mypyPath&amp;#34;: &amp;#34;/usr/local/bin/mypy&amp;#34;, }, &amp;#34;extensions&amp;#34;: [ &amp;#34;ms-python.python&amp;#34; ], &amp;#34;mounts&amp;#34;: [ &amp;#34;source=${localEnv:HOME}/.aws,target=/root/.aws,type=bind,readonly&amp;#34; ] } アプリケーションで使うパッケージはPoetryでインストールすることにしている。
PoetryでPythonの依存パッケージを管理する - sambaiz-net
KernalをvenvのPythonに切り替えるためにipykernelをインストールする必要がある。
[tool.poetry.dependencies] python = &amp;#34;^3.8&amp;#34; PyAthena = &amp;#34;^1.11.1&amp;#34; pandas = &amp;#34;^1.1.1&amp;#34; ipykernel = &amp;#34;^5.</description>
    </item>
    
    <item>
      <title>SageMakerでTensorFlowのモデルを学習させる</title>
      <link>https://www.sambaiz.net/article/293/</link>
      <pubDate>Mon, 10 Aug 2020 13:39:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/293/</guid>
      <description>以前PyTorchのモデルを学習させたが、そのTensorFlow版。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
コード 全体のコードはGitHubにある。
モデル モデルはTitanicのを使う。
TensorFlow2のKeras APIでTitanicのモデルを作る - sambaiz-net
make_csv_dataset()はbatch_sizeが必須になっているが、 これをそのままfilter()しようとすると、ValueError: predicate return type must be convertible to a scalar boolean tensor.になってしまうのでunbatch()している。 SageMakerのServing containerを用いる場合はsave_format=&amp;quot;tf&amp;quot;にしてSavedModel形式で保存する必要がある。
$ cat model.py import tensorflow as tf import logging class Model: def __init__(self, logger: logging.Logger, dropout: float): self.logger = logger self.model = tf.keras.Sequential([ tf.keras.layers.DenseFeatures(self._feature_columns()), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dropout(dropout), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dense(2, activation=tf.nn.sigmoid), ]) self.model.compile(optimizer=&amp;#39;adam&amp;#39;, loss=&amp;#39;binary_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;]) def _feature_columns(self): return [ tf.feature_column.numeric_column(&amp;#39;age&amp;#39;), tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(&amp;#39;sex&amp;#39;, 2)), tf.feature_column.numeric_column(&amp;#39;fare&amp;#39;) ] def _fill(self, feature: str, value): def __fill(x): if x[feature] == -1.</description>
    </item>
    
    <item>
      <title>TensorFlow2のKeras APIでTitanicのモデルを作る</title>
      <link>https://www.sambaiz.net/article/291/</link>
      <pubDate>Sat, 08 Aug 2020 18:32:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/291/</guid>
      <description>データセット TensorFlow Datasetsの Titanicを使う。
$ pip install tensorflow-datasets tfds.load()して tf.data.Datasetを作る。 tf.data はCPUやGPUがなるべくアイドル状態にならないようにする効率的な入力パイプラインを構築するAPI。
TensorFlowのtf.data API - sambaiz-net
import tensorflow as tf import tensorflow_datasets as tfds ds_train = tfds.load(&amp;#39;titanic&amp;#39;, split=&amp;#39;train&amp;#39;, shuffle_files=True) print(ds_train.element_spec) &amp;#39;&amp;#39;&amp;#39; {&amp;#39;features&amp;#39;: {&amp;#39;age&amp;#39;: TensorSpec(shape=(), dtype=tf.float32, name=None), &amp;#39;boat&amp;#39;: TensorSpec(shape=(), dtype=tf.string, name=None), &amp;#39;body&amp;#39;: TensorSpec(shape=(), dtype=tf.int32, name=None), &amp;#39;cabin&amp;#39;: TensorSpec(shape=(), dtype=tf.string, name=None), &amp;#39;embarked&amp;#39;: TensorSpec(shape=(), dtype=tf.int64, name=None), &amp;#39;fare&amp;#39;: TensorSpec(shape=(), dtype=tf.float32, name=None), &amp;#39;home.dest&amp;#39;: TensorSpec(shape=(), dtype=tf.string, name=None), &amp;#39;name&amp;#39;: TensorSpec(shape=(), dtype=tf.string, name=None), &amp;#39;parch&amp;#39;: TensorSpec(shape=(), dtype=tf.int32, name=None), &amp;#39;pclass&amp;#39;: TensorSpec(shape=(), dtype=tf.</description>
    </item>
    
    <item>
      <title>SageMakerで学習したPyTorchのモデルをElastic Inferenceを有効にしてデプロイする</title>
      <link>https://www.sambaiz.net/article/290/</link>
      <pubDate>Sun, 26 Jul 2020 02:43:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/290/</guid>
      <description>学習させたモデルをSageMakerのホスティングサービスにデプロイする。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
推論時に呼ばれる関数 推論時には次の関数が呼ばれる。
 model_fn(model_dir): モデルをロードする input_fn(request_body, request_content_type): リクエストボディのデシリアライズ predict_fn(input_data, model): モデルで推論する output_fn(prediction, content_type): Content-Typeに応じたシリアライズ  input_fn() と output_fn() はJSON, CSV, NPYに対応した実装が、predict_fn() はモデルを呼び出す実装がデフォルトとして用意されていて、 model_fn() も後述するElastic Inferenceを使う場合model.ptというファイルをロードするデフォルト実装が使われる。 ただしその場合モデルがtorch.jit.save()でTorchScriptとして保存してある必要がある。
今回は predict_fn() のみ実装した。
$ cat inference.py import torch def predict_fn(input_data, model): device = torch.device(&amp;#39;cuda&amp;#39; if torch.cuda.is_available() else &amp;#39;cpu&amp;#39;) model = model.to(device) input_data = input_data.to(device) model.eval() with torch.jit.optimized_execution(True, {&amp;#34;target_device&amp;#34;: &amp;#34;eia:0&amp;#34;}): output = model(input_data) return output.max(1)[1] デプロイ Training jobがモデルを保存したS3のパスを取ってきてPyTorchModelを作る。
from sagemaker.pytorch.model import PyTorchModel training_job_name = &amp;#39;pytorch-training-2020-07-25-08-41-45-674&amp;#39; training_job = sess.</description>
    </item>
    
    <item>
      <title>SageMakerでPyTorchのモデルを学習させる</title>
      <link>https://www.sambaiz.net/article/287/</link>
      <pubDate>Fri, 24 Jul 2020 22:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/287/</guid>
      <description>AWSの機械学習サービスSageMakerでPyTorchのモデルを学習させる。
コード まず学習させるモデルとそれを呼び出すエントリーポイントになるコードを書く。全体のコードはGitHubにある。 実際の環境と同じSageMakerのコンテナをローカルで動かしてVSCodeのRemote Developmentで接続して開発すると入っていないパッケージは警告が出たりして良い。
VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する - sambaiz-net
モデル 以前作ったMNISTのモデルを使う。
PyTorchでMNISTする - sambaiz-net
$ cat model.py import torch from torch import nn, cuda import torch.nn.functional as F import torch.distributed as dist import torch.optim as optim class Model(nn.Module): def __init__(self, dropout): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 64, 5) # -&amp;gt; 24x24 self.pool1 = nn.MaxPool2d(2) # -&amp;gt; 12x12 self.conv2 = nn.Conv2d(64, 128, 5) # -&amp;gt; 8x8 self.dropout = nn.Dropout(p=dropout) self.dense = nn.</description>
    </item>
    
    <item>
      <title>VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する</title>
      <link>https://www.sambaiz.net/article/289/</link>
      <pubDate>Sun, 19 Jul 2020 19:34:04 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/289/</guid>
      <description>SageMakerで学習させるモデルを開発するにあたって、Notebooks上ではコードを書きづらいのでVS Codeで書いているのだが、ローカルに依存パッケージをインストールして実行しているため エディタ上では警告が出ていなくても、実際の環境にはパッケージがなかったりすることがある。
そんな場合に便利なのがVS CodeのRemote Development。 これはローカルのVS CodeからリモートのVS Code Serverに接続してその環境で開発することができるエクステンションで、 Dockerコンテナのほか、SSHでリモートマシンやVMに接続したり、WindowsならWSLにも接続して開発環境を揃えることができる。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
設定 .devcontainer/に次のファイルを置く。
PyTorch  Dockerfile  aws/deep-learning-containersの Deep Learning Containers Imagesから選び、ECRからpullするため認証情報を登録しておく。
$ aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin https://763104351884.dkr.ecr.us-east-1.amazonaws.com これをベースに、開発用ツールを入れてVSCodeのDevelopment Container Scriptsを実行する。
$ cat .devcontainer/Dockerfile FROM763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.1-cpu-py36-ubuntu16.04 RUN conda install -y yapf flake8 mypy # VS Code Development Container Scripts # https://github.com/microsoft/vscode-dev-containers/tree/v0.128.0/script-library ARG INSTALL_ZSH=&amp;#34;true&amp;#34; ARG USERNAME=&amp;#34;vscode&amp;#34; ARG USER_UID=&amp;#34;1000&amp;#34; ARG USER_GID=&amp;#34;${USER_UID}&amp;#34; ARG UPGRADE_PACKAGES=&amp;#34;true&amp;#34; ARG COMMON_SCRIPT_SOURCE=&amp;#34;https://raw.</description>
    </item>
    
    <item>
      <title>PoetryでPythonの依存パッケージを管理する</title>
      <link>https://www.sambaiz.net/article/288/</link>
      <pubDate>Sat, 18 Jul 2020 21:23:04 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/288/</guid>
      <description>Poetryは比較的新しいPythonの依存管理ツールで、 pipenvの依存解決に失敗することがある問題を解消したり、ライブラリを開発しやすくしたものらしい。 まだスターはpipenvの半分ほどだがバージョンもv1.0.0に到達したしpipenvよりも速くて安定しているという話もあるので使ってみる。
インストール pipでインストールした。ドキュメントによると依存が衝突する可能性があるとのことだったが、自分の環境では特に問題なかった。
$ pip install --user poetry $ poetry --version Poetry version 1.0.9 使い方 PEP 518で定義されている設定ファイルpyproject.tomlを置く。
$ cat pyproject.toml [tool.poetry] name = &amp;#34;poetry-demo&amp;#34; version = &amp;#34;0.1.0&amp;#34; description = &amp;#34;&amp;#34; authors = [&amp;#34;sambaiz &amp;lt;godgourd@gmail.com&amp;gt;&amp;#34;] [tool.poetry.dependencies] python = &amp;#34;^3.7&amp;#34; [build-system] requires = [&amp;#34;poetry&amp;gt;=0.12&amp;#34;] build-backend = &amp;#34;poetry.masonry.api&amp;#34; poetry addするとvenvがなければ作成してその中にパッケージをインストールしpyproject.tomlにも追加する。
$ poetry add pendulum Creating virtualenv poetry-demo-t1vYebNd-py3.7 in /Users/*****/Library/Caches/pypoetry/virtualenvs ... $ cat pyproject.toml ... [tool.poetry.dependencies] python = &amp;#34;^3.7&amp;#34; pendulum = &amp;#34;^2.1.1&amp;#34; poetry install でpyproject.</description>
    </item>
    
    <item>
      <title>Pythonで時系列データを検定(Shapiro-Wilk test, runs test, Ljung-Box test)する</title>
      <link>https://www.sambaiz.net/article/273/</link>
      <pubDate>Sun, 21 Jun 2020 01:20:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/273/</guid>
      <description>統計的仮説検定 - sambaiz-net
テストデータ Dominick&amp;rsquo;s datasetのビールの週売上データを使う。 UPC(Universal Product Code)に対応する商品データと、店(STORE)で週(WEEK)に売れた数(MOVE)と価格(PRICE)、収益(PROFIT)を含むMovementデータがCSVで提供されている。
これらをカレントディレクトリに置いてJupyter Notebookを立ち上げる。
$ docker run -p 8888:8888 -v `pwd`:/home/jovyan/work jupyter/datascience-notebook start-notebook.sh ロードしてplotしてみる。
import pandas as pd df = pd.read_csv(&amp;#39;./wber.csv&amp;#39;, usecols=[&amp;#39;STORE&amp;#39;, &amp;#39;WEEK&amp;#39;, &amp;#39;UPC&amp;#39;, &amp;#39;MOVE&amp;#39;, &amp;#39;PRICE&amp;#39;, &amp;#39;PROFIT&amp;#39;]) agg = df[df[&amp;#39;STORE&amp;#39;] == 5].groupby([&amp;#39;WEEK&amp;#39;]).sum().loc[:, [&amp;#39;MOVE&amp;#39;, &amp;#39;PROFIT&amp;#39;]] agg.plot() この内、中央の区間の値や差分に対してα=0.05で検定する。
Shapiro-Wilk test 帰無仮説は&amp;quot;正規分布に従っている&amp;quot;。p&amp;gt;αとなり帰無仮説は棄却されず、正規分布に従うとみなせる。
from scipy import stats W, p = stats.shapiro(agg[&amp;#39;PROFIT&amp;#39;].loc[230:310]) print(f&amp;#39;p={p:.3f}&amp;#39;) # p=0.183 runs test 帰無仮説は&amp;quot;2値の数列の値がランダムである&amp;quot;。runというのは数列の連続して増加/減少している部分のことで、帰無仮説が正しい場合数列に含まれるrunの数は次の平均と分散の正規分布に従う。
updown = agg[&amp;#39;PROFIT&amp;#39;].loc[230:310].diff().map(lambda x: x &amp;gt; 0) Np = updown.value_counts()[True] Nf = updown.</description>
    </item>
    
    <item>
      <title>カテゴリカル変数をLabel/OneHotEncoderやget_dummiesで変換する</title>
      <link>https://www.sambaiz.net/article/220/</link>
      <pubDate>Mon, 06 May 2019 15:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/220/</guid>
      <description>data = [&amp;#34;tokyo&amp;#34;, &amp;#34;berlin&amp;#34;, &amp;#34;tokyo&amp;#34;, &amp;#34;paris&amp;#34;, &amp;#34;amsterdam&amp;#34;, &amp;#34;paris&amp;#34;, &amp;#34;amsterdam&amp;#34;, &amp;#34;berlin&amp;#34;] partial_data = data[:3] scikit-learnのpreprocessing.LabelEncoderでカテゴリカル変数を数値のラベルに変換できる。
from sklearn import preprocessing le = preprocessing.LabelEncoder() le.fit(data) print(le.classes_) # [&amp;#39;amsterdam&amp;#39; &amp;#39;berlin&amp;#39; &amp;#39;paris&amp;#39; &amp;#39;tokyo&amp;#39;] encoded = le.transform(partial_data) print(encoded) # [3 1 3] print(le.inverse_transform(encoded)) # [&amp;#39;tokyo&amp;#39; &amp;#39;berlin&amp;#39; &amp;#39;tokyo&amp;#39;] preprocessing.OneHotEncoder でone hot vectorに変換できる。
oh = preprocessing.OneHotEncoder() oh.fit([[d] for d in data]) print(oh.categories_[0]) # [&amp;#39;amsterdam&amp;#39; &amp;#39;berlin&amp;#39; &amp;#39;paris&amp;#39; &amp;#39;tokyo&amp;#39;] encoded = oh.transform([[d] for d in partial_data]).toarray() print(encoded) # [[0. 0. 0.</description>
    </item>
    
    <item>
      <title>Box-Cox transformationで非正規分布のデータを正規分布に近づける</title>
      <link>https://www.sambaiz.net/article/218/</link>
      <pubDate>Tue, 30 Apr 2019 17:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/218/</guid>
      <description>Box-Cox Transormationは次の式による変換。λ=0のときはlog(x)。
λが1より大きい場合は小さな値の間隔が圧縮され、小さい場合は大きな値の間隔が圧縮されるように変換される。
import numpy as np from scipy.special import boxcox1p import matplotlib.pyplot as plt from bokeh.plotting import figure from bokeh.io import output_notebook, show output_notebook() p = figure( title=&amp;#34;Box-Cox Transformations&amp;#34;, x_axis_label=&amp;#39;x&amp;#39;, y_axis_label=&amp;#39;λ&amp;#39;, ) for lam in [-1, 0, 1, 2]: v = np.array([boxcox1p(i, lam) for i in range(10)]) v = v / v.max() p.line(v, lam) p.circle(v, lam, size=8) show(p) これによって左右非対称な分布を対称(skew=0)な正規分布に近づけることができる。 以前正規分布に近づけるのに対数を取ったが、これはBox-Cox transformationの1ケースだといえる。
KaggleのHome Prices CompetitionのKernelからデータの探り方を学ぶ - sambaiz-net
試しに適当な左右非対称な分布のデータを変換してみる。
import pandas as pd import seaborn as sns p = np.</description>
    </item>
    
    <item>
      <title>KaggleのHouse Prices CompetitionのKernelからデータの探り方を学ぶ</title>
      <link>https://www.sambaiz.net/article/216/</link>
      <pubDate>Mon, 08 Apr 2019 21:01:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/216/</guid>
      <description>Kaggleの家の売値を予測するCompetitionのKernelからデータの探り方を学ぶ。
Comprehensive data exploration with Python
正規化 予測する値であるSalePriceの分布を出すと、やや左に寄った非対称の分布をしている。
import pandas as pd import seaborn as sns df = pd.read_csv(&amp;#39;train.csv&amp;#39;) df[&amp;#39;SalePrice&amp;#39;].describe() sns.distplot(df[&amp;#39;SalePrice&amp;#39;]) count 1460.000000 mean 180921.195890 std 79442.502883 min 34900.000000 25% 129975.000000 50% 163000.000000 75% 214000.000000 max 755000.000000 scipy.stats.probplot()で 生成できる、2つの分布(今回はSalePriceの分布と正規分布)の同じ分位数の値をプロットしたQ-Q (quantile-quantile) plotを見ても直線で表されている正規分布から外れていることが分かる。 本来のQ-Q plotではこの直線は同じ分位数に同じ値が来るy=xに引かれるが、この関数が生成するプロットはxがスケールされているのでそうなっていない。
from scipy import stats import matplotlib.pyplot as plt res = stats.probplot(df[&amp;#39;SalePrice&amp;#39;], dist=&amp;#39;norm&amp;#39;, plot=plt) 正規分布であることが望ましいので対数をとって近づけてやる。
import numpy as np res = stats.probplot(np.log(df[&amp;#39;SalePrice&amp;#39;]), dist=&amp;#39;norm&amp;#39;, plot=plt) なお、この変換はBox-Cox transformationのλ=0のときにあたる。
Box-Cox transformationで非正規分布のデータを正規分布に近づける - sambaiz-net</description>
    </item>
    
    <item>
      <title>HI-VAE(Heterogeneous-Incomple VAE)の論文を読んで処理を追う</title>
      <link>https://www.sambaiz.net/article/214/</link>
      <pubDate>Fri, 22 Mar 2019 20:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/214/</guid>
      <description>HI-VAE(Heterogeneous-Incomple VAE)は現実のデータセットにありがちな連続値と離散値が混ざっていたり欠損値を含んでいるものを扱えるようにしたVAE。
論文: Alfredo Nazabal, Pablo M. Olmos, Zoubin Ghahramani, Isabel Valera (2018) Handling Incomplete Heterogeneous Data using VAEs
生成モデルVAE(Variational Autoencoder) - sambaiz-net
GitHubにTensorFlow実装が上がっているので論文と合わせて追ってみる。
入力データ 入力データdata.csvと、そのスキーマdata_types.csvが用意されていて、 様々なtypeのデータが含まれる24次元のデータセットであることが分かる。
type,dim,nclass pos,1, cat,3,3 cat,7,7 cat,4,4 count,1, ordinal,11,11 ordinal,11,11 ordinal,11,11 ordinal,11,11 ordinal,11,11 ordinal,11,11 real,1, real,1, real,1, real,1, real,1, real,1, pos,1, pos,1, pos,1, pos,1, pos,1, pos,1, cat,2,2 これに対して、xx%の確率でランダムな次元を欠損値として扱う際に対象とする行と次元を表す Missingxx_y.csvがある。
2,1 3,1 ... 29985,24 29998,24 typeごとのデータの扱い real(実数値) 標準化してencoderに入力し、decoderでは正規分布の平均と分散を 出力し サンプリングしてデータを生成する。
pos(正の実数) 対数を標準化してencoderに入力し、decoderでは正規分布の平均と分散を 出力し サンプリングしたデータをexp()で元のレンジに戻す。
count 対数を取ってencoderに入力し、decoderではポアソン分布の平均λを 出力し サンプリングしてデータを生成する。</description>
    </item>
    
    <item>
      <title>VAEでエンコードしたMNISTの潜在空間をt-SNEで可視化する</title>
      <link>https://www.sambaiz.net/article/213/</link>
      <pubDate>Sun, 10 Mar 2019 19:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/213/</guid>
      <description>t-SNEは多次元のデータを2,3次元上にマッピングして可視化できるようにする手法の一つで、 Stochastic Neighbor Embedding(SNE, 確率的近傍埋め込み)という手法をベースに、t分布を用いるなどして改良したもの。
Visualizing Data using t-SNE
SNE まず入力データ間の類似度をユークリッド距離を用いた次の条件付き確率p_{j|i}で表す。 これはx_iを中心とした正規分布上で、確率密度に基づいて隣り合うデータを選ぶ場合x_jが選ばれる確率となる。
同様に出力データでも次の条件付き確率q_{j|i}を計算する。σ=1/sqrt(2)とする。
この分布間のKL情報量を勾配降下法で最小化していくことで出力を最適化するのがSME。
自己情報量、エントロピー、KL情報量、交差エントロピーと尤度関数 - sambaiz-net
ロジスティック回帰の尤度と交差エントロピーと勾配降下法 - sambaiz-net
t-SNE t-SNEでは条件付き確率ではなく同時確率を用いる。また、qを自由度1のt分布で表す。
MNISTの潜在空間をt-SNEで可視化した結果 以前作ったVAEのMNISTモデルの潜在空間を scikit-learnのTSNEで可視化する。
PyTorchでVAEのモデルを実装してMNISTの画像を生成する - sambaiz-net
%matplotlib inline import matplotlib.pyplot as plt from sklearn.manifold import TSNE from random import random colors = [&amp;#34;red&amp;#34;, &amp;#34;green&amp;#34;, &amp;#34;blue&amp;#34;, &amp;#34;orange&amp;#34;, &amp;#34;purple&amp;#34;, &amp;#34;brown&amp;#34;, &amp;#34;fuchsia&amp;#34;, &amp;#34;grey&amp;#34;, &amp;#34;olive&amp;#34;, &amp;#34;lightblue&amp;#34;] def visualize_zs(zs, labels): plt.figure(figsize=(10,10)) points = TSNE(n_components=2, random_state=0).fit_transform(zs) for p, l in zip(points, labels): plt.scatter(p[0], p[1], marker=&amp;#34;${}$&amp;#34;.</description>
    </item>
    
    <item>
      <title>PyTorchでVAEのモデルを実装してMNISTの画像を生成する</title>
      <link>https://www.sambaiz.net/article/212/</link>
      <pubDate>Thu, 07 Mar 2019 19:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/212/</guid>
      <description>PyTorchでVAEを実装しMNISTの画像を生成する。
生成モデルVAE(Variational Autoencoder) - sambaiz-net
学習データ datasetsのMNIST画像を使う。
from torchvision import datasets, transforms transform = transforms.Compose([ transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))]) dataset_train = datasets.MNIST( &amp;#39;~/mnist&amp;#39;, train=True, download=True, transform=transform) dataset_valid = datasets.MNIST( &amp;#39;~/mnist&amp;#39;, train=False, download=True, transform=transform) dataloader_train = utils.data.DataLoader(dataset_train, batch_size=1000, shuffle=True, num_workers=4) dataloader_valid = utils.data.DataLoader(dataset_valid, batch_size=1000, shuffle=True, num_workers=4) VAE それぞれ3層のEncoderとDecoder。
import torch import torch.nn as nn import torch.nn.functional as F device = &amp;#39;cuda&amp;#39; class VAE(nn.Module): def __init__(self, z_dim): super(VAE, self).__init__() self.dense_enc1 = nn.Linear(28*28, 200) self.</description>
    </item>
    
    <item>
      <title>SageMaker NotebookでGitリポジトリにSSHでpush/pullできるようにする</title>
      <link>https://www.sambaiz.net/article/211/</link>
      <pubDate>Mon, 04 Mar 2019 22:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/211/</guid>
      <description>Sagemaker NotebookはAWSの機械学習のワークフローを提供するSageMakerの一部である マネージドなJupyter Notebooksで、可視化などはもちろん、ここから複数インタンスでの学習ジョブを実行したりすることができる。
Git統合 によってノートブック作成時にGitHubなどのリポジトリを指定すると前もって持ってきてくれるようになったが、 今のところHTTPSエンドポイントにしか対応していないようで、ユーザー名・パスワードまたはトークンといった個人に紐づく認証情報が必要になる。 今回はこの機能を使わずに、ライフサイクル設定でssh鍵を置き、これでpush/pullできるようにする。
パスフレーズなしの鍵を作って公開鍵を対象リポジトリのDeployKeyに登録してread/writeできるようにする。
$ mkdir sagemaker-sshkey $ cd sagemaker-sshkey $ ssh-keygen -t rsa -b 4096 -f id_rsa -N &amp;#34;&amp;#34; $ pbcopy &amp;lt; id_rsa.pub 秘密鍵はSSMのParameter Storeに登録する。
AWS Systems Manager (SSM)のParameter Storeに認証情報を置き参照する - sambaiz-net
$ aws ssm put-parameter --name &amp;#34;sagemaker-sshkey&amp;#34; --value &amp;#34;`cat id_rsa`&amp;#34; --type String --overwrite $ aws ssm get-parameters --names &amp;#34;sagemaker-sshkey&amp;#34; ライフサイクル設定でノートブック開始時に次のスクリプトが実行されるようにする。 このスクリプトはrootで実行される。Parameter Storeが読める権限をNotebookのIAMに付けておく。
#!/bin/bash  set -e su - ec2-user &amp;lt;&amp;lt;EOF cd /home/ec2-user aws ssm get-parameters --names &amp;#34;sagemaker-sshkey&amp;#34; | jq -r &amp;#34;.</description>
    </item>
    
    <item>
      <title>PyTorchでMNISTする</title>
      <link>https://www.sambaiz.net/article/205/</link>
      <pubDate>Sat, 19 Jan 2019 23:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/205/</guid>
      <description>PyTorchはFacebookによるOSSの機械学習フレームワーク。 TensorFlow(v1)よりも簡単に使うことができる。 TensorFlow 2.0ではPyTorchのようにDefine-by-runなeager executionがデフォルトになるのに加え、パッケージも整理されるようなのでいくらか近くなると思われる。
使い方 インストール Colabで動かす。まずpipでインストール。
!pip install torch torchvision autograd(自動微分) Tensorは自身が作成された関数の参照.grad_fnを持ち、backward()が呼ばれるとbackpropしてrequires_grad=TrueなTensorの勾配を自動で計算し.gradに入れてくれる。
MLPと誤差逆伝搬法(Backpropagation) - sambaiz-net
import torch x = torch.randn(4, 4) y = torch.randn(4, 1) w = torch.randn(4, 1, requires_grad=True) b = torch.randn(1, requires_grad=True) y_pred = torch.matmul(x, w) + b loss = (y_pred - y).pow(2).sum() print(x.grad, w.grad) # None None  loss.backward() print(x.grad, w.grad) # None tensor([...]) with torch.no_grad(): y_eval = torch.matmul(x, w) + b print(y_eval.requires_grad) # False Module nnパッケージにLinearやConv2dといったModuleが実装されていて、次のように呼び出すとforward()が 呼ばれ順伝播する。</description>
    </item>
    
    <item>
      <title>AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する</title>
      <link>https://www.sambaiz.net/article/203/</link>
      <pubDate>Tue, 01 Jan 2019 17:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/203/</guid>
      <description>AWS GlueはマネージドなETL(Extract/Transform/Load)サービスで、Sparkを使ってS3などにあるデータを読み込み加工して変換出力したり、AthenaやRedshift Spectrumで参照できるデータカタログを提供する。 今回はS3のCSVを読み込んで加工し、列指向フォーマットParquetに変換しパーティションを切って出力、その後クローラを回してデータカタログにテーブルを作成してAthenaで参照できることを確認する。
料金はジョブがDPU(4vCPU/16GBメモリ)時間あたり$0.44(最低2DPU/10分)かかる。 また、クローラも同様にDPUで課金される。
なお、AthenaのCTASでもParquetを出力することができる。 出力先にファイルがないようにする必要があったり重いクエリは失敗することがあるが手軽で良い。
import * as athena from &amp;#39;athena-client&amp;#39; const clientConfig: athena.AthenaClientConfig = { bucketUri: &amp;#39;s3://*****/*****&amp;#39; skipFetchResult: true, }; const awsConfig: athena.AwsConfig = { region: &amp;#39;us-east-1&amp;#39;, }; const client = athena.createClient(clientConfig, awsConfig); (async () =&amp;gt; { await client.execute(` CREATE TABLE ***** WITH ( format = &amp;#39;PARQUET&amp;#39;, external_location = &amp;#39;s3://*****&amp;#39; ) AS ( SELECT ~~ ) })(); 開発用エンドポイント ジョブの立ち上がりにやや時間がかかるため開発用エンドポイントを立ち上げておくとDPUが確保されて効率よく開発できる。 立ち上げている間のDPUの料金がかかる。つまりずっとジョブを実行し続けているようなもので結構高くつくので終わったら閉じるようにしたい。
ローカルやEC2から自分で開発用エンドポイントにsshしてREPLで実行したりNotebookを立てることもできるが、 コンソールから立ち上げたNotebookは最初からつながっていて鍵の登録も必要なくて楽。
$ ssh -i private-key-file-path 9007:169.</description>
    </item>
    
    <item>
      <title>PythonのType Hintsとmypy</title>
      <link>https://www.sambaiz.net/article/188/</link>
      <pubDate>Sun, 30 Sep 2018 14:13:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/188/</guid>
      <description>動的型付け言語であるPythonで型アノテーションを書けるようにするための構文。 PEP 484で提案され、Python 3.5で実装された。 実行には影響せず、mypyのようなツールで静的解析したりするために使われる。
mypyをインストールする。
$ python -m pip install -U mypy 以下のように引数と返り値に型を書くと、型が誤っている場合にmypyで検知できるようになる。
$ cat main.py def foo(n: int) -&amp;gt; str: return str(n) print(foo(3)) print(foo(&amp;#39;3&amp;#39;)) $ python main.py 3 3 $ mypy main.py main.py:5: error: Argument 1 to &amp;#34;foo&amp;#34; has incompatible type &amp;#34;str&amp;#34;; expected &amp;#34;int&amp;#34; また、Type HintsがないライブラリなどのためにStubファイルを別に作って型を書くこともできるようにもなっている。デフォルトではStubがないモジュールはエラーになってしまうので必要に応じてignore_missing_importする。 mypy.iniやsetup.cfgに設定を書くと自動で使われる。
$ cat main.py import numpy as np $ mypy main.py main.py:1: error: No library stub file for module &amp;#39;numpy&amp;#39; main.py:1: note: (Stub files are from https://github.</description>
    </item>
    
    <item>
      <title>numpyの関数</title>
      <link>https://www.sambaiz.net/article/187/</link>
      <pubDate>Sun, 23 Sep 2018 23:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/187/</guid>
      <description>ndarrayの生成 ndarrayはnumpyの多次元の配列を表すオブジェクトで、[start:stop:step, &amp;hellip;]の indexでアクセスできる。
x = np.array([[1, 2, 3, 4], [2, 4, 6, 8]]) print(x[0, 1]) # 2 print(x[0,1:-1]) # [2 3] print(x[:,2]) # [3 6] print(x[:,::2]) # [[1 3] [2 6]] print(x[1,::-1]) # [8 6 4 2]  array() fromiter()  arrayやiteratableオブジェクトからndarrayを生成する。
print(np.array([1, 2, 3])) # [1 2 3] def generate(): for x in range(3): yield x x = np.fromiter(generate(), dtype=float) print(x) # [ 0. 1. 2.]  zeros() ones() full()  引数で渡したshapeを特定の値で埋めたndarrayを生成する。</description>
    </item>
    
    <item>
      <title>Pandasの操作</title>
      <link>https://www.sambaiz.net/article/170/</link>
      <pubDate>Wed, 13 Jun 2018 23:47:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/170/</guid>
      <description>SeriesとDataframe import pandas as pd import numpy as np s = pd.Series([1,3]) print(s[1]) # 3 print(s.values) # [1 3] (ndarray) dates = pd.date_range(&amp;#39;2014-11-01 10:00&amp;#39;,periods=3, freq=&amp;#39;2H&amp;#39;) print(dates) # DatetimeIndex([&amp;#39;2014-11-01 10:00:00&amp;#39;, &amp;#39;2014-11-01 12:00:00&amp;#39;, &amp;#39;2014-11-01 14:00:00&amp;#39;], dtype=&amp;#39;datetime64[ns]&amp;#39;, freq=&amp;#39;2H&amp;#39;) datestr = lambda d: pd.to_datetime(d).strftime(&amp;#39;%Y-%m-%d%H:%M&amp;#39;) df = pd.DataFrame({ &amp;#39;A&amp;#39; : 1., &amp;#39;B&amp;#39; : pd.Series(range(6), index=pd.date_range(&amp;#39;2014-11-01 10:00&amp;#39;,periods=6, freq=&amp;#39;H&amp;#39;)), &amp;#39;C&amp;#39; : [9, 1, 5], }, index=dates) df2 = pd.DataFrame({ &amp;#39;D&amp;#39; : range(3), }, index=dates) print(df) |(index) |A |B|C| |-------------------|---|-|-| |2014-11-01 10:00:00|1.</description>
    </item>
    
    <item>
      <title>ベイズ最適化でランダムフォレストとXGBoostの良いハイパーパラメータを探す</title>
      <link>https://www.sambaiz.net/article/169/</link>
      <pubDate>Sun, 10 Jun 2018 17:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/169/</guid>
      <description>ベイズ最適化で良いハイパーパラメータを総当りのグリッドサーチより効率的に探す。
ベイズ最適化はSMBO(Sequential Model-based Global Optimization)と呼ばれる、目的関数を近似するモデルと、ある値を探索すべきか評価する関数を用いて探索を進め、 実際の目的関数での評価とモデルの修正を行っていく手法の一種。 ベイズ最適化ではモデルとして、直近探索したパラメータとスコアの組の集合Dの条件付き事後確率分布P(y|x, D)を用いる。 確率モデルは、任意の入力(x1, x2, ... , xn)に対応する出力(y1, y2, ..., yn)がガウス分布(=正規分布)に従うガウス過程(GP)や、TPE(Tree-structured Parzen Estimator)が仮定される。
今回はKaggleのTitanicのチュートリアルを、チューニングなしのランダムフォレストとXGBoostで解いたときの結果と比較して、ベイズ最適化によるハイパーパラメータで精度が向上するか確認する。
KaggleのTitanicのチュートリアルをランダムフォレストで解く - sambaiz-net
KaggleのTitanicのチュートリアルをXGBoostで解く - sambaiz-net
ランダムフォレスト Pythonのベイズ最適化のライブラリ、BayesianOptimizationを使う。
$ pip install bayesian-optimization RandomForestClassifierのハイパーパラメータ
 n_estimators: 木の数 min_samples_split: ノードを分割するのに必要な最小サンプル数 max_features: 分割するときに考慮する特徴量の割合  の値を探すため、BayesianOptimizationに最大化したい値(精度)とパラメータの範囲を渡す。
from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier from bayes_opt import BayesianOptimization import pandas as pd def preprocess(df): df[&amp;#39;Fare&amp;#39;] = df[&amp;#39;Fare&amp;#39;].fillna(df[&amp;#39;Fare&amp;#39;].mean()) df[&amp;#39;Age&amp;#39;] = df[&amp;#39;Age&amp;#39;].fillna(df[&amp;#39;Age&amp;#39;].mean()) df[&amp;#39;Embarked&amp;#39;] = df[&amp;#39;Embarked&amp;#39;].fillna(&amp;#39;Unknown&amp;#39;) df[&amp;#39;Sex&amp;#39;] = df[&amp;#39;Sex&amp;#39;].</description>
    </item>
    
    <item>
      <title>KaggleのTitanicのチュートリアルをXGBoostで解く</title>
      <link>https://www.sambaiz.net/article/168/</link>
      <pubDate>Sat, 02 Jun 2018 18:16:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/168/</guid>
      <description>XGBoostは高性能なGradient Boostingのライブラリ。 Boostingというのはアンサンブル学習の種類の一つで、ランダムフォレストのように弱学習器をそれぞれ並列に学習するBaggingに対して、 順番に前回までの結果を受けながら学習し、結果をまとめる際にそれぞれの重みを掛けるもの。 XGBoostではランダムフォレストと同様に決定木を弱学習器とする。
KaggleのTitanicのチュートリアルをランダムフォレストで解く - sambaiz-net
$ pip install xgboost XGBoostは欠損値をそのまま扱うこともできるが、今回は以前と同じようにデータの前処理を行った。 パラメータの objective(目的関数)には二値分類なのでbinary:logisticを指定し、確率が返るのでroundして出力している。
import pandas as pd import xgboost as xgb from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def preprocess(df): df[&amp;#39;Fare&amp;#39;] = df[&amp;#39;Fare&amp;#39;].fillna(df[&amp;#39;Fare&amp;#39;].mean()) df[&amp;#39;Age&amp;#39;] = df[&amp;#39;Age&amp;#39;].fillna(df[&amp;#39;Age&amp;#39;].mean()) df[&amp;#39;Embarked&amp;#39;] = df[&amp;#39;Embarked&amp;#39;].fillna(&amp;#39;Unknown&amp;#39;) df[&amp;#39;Sex&amp;#39;] = df[&amp;#39;Sex&amp;#39;].apply(lambda x: 1 if x == &amp;#39;male&amp;#39; else 0) df[&amp;#39;Embarked&amp;#39;] = df[&amp;#39;Embarked&amp;#39;].map( {&amp;#39;S&amp;#39;: 0, &amp;#39;C&amp;#39;: 1, &amp;#39;Q&amp;#39;: 2, &amp;#39;Unknown&amp;#39;: 3} ).astype(int) df = df.drop([&amp;#39;Cabin&amp;#39;,&amp;#39;Name&amp;#39;,&amp;#39;PassengerId&amp;#39;,&amp;#39;Ticket&amp;#39;],axis=1) return df def train(df): train_x = df.</description>
    </item>
    
    <item>
      <title>KaggleのTitanicのチュートリアルをランダムフォレストで解く</title>
      <link>https://www.sambaiz.net/article/166/</link>
      <pubDate>Tue, 29 May 2018 09:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/166/</guid>
      <description>ランダムフォレストはデータや特徴量をランダムにサンプリングして決定木を複数生成し並列に学習するアンサンブル学習のBaggingという種類の手法。 決定木なので特徴量の影響が分かりやすく、値の大小で分岐するので標準化の必要がないが、複数の変数で表現される特徴を学習しづらい。 また、単一の決定木と比べて過学習を防ぐことができる。
KaggleのTitanicのチュートリアルをXGBoostで解く - sambaiz-net
train.csvとtest.csvをKaggleからダウンロードする。 csvにはタイタニックの乗客者リストが含まれ、test.csvには生還したかを表すSurvivedが抜けている。 これを予測するのがこのコンペティションの目的。
データのうちFareやAge、Embarkedは入っていないものがあって、これらの欠損値をどう扱うという問題がある。
df = pd.read_csv(&amp;#39;./train.csv&amp;#39;) print(len(df)) print(df.isnull().sum()) 891 PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 欠損値については連続値をとるFareとAgeは平均を取り、Embarkedは専用のカテゴリーにしてみた。決定木は分岐を繰り返すためlabel encodingで学習できる。 数値化できないものについては除いている。
def preprocess(df): df[&amp;#39;Fare&amp;#39;] = df[&amp;#39;Fare&amp;#39;].fillna(df[&amp;#39;Fare&amp;#39;].mean()) df[&amp;#39;Age&amp;#39;] = df[&amp;#39;Age&amp;#39;].fillna(df[&amp;#39;Age&amp;#39;].mean()) df[&amp;#39;Embarked&amp;#39;] = df[&amp;#39;Embarked&amp;#39;].fillna(&amp;#39;Unknown&amp;#39;) df[&amp;#39;Sex&amp;#39;] = df[&amp;#39;Sex&amp;#39;].apply(lambda x: 1 if x == &amp;#39;male&amp;#39; else 0) df[&amp;#39;Embarked&amp;#39;] = df[&amp;#39;Embarked&amp;#39;].map( {&amp;#39;S&amp;#39;: 0, &amp;#39;C&amp;#39;: 1, &amp;#39;Q&amp;#39;: 2, &amp;#39;Unknown&amp;#39;: 3} ).</description>
    </item>
    
    <item>
      <title>Pythonのasyncioで非同期にリクエストを飛ばす</title>
      <link>https://www.sambaiz.net/article/162/</link>
      <pubDate>Sat, 14 Apr 2018 13:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/162/</guid>
      <description>Pythonのasyncioはイベントループを回してシングルスレッドで並行に非同期処理を行う。 マルチスレッドで並列に実行するのがthreadingで、 マルチプロセスで並列に実行するのがmultiprocessing。
import asyncio async def sleep(s): await asyncio.sleep(s) print(s) return s loop = asyncio.get_event_loop() loop.run_until_complete(sleep(5)) coros = [sleep(3), sleep(2)] futures = asyncio.gather(*coros) loop.run_until_complete(futures) print(futures.result()) loop.close() $ python main.py 5 2 3 [3, 2] get_event_loop() でイベントループを取得し、 gather()で処理をまとめたりして、 run_until_complete()で Futureの完了を待ち、 結果を取得してイベントループをclose()している。
async defを付けた関数はCoroutineとなり、 ensure_future()でFutureのサブクラスの、イベントループで実行させるTaskにすることができる。 run_until_complete()はそのままCoroutineを投げてもensure_future()でwrapしてくれる。
httpクライアントrequestsはBlockingするようなので、asyncioに対応しているaiohttpを使ってリクエストしてみる。
import aiohttp import asyncio import async_timeout async def fetch(session, url): print(&amp;#34;{} start&amp;#34;.format(url)) async with async_timeout.timeout(10): async with session.get(url) as response: text = await response.text() print(&amp;#34;{} done&amp;#34;.</description>
    </item>
    
    <item>
      <title>KubernetesにHelmでLocustによる分散負荷試験環境を立てる</title>
      <link>https://www.sambaiz.net/article/161/</link>
      <pubDate>Sun, 18 Mar 2018 22:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/161/</guid>
      <description>OSSの負荷試験ツールLocustをK8sクラスタに立てる。 K8sならworkerの増減も簡単だし、HelmのChartもあるので立てるのも楽。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
LocustはPython製で、以下のようなコードで処理を書くことができる。
@task(10)のように括弧の中に数字を書いて実行される割合を偏らせることもできる。 異なるTaskSetに対応するユーザーを複数作ることもできて、こちらもweightで重みを付けられる。 ユーザー数はあとでWeb上から入力する。
$ mkdir tasks $ cat tasks/tasks.py from locust import HttpLocust, TaskSet, task class ElbTasks(TaskSet): @task def task1(self): with self.client.get(&amp;#34;/&amp;#34;, catch_response=True) as response: if response.content != &amp;#34;Success&amp;#34;: response.failure(&amp;#34;Got wrong response&amp;#34;) class ElbWarmer(HttpLocust): task_set = ElbTasks min_wait = 1000 max_wait = 3000 stableにChartはあるが、今のところtasksの中を書き換えなくてはいけないようなので、forkしてきてtasksの中を書き換え、package して、helm repo index でこれを参照するindex.yamlを生成した。
 追記(2020-03-11): 今はConfigmapを自分で作成し --set worker.config.configmapName=*** することでforkしなくてもよくなった kubectl create configmap locust-worker-configs --from-file tasks/tasks.py
 $ helm package .</description>
    </item>
    
    <item>
      <title>Pythonのインタラクティブな可視化ライブラリBokehでグラフを描く</title>
      <link>https://www.sambaiz.net/article/129/</link>
      <pubDate>Sat, 26 Aug 2017 18:02:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/129/</guid>
      <description>Pythonの可視化というとmatplotlibや、 そのラッパーのseaborn、 データ解析ライブラリのPandasにもそういう機能があるけど、 これらが表示するのが静止画なのに対して、BokehはD3.jsで描画し、 拡大したりスクロールしたり、動的に何か表示することができる。Bokehはカメラのボケ。 似たようなのにPlotlyというのもあるけど、 こちらはPandasと同じpydata.orgドメインで、スターが多い。
jupyter/datascience-notebookイメージにもBokehがインストールされている。
$ docker run -d -p 8888:8888 jupyter/datascience-notebook start-notebook.sh 簡単なグラフを描く output_notebookでJupytor Notebokに出力する。ファイルに出力する場合はouput_fileを呼ぶ。
from bokeh.plotting import figure from bokeh.io import output_notebook, show output_notebook() figure()でplotするFigureオブジェクトを作成する。
p = figure( title=&amp;#34;Hoge&amp;#34;, x_axis_label=&amp;#39;x&amp;#39;, y_axis_label=&amp;#39;y&amp;#39;, y_axis_type=&amp;#34;log&amp;#34; ) line()で線をつないでcircle()で円を描く。
x = [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0] y0 = [i**2 for i in x] y1 = [10**i for i in x] y2 = [10**(i**2) for i in x] p.line(x, x, legend=&amp;#34;y=x&amp;#34;) p.</description>
    </item>
    
    <item>
      <title>PythonのLintとFormatter</title>
      <link>https://www.sambaiz.net/article/125/</link>
      <pubDate>Fri, 11 Aug 2017 14:57:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/125/</guid>
      <description>YAPF スタイルに沿って整形してくれる、Goでいうgo fmtみたいなもの。 デフォルトはPython公式のスタイルガイドPEP8でフォーマットされる。
$ pip install yapf VSCodeでPythonを書くときは、 Pythonプラグイン を入れてこんな設定をWorkspaceのconfigに入れておいて、 保存した時にフォーマットがかかるようにすると快適。
&amp;#34;editor.formatOnSave&amp;#34;: true, &amp;#34;python.formatting.provider&amp;#34;: &amp;#34;yapf&amp;#34; setup.cfgに次のような項目を追加しスタイルを設定する。
[yapf] based_on_style = google column_limit = 120 indent_width = 2 Lint YAPFでフォーマットされた以下のコードにLintをかける。
class FizzBuzz: def __init__(self, start=0): self.num = start def __iter__(self): return self def __next__(self): self.num += 1 if self.num % 15 == 0: return &amp;#34;FizzBuzz&amp;#34; if self.num % 3 == 0: return &amp;#34;Fizz&amp;#34; if self.num % 5 == 0: return &amp;#34;Buzz&amp;#34; return self.</description>
    </item>
    
  </channel>
</rss>
