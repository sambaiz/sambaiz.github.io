<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/python/</link>
    <description>Recent content in Python on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Sun, 10 Mar 2019 19:03:00 +0900</lastBuildDate>
    
	<atom:link href="https://www.sambaiz.net/tags/python/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>VAEでエンコードしたMNISTの潜在空間をt-SNEで可視化する</title>
      <link>https://www.sambaiz.net/article/213/</link>
      <pubDate>Sun, 10 Mar 2019 19:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/213/</guid>
      <description>t-SNEは多次元のデータを2,3次元上にマッピングして可視化できるようにする手法の一つで、 Stochastic Neighbor Embedding(SNE, 確率的近傍埋め込み)という手法をベースに、t分布を用いるなどして改良したもの。
Visualizing Data using t-SNE
SNE まず入力データ間の類似度をユークリッド距離を用いた次の条件付き確率p_{j|i}で表す。 これはx_iを中心とした正規分布上で、確率密度に基づいて隣り合うデータを選ぶ場合x_jが選ばれる確率となる。
同様に出力データでも次の条件付き確率q_{j|i}を計算する。σ=1/sqrt(2)とする。
この分布間のKL情報量を勾配降下法で最小化していくことで出力を最適化するのがSME。
自己情報量、エントロピー、KL情報量、交差エントロピーと尤度関数 - sambaiz-net
ロジスティック回帰の尤度と交差エントロピーと勾配降下法 - sambaiz-net
t-SNE t-SNEでは条件付き確率ではなく同時確率を用いる。また、qを自由度1のt分布で表す。
MNISTの潜在空間をt-SNEで可視化した結果 以前作ったVAEのMNISTモデルの潜在空間を scikit-learnのTSNEで可視化する。
PyTorchでVAEのモデルを実装してMNISTの画像を生成する - sambaiz-net
%matplotlib inline import matplotlib.pyplot as plt from sklearn.manifold import TSNE from random import random colors = [&amp;quot;red&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;blue&amp;quot;, &amp;quot;orange&amp;quot;, &amp;quot;purple&amp;quot;, &amp;quot;brown&amp;quot;, &amp;quot;fuchsia&amp;quot;, &amp;quot;grey&amp;quot;, &amp;quot;olive&amp;quot;, &amp;quot;lightblue&amp;quot;] def visualize_zs(zs, labels): plt.figure(figsize=(10,10)) points = TSNE(n_components=2, random_state=0).fit_transform(zs) for p, l in zip(points, labels): plt.scatter(p[0], p[1], marker=&amp;quot;${}$&amp;quot;.</description>
    </item>
    
    <item>
      <title>PyTorchでVAEのモデルを実装してMNISTの画像を生成する</title>
      <link>https://www.sambaiz.net/article/212/</link>
      <pubDate>Thu, 07 Mar 2019 19:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/212/</guid>
      <description>PyTorchでVAEを実装しMNISTの画像を生成する。
生成モデルVAE(Variational Autoencoder) - sambaiz-net
学習データ datasetsのMNIST画像を使う。
from torchvision import datasets, transforms transform = transforms.Compose([ transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))]) dataset_train = datasets.MNIST( &#39;~/mnist&#39;, train=True, download=True, transform=transform) dataset_valid = datasets.MNIST( &#39;~/mnist&#39;, train=False, download=True, transform=transform) dataloader_train = utils.data.DataLoader(dataset_train, batch_size=1000, shuffle=True, num_workers=4) dataloader_valid = utils.data.DataLoader(dataset_valid, batch_size=1000, shuffle=True, num_workers=4)  VAE それぞれ3層のEncoderとDecoder。
import torch import torch.nn as nn import torch.nn.functional as F device = &#39;cuda&#39; class VAE(nn.Module): def __init__(self, z_dim): super(VAE, self).__init__() self.dense_enc1 = nn.</description>
    </item>
    
    <item>
      <title>SageMaker NotebookでGitリポジトリにSSHでpush/pullできるようにする</title>
      <link>https://www.sambaiz.net/article/211/</link>
      <pubDate>Mon, 04 Mar 2019 22:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/211/</guid>
      <description>Sagemaker NotebookはAWSの機械学習のワークフローを提供するSageMakerの一部である マネージドなJupyter Notebooksで、可視化などはもちろん、ここから複数インタンスでの学習ジョブを実行したりすることができる。
Git統合 によってノートブック作成時にGitHubなどのリポジトリを指定すると前もって持ってきてくれるようになったが、 今のところHTTPSエンドポイントにしか対応していないようで、ユーザー名・パスワードまたはトークンといった個人に紐づく認証情報が必要になる。 今回はこの機能を使わずに、ライフサイクル設定でssh鍵を置き、これでpush/pullできるようにする。
パスフレーズなしの鍵を作って公開鍵を対象リポジトリのDeployKeyに登録してread/writeできるようにする。
$ mkdir sagemaker-sshkey $ cd sagemaker-sshkey $ ssh-keygen -t rsa -b 4096 -f id_rsa -N &amp;quot;&amp;quot; $ pbcopy &amp;lt; id_rsa.pub  秘密鍵はSSMのParameter Storeに登録する。
AWS Systems Manager (SSM)のParameter Storeに認証情報を置き参照する - sambaiz-net
$ aws ssm put-parameter --name &amp;quot;sagemaker-sshkey&amp;quot; --value &amp;quot;`cat id_rsa`&amp;quot; --type String --overwrite $ aws ssm get-parameters --names &amp;quot;sagemaker-sshkey&amp;quot;  ライフサイクル設定でノートブック開始時に次のスクリプトが実行されるようにする。 このスクリプトはrootで実行される。Parameter Storeが読める権限をNotebookのIAMに付けておく。
#!/bin/bash set -e su - ec2-user &amp;lt;&amp;lt;EOF cd /home/ec2-user aws ssm get-parameters --names &amp;quot;sagemaker-sshkey&amp;quot; | jq -r &amp;quot;.</description>
    </item>
    
    <item>
      <title>PyTorchでMNISTする</title>
      <link>https://www.sambaiz.net/article/205/</link>
      <pubDate>Sat, 19 Jan 2019 23:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/205/</guid>
      <description>PyTorchはFacebookによるOSSの機械学習フレームワーク。 TensorFlow(v1)よりも簡単に使うことができる。 TensorFlow 2.0ではPyTorchのようにDefine-by-runなeager executionがデフォルトになるのに加え、パッケージも整理されるようなのでいくらか近くなると思われる。
使い方 インストール Colabで動かす。まずpipでインストール。
!pip install torch torchvision  autograd(自動微分) Tensorは自身が作成された関数の参照.grad_fnを持ち、backward()が呼ばれるとbackpropしてrequires_grad=TrueなTensorの勾配を自動で計算し.gradに入れてくれる。
MLPと誤差逆伝搬法(Backpropagation) - sambaiz-net
import torch x = torch.randn(4, 4) y = torch.randn(4, 1) w = torch.randn(4, 1, requires_grad=True) b = torch.randn(1, requires_grad=True) y_pred = torch.matmul(x, w) + b loss = (y_pred - y).pow(2).sum() print(x.grad, w.grad) # None None loss.backward() print(x.grad, w.grad) # None tensor([...]) with torch.no_grad(): y_eval = torch.matmul(x, w) + b print(y_eval.requires_grad) # False  Module nnパッケージにLinearやConv2dといったModuleが実装されていて、次のように呼び出すとforward()が 呼ばれ順伝播する。</description>
    </item>
    
    <item>
      <title>AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する</title>
      <link>https://www.sambaiz.net/article/203/</link>
      <pubDate>Tue, 01 Jan 2019 17:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/203/</guid>
      <description>AWS GlueはマネージドなETL(Extract/Transform/Load)サービスで、Sparkを使ってS3などにあるデータを読み込み加工して変換出力したり、AthenaやRedshift Spectrumで参照できるデータカタログを提供する。 今回はS3のCSVを読み込んで加工し、列指向フォーマットParquetに変換しパーティションを切って出力、その後クローラを回してデータカタログにテーブルを作成してAthenaで参照できることを確認する。
料金はジョブがDPU(4vCPU/16GBメモリ)時間あたり$0.44(最低2DPU/10分)かかる。 また、クローラも同様にDPUで課金される。結構高い。
なお、AthenaのCTASでもParquetを出力することができる。 出力先にファイルがないようにする必要があったり重いクエリは失敗することがあるが手軽で良い。
import * as athena from &#39;athena-client&#39; const clientConfig: athena.AthenaClientConfig = { bucketUri: &#39;s3://*****/*****&#39; skipFetchResult: true, }; const awsConfig: athena.AwsConfig = { region: &#39;us-east-1&#39;, }; const client = athena.createClient(clientConfig, awsConfig); (async () =&amp;gt; { await client.execute(` CREATE TABLE ***** WITH ( format = &#39;PARQUET&#39;, external_location = &#39;s3://*****&#39; ) AS ( SELECT ~~ ) })();  開発用エンドポイント ジョブの立ち上がりにやや時間がかかるため開発用エンドポイントを立ち上げておくとDPUが確保されて効率よく開発できる。 立ち上げている間のDPUの料金がかかる。つまりずっとジョブを実行し続けているようなもので結構高くつくので終わったら閉じるようにしたい。
ローカルやEC2から自分で開発用エンドポイントにsshしてNotebookを立てることもできるが、 コンソールから立ち上げたNotebookは最初からつながっていて鍵の登録も必要なくて楽。
ssh -i private-key-file-path -NTL 9007:169.</description>
    </item>
    
    <item>
      <title>PythonのType Hintsとmypy</title>
      <link>https://www.sambaiz.net/article/188/</link>
      <pubDate>Sun, 30 Sep 2018 14:13:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/188/</guid>
      <description>動的型付け言語であるPythonで型アノテーションを書けるようにするための構文。 PEP 484で提案され、Python 3.5で実装された。 実行には影響せず、mypyのようなツールで静的解析したりするために使われる。
mypyをインストールする。
$ python -m pip install -U mypy  以下のように引数と返り値に型を書くと、型が誤っている場合にmypyで検知できるようになる。
$ cat main.py def foo(n: int) -&amp;gt; str: return str(n) print(foo(3)) print(foo(&#39;3&#39;)) $ python main.py 3 3 $ mypy main.py main.py:5: error: Argument 1 to &amp;quot;foo&amp;quot; has incompatible type &amp;quot;str&amp;quot;; expected &amp;quot;int&amp;quot;  また、Type HintsがないライブラリなどのためにStubファイルを別に作って型を書くこともできるようにもなっている。デフォルトではStubがないモジュールはエラーになってしまうので必要に応じてignore_missing_importする。 mypy.iniやsetup.cfgに設定を書くと自動で使われる。
$ cat main.py import numpy as np $ mypy main.py main.py:1: error: No library stub file for module &#39;numpy&#39; main.</description>
    </item>
    
    <item>
      <title>numpyの関数</title>
      <link>https://www.sambaiz.net/article/187/</link>
      <pubDate>Sun, 23 Sep 2018 23:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/187/</guid>
      <description>ndarrayの生成 ndarrayはnumpyの多次元の配列を表すオブジェクトで、[start:stop:step, &amp;hellip;]の indexでアクセスできる。
x = np.array([[1, 2, 3, 4], [2, 4, 6, 8]]) print(x[0, 1]) # 2 print(x[0,1:-1]) # [2 3] print(x[:,2]) # [3 6] print(x[:,::2]) # [[1 3] [2 6]] print(x[1,::-1]) # [8 6 4 2]   array() fromiter()  arrayやiteratableオブジェクトからndarrayを生成する。
print(np.array([1, 2, 3])) # [1 2 3] def generate(): for x in range(3): yield x x = np.fromiter(generate(), dtype=float) print(x) # [ 0. 1. 2.]   zeros() ones() full()  引数で渡したshapeを特定の値で埋めたndarrayを生成する。</description>
    </item>
    
    <item>
      <title>Pandasの操作</title>
      <link>https://www.sambaiz.net/article/170/</link>
      <pubDate>Wed, 13 Jun 2018 23:47:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/170/</guid>
      <description>SeriesとDataframe import pandas as pd import numpy as np s = pd.Series([1,3]) print(s[1]) # 3 print(s.values) # [1 3] (ndarray) dates = pd.date_range(&#39;2014-11-01 10:00&#39;,periods=3, freq=&#39;2H&#39;) print(dates) # DatetimeIndex([&#39;2014-11-01 10:00:00&#39;, &#39;2014-11-01 12:00:00&#39;, &#39;2014-11-01 14:00:00&#39;], dtype=&#39;datetime64[ns]&#39;, freq=&#39;2H&#39;) datestr = lambda d: pd.to_datetime(d).strftime(&#39;%Y-%m-%d %H:%M&#39;) df = pd.DataFrame({ &#39;A&#39; : 1., &#39;B&#39; : pd.Series(range(6), index=pd.date_range(&#39;2014-11-01 10:00&#39;,periods=6, freq=&#39;H&#39;)), &#39;C&#39; : [9, 1, 5], }, index=dates) df2 = pd.DataFrame({ &#39;D&#39; : range(3), }, index=dates) print(df)  |(index) |A |B|C| |-------------------|---|-|-| |2014-11-01 10:00:00|1.</description>
    </item>
    
    <item>
      <title>ベイズ最適化でランダムフォレストとXGBoostの良いハイパーパラメータを探す</title>
      <link>https://www.sambaiz.net/article/169/</link>
      <pubDate>Sun, 10 Jun 2018 17:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/169/</guid>
      <description>機械学習の良いハイパーパラメータを探す方法として、scikit-learnにもあるグリッドサーチがあるが、これは総当たりで試すもので時間がかかる。
それに対してベイズ最適化は、まず現在の最大値を超える確率や期待値を出力とする獲得関数を決めて、ガウス過程(GP)に従うと仮定する。 ガウス過程は回帰関数の確率モデルで、任意の入力(x1, x2, &amp;hellip; , xn)に対応する出力(y1, y2, &amp;hellip;, yn)がガウス分布(=正規分布)に従うというもの。 これによって予測されるまだ試していない入力での期待値や分散から次に試す値を決めて効率的に探すことができる。
今回はKaggleのTitanicのチュートリアルを、チューニングなしのランダムフォレストとXGBoostで解いたときの結果と比較して、ベイズ最適化によるハイパーパラメータで精度が向上するか確認する。
KaggleのTitanicのチュートリアルをランダムフォレストで解く - sambaiz-net
KaggleのTitanicのチュートリアルをXGBoostで解く - sambaiz-net
ランダムフォレスト Pythonのベイズ最適化のライブラリ、BayesianOptimizationを使う。
$ pip install bayesian-optimization  RandomForestClassifierのハイパーパラメータ
 n_estimators: 木の数 min_samples_split: ノードを分割するのに必要な最小サンプル数 max_features: 分割するときに考慮する特徴量の割合  の値を探すため、BayesianOptimizationに最大化したい値(精度)とパラメータの範囲を渡す。
from sklearn.model_selection import cross_val_score from sklearn.ensemble import RandomForestClassifier from bayes_opt import BayesianOptimization import pandas as pd def preprocess(df): df[&#39;Fare&#39;] = df[&#39;Fare&#39;].fillna(df[&#39;Fare&#39;].mean()) df[&#39;Age&#39;] = df[&#39;Age&#39;].fillna(df[&#39;Age&#39;].mean()) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].fillna(&#39;Unknown&#39;) df[&#39;Sex&#39;] = df[&#39;Sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].</description>
    </item>
    
    <item>
      <title>KaggleのTitanicのチュートリアルをXGBoostで解く</title>
      <link>https://www.sambaiz.net/article/168/</link>
      <pubDate>Sat, 02 Jun 2018 18:16:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/168/</guid>
      <description>XGBoostは高性能なGradient Boostingのライブラリ。 Boostingというのはアンサンブル学習の種類の一つで、ランダムフォレストのように弱学習器をそれぞれ並列に学習するBaggingに対して、 順番に前回までの結果を受けながら学習し、結果をまとめる際にそれぞれの重みを掛けるもの。 XGBoostではランダムフォレストと同様に決定木を弱学習器とする。
KaggleのTitanicのチュートリアルをランダムフォレストで解く - sambaiz-net
$ pip install xgboost  データの前処理はランダムフォレストと同じようにした。 パラメータの objective(目的関数)には二値分類なのでbinary:logisticを指定し、確率が返るのでroundして出力している。
import pandas as pd import xgboost as xgb from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score def preprocess(df): df[&#39;Fare&#39;] = df[&#39;Fare&#39;].fillna(df[&#39;Fare&#39;].mean()) df[&#39;Age&#39;] = df[&#39;Age&#39;].fillna(df[&#39;Age&#39;].mean()) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].fillna(&#39;Unknown&#39;) df[&#39;Sex&#39;] = df[&#39;Sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].map( {&#39;S&#39;: 0, &#39;C&#39;: 1, &#39;Q&#39;: 2, &#39;Unknown&#39;: 3} ).astype(int) df = df.drop([&#39;Cabin&#39;,&#39;Name&#39;,&#39;PassengerId&#39;,&#39;Ticket&#39;],axis=1) return df def train(df): train_x = df.</description>
    </item>
    
    <item>
      <title>KaggleのTitanicのチュートリアルをランダムフォレストで解く</title>
      <link>https://www.sambaiz.net/article/166/</link>
      <pubDate>Tue, 29 May 2018 09:33:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/166/</guid>
      <description>ランダムフォレストはデータや特徴量をランダムにサンプリングして決定木を複数生成し並列に学習するアンサンブル学習のBaggingという種類の手法。 決定木なので特徴量の影響が分かりやすく、単一の決定木と比べて過学習を防ぐことができる。
KaggleのTitanicのチュートリアルをXGBoostで解く - sambaiz-net
train.csvとtest.csvをKaggleからダウンロードする。 csvにはタイタニックの乗客者リストが含まれ、test.csvには生還したかを表すSurvivedが抜けている。 これを予測するのがこのコンペティションの目的だ。
データのうちFareやAge、Embarkedは入っていないものがあって、これらの欠損値をどう扱うという問題がある。
df = pd.read_csv(&#39;./train.csv&#39;) print(len(df)) print(df.isnull().sum())  891 PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64  連続値をとるFareとAgeは平均を取り、Embarkedは欠損値用の値にしてみた。数値化できないものについては除いている。
def preprocess(df): df[&#39;Fare&#39;] = df[&#39;Fare&#39;].fillna(df[&#39;Fare&#39;].mean()) df[&#39;Age&#39;] = df[&#39;Age&#39;].fillna(df[&#39;Age&#39;].mean()) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].fillna(&#39;Unknown&#39;) df[&#39;Sex&#39;] = df[&#39;Sex&#39;].apply(lambda x: 1 if x == &#39;male&#39; else 0) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].map( {&#39;S&#39;: 0, &#39;C&#39;: 1, &#39;Q&#39;: 2, &#39;Unknown&#39;: 3} ).</description>
    </item>
    
    <item>
      <title>Pythonのasyncioで非同期にリクエストを飛ばす</title>
      <link>https://www.sambaiz.net/article/162/</link>
      <pubDate>Sat, 14 Apr 2018 13:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/162/</guid>
      <description>Pythonのasyncioはイベントループを回してシングルスレッドで並行に非同期処理を行う。 マルチスレッドで並列に実行するのがthreadingで、 マルチプロセスで並列に実行するのがmultiprocessing。
import asyncio async def sleep(s): await asyncio.sleep(s) print(s) return s loop = asyncio.get_event_loop() loop.run_until_complete(sleep(5)) coros = [sleep(3), sleep(2)] futures = asyncio.gather(*coros) loop.run_until_complete(futures) print(futures.result()) loop.close()  $ python main.py 5 2 3 [3, 2]  get_event_loop() でイベントループを取得し、 gather()で処理をまとめたりして、 run_until_complete()で Futureの完了を待ち、 結果を取得してイベントループをclose()している。
async defを付けた関数はCoroutineとなり、 ensure_future()でFutureのサブクラスの、イベントループで実行させるTaskにすることができる。 run_until_complete()はそのままCoroutineを投げてもensure_future()でwrapしてくれる。
httpクライアントrequestsはBlockingするようなので、asyncioに対応しているaiohttpを使ってリクエストしてみる。
import aiohttp import asyncio import async_timeout async def fetch(session, url): print(&amp;quot;{} start&amp;quot;.format(url)) async with async_timeout.timeout(10): async with session.get(url) as response: text = await response.</description>
    </item>
    
    <item>
      <title>Kubernetes,Helmで負荷試験ツールLocustを立てる</title>
      <link>https://www.sambaiz.net/article/161/</link>
      <pubDate>Sun, 18 Mar 2018 22:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/161/</guid>
      <description>OSSの負荷試験ツールLocustをK8sクラスタに立てる。 K8sならworkerの増減も簡単だし、HelmのChartもあるので立てるのも楽。
LocustはPython製で、以下のようなコードで処理を書くことができる。
@task(10)のように括弧の中に数字を書いて実行される割合を偏らせることもできる。 異なるTaskSetに対応するユーザーを複数作ることもできて、こちらもweightで重みを付けられる。 ユーザー数はあとでWeb上から入力する。
$ mkdir tasks $ cat tasks/tasks.py from locust import HttpLocust, TaskSet, task class ElbTasks(TaskSet): @task def task1(self): with client.get(&amp;quot;/&amp;quot;, catch_response=True) as response: if response.content != &amp;quot;Success&amp;quot;: response.failure(&amp;quot;Got wrong response&amp;quot;) class ElbWarmer(HttpLocust): task_set = ElbTasks min_wait = 1000 max_wait = 3000  stableにChartはあるが、今のところtasksの中を書き換えなくてはいけないようなので、forkしてきてtasksの中を書き換え、helm repo addするためにpackageして、これを参照するindex.yamlを生成した。
$ helm package . $ helm repo index . $ ls locust-0.1.2.tgz index.yaml index.yaml	locust-0.1.2.tgz $ cat index.yaml apiVersion: v1 entries: locust: .</description>
    </item>
    
    <item>
      <title>Pythonのインタラクティブな可視化ライブラリBokehでグラフを描く</title>
      <link>https://www.sambaiz.net/article/129/</link>
      <pubDate>Sat, 26 Aug 2017 18:02:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/129/</guid>
      <description>Pythonの可視化というとmatplotlibや、 そのラッパーのseaborn、 データ解析ライブラリのPandasにもそういう機能があるけど、 これらが表示するのが静止画なのに対して、BokehはD3.jsで描画し、 拡大したりスクロールしたり、動的に何か表示することができる。Bokehはカメラのボケ。 似たようなのにPlotlyというのもあるけど、 こちらはPandasと同じpydata.orgドメインで、スターが多い。
jupyter/datascience-notebookイメージにもBokehがインストールされている。
$ docker run -d -p 8888:8888 jupyter/datascience-notebook start-notebook.sh  簡単なグラフを描く output_notebookでJupytor Notebokに出力する。ファイルに出力する場合はouput_fileを呼ぶ。
from bokeh.plotting import figure from bokeh.io import output_notebook, show output_notebook()  figure()でplotするFigureオブジェクトを作成する。
p = figure( title=&amp;quot;Hoge&amp;quot;, x_axis_label=&#39;x&#39;, y_axis_label=&#39;y&#39;, y_axis_type=&amp;quot;log&amp;quot; )  line()で線をつないでcircle()で円を描く。
x = [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0] y0 = [i**2 for i in x] y1 = [10**i for i in x] y2 = [10**(i**2) for i in x] p.</description>
    </item>
    
    <item>
      <title>PythonのLintとFormatter</title>
      <link>https://www.sambaiz.net/article/125/</link>
      <pubDate>Fri, 11 Aug 2017 14:57:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/125/</guid>
      <description>YAPF スタイルに沿って整形してくれる、Goでいうgo fmtみたいなもの。 デフォルトはPython公式のスタイルガイドPEP8でフォーマットされる。
$ pip install yapf  VSCodeでPythonを書くときは、 Pythonプラグイン を入れてこんな設定をWorkspaceのconfigに入れておいて、 保存した時にフォーマットがかかるようにすると快適。
&amp;quot;editor.formatOnSave&amp;quot;: true, &amp;quot;python.formatting.provider&amp;quot;: &amp;quot;yapf&amp;quot;  Lint YAPFでフォーマットされた以下のコードにLintをかける。
class FizzBuzz: def __init__(self, start=0): self.num = start def __iter__(self): return self def __next__(self): self.num += 1 if self.num % 15 == 0: return &amp;quot;FizzBuzz&amp;quot; if self.num % 3 == 0: return &amp;quot;Fizz&amp;quot; if self.num % 5 == 0: return &amp;quot;Buzz&amp;quot; return self.num if __name__ == &amp;quot;__main__&amp;quot;: fizzBuzz = FizzBuzz() for i in range(100): print(next(fizzBuzz))  Pylint PythonプラグインではデフォルトでPylintが使われる。</description>
    </item>
    
  </channel>
</rss>