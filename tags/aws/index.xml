<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>aws on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/aws/</link>
    <description>Recent content in aws on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Sun, 06 Dec 2020 18:25:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>CDKでCognito UserPoolとClientを作成しトリガーやFederationを設定する</title>
      <link>https://www.sambaiz.net/article/318/</link>
      <pubDate>Sun, 06 Dec 2020 18:25:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/318/</guid>
      <description>CloudFormationでCognito UserPoolを作成すると、以前はドメインやFederationの設定などを手作業で行う必要があったが、 去年の10月に諸々のリソースが追加され、その必要がなくなった。
API GatewayでCognitoの認証をかけて必要ならログイン画面に飛ばす処理をGoで書く - sambaiz-net
今回はCDKの高レベルAPIを用いて、UserPoolとClientを作成し、トリガーやGoogleのFederationの設定を行って、特定のGoogleアカウントでのみ登録されるようにする。 全体のコードはGitHubにある。
Cognito UserPoolのPreSignUp時に呼ばれるLambdaで登録ユーザーを制限する - sambaiz-net
UserPool 事前にdomainPrefixを決めておき、OAuthのclient_idとsecretをSecretsManagerに置いておく。
standardAttributesと、ログイン時にusernameの代わりとなるsignInAliasesは後から変更できない。 変更してデプロイしてもreplaceではなくエラーになってしまう。 トリガーのruntimeはCDKと同じNodeにしても良いがバージョンのライフサイクルが早く追従するのが大変なのでGoにしている。
private createUserPool(userPoolName: string, domainPrefix: string, signUpAllowEmails: string[]): UserPool { const userPool = new UserPool(this, &#39;UserPool&#39;, { userPoolName, standardAttributes: { email: { required: true, mutable: true }, fullname: { required: true, mutable: true }, }, signInAliases: { username: true, email: true }, lambdaTriggers: { preSignUp: new Function(this, &#39;PreSignUpFunction&#39;, { runtime: Runtime.GO_1_X, code: Code.fromAsset(path.join(__dirname, &#39;userPoolTrigger&#39;)), handler: &amp;quot;preSignUp.</description>
    </item>
    
    <item>
      <title>EKSにKubeflowをインストールする</title>
      <link>https://www.sambaiz.net/article/316/</link>
      <pubDate>Sun, 29 Nov 2020 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/316/</guid>
      <description>Kubernetes上で機械学習を行うためのツールキットKubeflowを EKSにインストールする。 m5.large * 4のクラスタをCDKで作成した。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
まずKubeflowのCLIツールkfctlをインストールする。 内部でeksctlが呼ばれるがフラグでprofileを指定できないので環境変数に入れておく。
$ curl -L https://github.com/kubeflow/kfctl/releases/download/v1.2.0/kfctl_v1.2.0-0-gbc038f9_darwin.tar.gz &amp;gt; kfctl.tar.gz $ tar -xvf kfctl.tar.gz $ ./kfctl version kfctl v1.2.0-0-gbc038f9 $ eksctl version 0.32.0 $ export AWS_PROFILE=*** AWS用の設定ファイルを持ってきて、RegionやRole、Cognito UserPoolあるいはusernameとpasswordによる認証の設定をKfAwsPluginに書く。UserPoolの作成もCDKで行える。それとACMで証明書を発行しておく。
CDKでCognito UserPoolとClientを作成しトリガーやFederationを設定する - sambaiz-net
AWSのサービスにアクセスするのにServiceAccountに関連づけられたRoleを用いる場合はenablePodIamPolicy: trueにして、ワーカーノードのRoleを用いる場合はrolesにそのロール名を書く。 KfDefにはManifestごとのKustomizeに関する設定が並んでいるがそのままで問題ない。
kustomizeでkubernetesのmanifestを環境ごとに生成する - sambaiz-net
$ mkdir &amp;lt;cluster_name&amp;gt; &amp;amp;&amp;amp; cd &amp;lt;cluster_name&amp;gt; $ curl -L https://raw.githubusercontent.com/kubeflow/manifests/v1.1-branch/kfdef/kfctl_aws_cognito.v1.2.0.yaml &amp;gt; kfctl_aws.yaml $ cat kfctl_aws.yaml apiVersion: kfdef.apps.kubeflow.org/v1 kind: KfDef metadata: namespace: kubeflow spec: applications: - kustomizeConfig: repoRef: name: manifests path: namespaces/base name: namespaces .</description>
    </item>
    
    <item>
      <title>GoでAthenaのクエリを実行する</title>
      <link>https://www.sambaiz.net/article/309/</link>
      <pubDate>Sat, 14 Nov 2020 16:19:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/309/</guid>
      <description>segmentio/go-athenaを使う。database/sqlのドライバーとして提供されていて、 StartQueryExecution()と stateのポーリング、 値のキャストを行う。
package main import ( &amp;quot;database/sql&amp;quot; &amp;quot;errors&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;github.com/aws/aws-sdk-go/aws/session&amp;quot; &amp;quot;github.com/aws/aws-sdk-go/service/sts&amp;quot; _ &amp;quot;github.com/segmentio/go-athena&amp;quot; ) func outputLocation() (string, error) { svc := sts.New(session.Must(session.NewSession())) result, err := svc.GetCallerIdentity(&amp;amp;sts.GetCallerIdentityInput{}) if err != nil { return &amp;quot;&amp;quot;, err } if result.Account == nil || svc.Config.Region == nil { return &amp;quot;&amp;quot;, errors.New(&amp;quot;account or region is nil&amp;quot;) } return fmt.Sprintf(&amp;quot;s3://aws-athena-query-results-%s-%s&amp;quot;, *result.Account, *svc.Config.Region), nil } func execute(query string) (*sql.Rows, error) { loc, err := outputLocation() if err !</description>
    </item>
    
    <item>
      <title>VSCodeのdevcontainerにSAM CLIをインストールしlocal invokeする</title>
      <link>https://www.sambaiz.net/article/301/</link>
      <pubDate>Mon, 12 Oct 2020 09:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/301/</guid>
      <description>VSCodeのdevcontainerにAWS SAM CLIをインストールしてDockerを用いたlocal invokeもできるようにする。
VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する - sambaiz-net
HomebrewとSAM CLIのインストール 手順に従ってbrewでインストールする。
 Homebrewがsudoとgit、psを必要とするのでインストールする デフォルトのrootでHomebrewをインストールするとDon&amp;rsquo;t run this as root!になるので non-root userを作って そのユーザーで実行する brew install aws-sam-cliでsamはインストールできているのにexit 1して失敗するのを握り潰している 次にdockerが必要となるので入れている  FROM debian:buster ARG USERNAME=vscode ARG USER_UID=1000 ARG USER_GID=$USER_UID # Create the user RUN groupadd --gid $USER_GID $USERNAME \ &amp;amp;&amp;amp; useradd --uid $USER_UID --gid $USER_GID -m $USERNAME \ # # [Optional] Add sudo support. Omit if you don&#39;t need to install software after connecting. &amp;amp;&amp;amp; apt-get update \ &amp;amp;&amp;amp; apt-get install -y sudo \ &amp;amp;&amp;amp; echo $USERNAME ALL=\(root\) NOPASSWD:ALL &amp;gt; /etc/sudoers.</description>
    </item>
    
    <item>
      <title>ElastiCacheでRedisクラスタを作成する際の設定</title>
      <link>https://www.sambaiz.net/article/303/</link>
      <pubDate>Fri, 09 Oct 2020 00:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/303/</guid>
      <description>CacheClusterとReplicationGroup コンソールでは意識することがないが、CloudFormationでElastiCacheクラスターを構築する場合は CacheClusterと ReplicationGroupという概念が存在する。 プライマリノードとリードレプリカノードは別のCacheClusterに作成され、それらをカプセル化したのがReplicationGroup。
Cluster Mode 有効にすると最大90までシャードを増やせるようになる。作成後にCluster Modeを有効にはできない。
シャードには1つのPrimary Nodeと0-5つのReplica Nodeが含まれる。 Replica NodeはPrimary NodeのデータをレプリケーションするRead Onlyなノード。手動でPrimary Nodeに昇格させたり、NodeやAZの障害時に自動で昇格したりする。
CloudFormationではシャード数はReplicationGroupのNumNodeGroupsで、レプリカノードの数はReplicasPerNodeGroupで設定できる。
リクエストはキーから計算されるハッシュスロットによって各シャードに分散され、 ClUSTER SLOTSコマンドでハッシュスロットとノードのマッピングが取れる。 go-redisのようなCluster対応クライアントは内部でこのマッピングと計算したハッシュスロットからリクエストを送るノードを決定している。
Node Size 少なくとも総アイテムサイズをシャード数で割った分がメモリに乗るようにする。 これに加えてスナップショットを作成する際にフォークしたプロセスで実行されるBGSAVEや、フェイルオーバーで使うメモリが必要。 reserved-memory-percentパラメータ(以前のバージョンではreserved-memoryだった)でmaxmemoryの25%を予約することが推奨されており、デフォルトは25になっている。
Subnet Group クラスタに設定するVPCのサブネットの集合。クラスタ作成後には変更できない。</description>
    </item>
    
    <item>
      <title>Kinesis Data AnalyticsのSQL, Lambdaへの出力とCDKによるリソースの作成</title>
      <link>https://www.sambaiz.net/article/302/</link>
      <pubDate>Sat, 03 Oct 2020 19:44:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/302/</guid>
      <description>Kinesis Data AnalyticsでStreaming SQLを実行し、 Lambdaに送る。ほかの接続先としてData StreamやFirehoseがあり、フォーマットはJSONとCSVから選べる。
Kinesis Streams/Firehose/Analyticsを試す - sambaiz-net
STREAMとPUMP (in-application) STREAMを作成し、PUMPで他のSTREAMをSELECTした結果をINSERTすることでデータを流していく。出力先を設定する際にSTREAMを選ぶ。 STREAMはRDBのテーブルのようにカラムを持ち、JOINもできる。
CREATE OR REPLACE STREAM &amp;quot;TEMPSTREAM&amp;quot; ( &amp;quot;column1&amp;quot; BIGINT NOT NULL, &amp;quot;column2&amp;quot; INTEGER, &amp;quot;column3&amp;quot; VARCHAR(64)); CREATE OR REPLACE PUMP &amp;quot;SAMPLEPUMP&amp;quot; AS INSERT INTO &amp;quot;TEMPSTREAM&amp;quot; (&amp;quot;column1&amp;quot;, &amp;quot;column2&amp;quot;, &amp;quot;column3&amp;quot;) SELECT STREAM inputcolumn1, inputcolumn2, inputcolumn3 FROM &amp;quot;INPUTSTREAM&amp;quot;; Windowed Queries  Tumbling Windows  固定の重複しない区間で集計するクエリ。次のように書くと60秒区切りで集計できる。
GROUP BY STEP(&amp;quot;SOURCE_SQL_STREAM_001&amp;quot;.ROWTIME BY INTERVAL &#39;60&#39; SECOND) ROWTIMEは 最初のSTREAMにデータが入った時のタイムスタンプが格納される特別なカラム。SELECTしなくても自動で渡される。
 Stagger Windows  固定の区間ではなく最初に対象パーティションのデータが届くとそこからINTERVALの区間のWindowが始まる。 Tumbling Windowsではデータに含まれるタイムスタンプで集計する場合、遅れて届くことで同じ区間のレコードが分かれてしまう問題があるがそれを緩和できる。</description>
    </item>
    
    <item>
      <title>EKS上のLocustから負荷をかける際のリソースの割り当てやインスタンスタイプの調整</title>
      <link>https://www.sambaiz.net/article/299/</link>
      <pubDate>Sun, 20 Sep 2020 16:39:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/299/</guid>
      <description>EKS上にLocustをインストールしたのだが、ユーザーを増やしてもRPSが大して伸びない。リソースを調整してなるべく効率的に負荷をかけられるようにする。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
前提 実行するシナリオは次のGETリクエストを送るだけのもの。Chartの都合で0.x系のlocustfileになっている。
from locust import HttpLocust, TaskSet, task class MyTaskSet(TaskSet): @task def index(self): self.client.get(&amp;quot;/&amp;quot;) class MyUser(HttpLocust): task_set = MyTaskSet min_wait = 5 max_wait = 15 ちなみに負荷をかける対象はECS+Fargateに立ち上げたAPIサーバーで、こちらが問題にならないよう余裕を持って動かしている。 スケールするからといってAPI Gatewayなどに向けるとリクエスト数による多額の課金が発生し得るので注意だ。
ECSでアプリケーションを動かすBoilerplateを作った - sambaiz-net
なお、ファイルディスクリプタの数は元から十分大きかったため特に変更していない。
ファイルディスクリプタの上限を増やす - sambaiz-net
$ kubectl exec tryeksstackclusterchartlocustchart1abdd876-worker-d5c7b85cbq6sh -- /bin/sh -c &amp;quot;ulimit -n&amp;quot; 1048576 ワーカー数とリソースの割り当て m5.large (2vCPU, メモリ10GiB)の2ノードに5ワーカーを立ち上げ負荷をかけたところ230RPSあたりで頭打ちになってしまった。
Container Insightsのメトリクスを見るとワーカーPodのCPUの使用率が100%に張り付いていることが分かる。
CloudWatch Container InsightsでEKSのメトリクスを取得する - sambaiz-net
ノードのCPUは40%ほどしかリクエストされておらず同量のlimitsがかかっているので、ワーカーのリクエストCPUを増やすか数を増やせば簡単にRPSを増やせそうだ。
まずはリクエストCPUを100mから500mに増やした。2.5倍ではないのは40%の中にはkube-systemやContainer InsightsのDaemonSetが含まれているため。リクエスト率は90%ほどになった。
$ kubectl describe nodes ... Non-terminated Pods: (9 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- amazon-cloudwatch cloudwatch-agent-sfz5l 200m (10%) 200m (10%) 200Mi (6%) 200Mi (6%) 5m49s amazon-cloudwatch fluentd-cloudwatch-5vnhg 100m (5%) 0 (0%) 200Mi (6%) 400Mi (13%) 5m49s default tryeksstackclusterchartlocustchart1abdd876-master-595c44cczwmcm 100m (5%) 100m (5%) 128Mi (4%) 128Mi (4%) 4m54s default tryeksstackclusterchartlocustchart1abdd876-worker-fddd54db4d7xr 500m (25%) 500m (25%) 128Mi (4%) 128Mi (4%) 4m54s default tryeksstackclusterchartlocustchart1abdd876-worker-fddd54dbz2kpj 500m (25%) 500m (25%) 128Mi (4%) 128Mi (4%) 4m54s kube-system aws-node-875rb 10m (0%) 0 (0%) 0 (0%) 0 (0%) 6m39s kube-system coredns-75b44cb5b4-7qpfl 100m (5%) 0 (0%) 70Mi (2%) 170Mi (5%) 4m54s kube-system coredns-75b44cb5b4-gc6mv 100m (5%) 0 (0%) 70Mi (2%) 170Mi (5%) 4m54s kube-system kube-proxy-wj8fk 100m (5%) 0 (0%) 0 (0%) 0 (0%) 6m39s worker: { replicaCount: 5, config: { configmapName: configmap.</description>
    </item>
    
    <item>
      <title>CloudWatch Container InsightsでEKSのメトリクスを取得する</title>
      <link>https://www.sambaiz.net/article/300/</link>
      <pubDate>Fri, 18 Sep 2020 19:58:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/300/</guid>
      <description>CloudWatch Container Insightsは EKS/EC2上で動くK8sクラスタおよびECSのメトリクスを取得する機能。今回はEKSで使う。
CDKでクラスタを作成する場合、ECSではcontainerInsightsをtrueにすることでセットアップされるが、EKSにはまだ存在しないため手動で行う。PRは上がっている。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
まずCloudWatchにログとメトリクスを送れるようにするため、ワーカーノードのIAMロールか、 PodのServiceAccountに関連づけられたIAMロールに CloudWatchAgentServerPolicyをアタッチする。今回はCDKで先にクラスタやロールを作るため前者の方法を取る。
cluster.defaultNodegroup?.role.addManagedPolicy(ManagedPolicy.fromAwsManagedPolicyName(&#39;CloudWatchAgentServerPolicy&#39;)) セットアップは次のコマンドの実行で完了し、amazon-cloudwatchネームスペースにCloudWatchメトリクスを送信するエージェントとFluentdのDaemonSetや、 各リソースを取得するClusterRoleやServiceAccountなどが作成される。cluster-nameとregion-nameの部分は書き換える。
$ curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed &amp;quot;s/{{cluster_name}}/cluster-name/;s/{{region_name}}/cluster-region/&amp;quot; | kubectl apply -f - $ kubectl get daemonset --namespace amazon-cloudwatch NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE cloudwatch-agent 2 2 2 2 2 &amp;lt;none&amp;gt; 60s fluentd-cloudwatch 2 2 2 2 2 &amp;lt;none&amp;gt; 57s メトリクスはCloudWatchのContainerInsightsネームスペースに送られる。 Podに割り当てられたCPUとメモリの使用率を出してみたところ、負荷をかけた際にCPUが100%に張り付いたので正しく送られていそうだ。</description>
    </item>
    
    <item>
      <title>CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う</title>
      <link>https://www.sambaiz.net/article/297/</link>
      <pubDate>Wed, 16 Sep 2020 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/297/</guid>
      <description>以前、Helmでlocustをインストールしたが、今回はEKSにCDKでインストールする。CDKだとクラスタの作成からできるのでcdk deployで一気に環境が整う。
KubernetesにHelmでLocustによる分散負荷試験環境を立てる - sambaiz-net
$ npm run cdk -- --version 1.62.0 (build 8c2d7fc) まずVPCとClusterを作成する。mastersRoleをユーザーもassumeできるようPrincipalにAWSアカウントも入れている。
AWSのAssumeRole - sambaiz-net
その後、実行するタスクを記述したスクリプトlocustfileのConfigMapを作成し、 これをChartのworker.config.configmapNameで参照する。キー名を間違えがち。 ChartのリポジトリはHelm Hubのもの。
import * as cdk from &#39;@aws-cdk/core&#39;; import { Cluster, KubernetesVersion, DefaultCapacityType } from &#39;@aws-cdk/aws-eks&#39; import { Vpc, SubnetType, InstanceType } from &#39;@aws-cdk/aws-ec2&#39; import { Role, ManagedPolicy, ServicePrincipal, AccountPrincipal, CompositePrincipal } from &#39;@aws-cdk/aws-iam&#39; import * as fs from &#39;fs&#39;; export class TryEksStack extends cdk.Stack { constructor(scope: cdk.Construct, id: string, props?</description>
    </item>
    
    <item>
      <title>AWS Organizaionsで複数のアカウントを一元管理する</title>
      <link>https://www.sambaiz.net/article/298/</link>
      <pubDate>Tue, 15 Sep 2020 23:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/298/</guid>
      <description>AWS Organizationsは複数のAWSアカウントを一元管理する機能。 既存のアカウントを一元管理できるだけではなく新規アカウントを簡単に作成することができるので、 セキュリティの向上やコストの分離、クォータなどアカウント全体に及ぶ影響の局所化などのためにサービスや環境ごとにアカウントを分けるハードルが下がる。 引き続きアカウントごとのコストも確認できるが、請求は一括で行われるので支払いの管理が楽になるし、ボリュームディスカウントの使用量が合算されRIも共有できるのでコストの上でも不利にならない。
また、アカウントやそれをグルーピングしたOrganizational Unit(OU)およびOrganizaion全体に対して、 サービスやアカウントを制御するService control policies (SCPs)などのポリシーを設定することもできる。
Organizationを作成したアカウントがマスターアカウントとなる。変更するにはOrganizationを削除する必要があるため管理専用のアカウントを作成するのがおすすめらしい。 通常、新規アカウントを作成するには住所や支払い情報などを入れる必要があるが、 Organizationだとその辺りが省略されアカウント名とルートアカウントのメールアドレスだけで済む。 管理に用いられるIAMロール名を入れる項目もあるが何も入れなければデフォルトのOrganizationAccountAccessRoleになる。
新アカウントのルートユーザーのパスワードは発行されないので必要なら再発行することになるが、 マスターアカウントのIAMユーザーにアカウント作成時に入力したロールのsts:AssumeRole権限を与えれば 新アカウントの方にユーザーを作らなくてもコンソール上部のメニューからスイッチできる。
AWSのAssumeRole - sambaiz-net
{ &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;, &amp;quot;Statement&amp;quot;: [ { &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;, &amp;quot;Action&amp;quot;: &amp;quot;sts:AssumeRole&amp;quot;, &amp;quot;Resource&amp;quot;: &amp;quot;arn:aws:iam::&amp;lt;new_account_id&amp;gt;:role/&amp;lt;role_name&amp;gt;&amp;quot; } ] } 基本的にマスターアカウントのリソースの権限は必要ないと思われるが、MFAの権限はないと設定できないので追加する。
初回はアカウントIDとロール名を入力することになる。一度入力すると履歴に残るのでそこから選べる。
参考 20180214 AWS Black Belt Online Seminar AWS Organizations</description>
    </item>
    
    <item>
      <title>VSCodeのDocker開発コンテナでJupyter Notebookを開いてAthenaのクエリを実行し可視化する</title>
      <link>https://www.sambaiz.net/article/294/</link>
      <pubDate>Fri, 04 Sep 2020 19:34:04 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/294/</guid>
      <description>ローカルでJupyter Notebookを動かすために以前はjupyter/datascience-notebookのイメージを立ち上げていた。 Notebookはエディタとしての機能に乏しいため通常のコードを書くのが大変だったのだが、 VSCodeのPython extensionにはJupyter notebookサポートが入っていてそのまま開けて実行できるのを知ったので移行することにした。 今回はVSCodeのDocker開発コンテナからNotebookを開いてAthenaのクエリを実行し可視化する。
VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する - sambaiz-net
環境構築 ~/.awsをマウントする。Dockerfileは上記の記事のと同じ。
{ &amp;quot;name&amp;quot;: &amp;quot;Python&amp;quot;, &amp;quot;build&amp;quot;: { &amp;quot;dockerfile&amp;quot;: &amp;quot;Dockerfile&amp;quot; }, &amp;quot;settings&amp;quot;: { &amp;quot;terminal.integrated.shell.linux&amp;quot;: &amp;quot;/bin/bash&amp;quot;, &amp;quot;python.pythonPath&amp;quot;: &amp;quot;/usr/local/bin/python&amp;quot;, &amp;quot;python.linting.enabled&amp;quot;: true, &amp;quot;python.linting.pylintEnabled&amp;quot;: false, &amp;quot;python.linting.flake8Enabled&amp;quot;: true, &amp;quot;python.linting.flake8Path&amp;quot;: &amp;quot;/usr/local/bin/flake8&amp;quot;, &amp;quot;editor.formatOnSave&amp;quot;: true, &amp;quot;python.formatting.provider&amp;quot;: &amp;quot;yapf&amp;quot;, &amp;quot;python.formatting.yapfPath&amp;quot;: &amp;quot;/usr/local/bin/yapf&amp;quot;, &amp;quot;python.linting.mypyEnabled&amp;quot;: true, &amp;quot;python.linting.mypyPath&amp;quot;: &amp;quot;/usr/local/bin/mypy&amp;quot;, }, &amp;quot;extensions&amp;quot;: [ &amp;quot;ms-python.python&amp;quot; ], &amp;quot;mounts&amp;quot;: [ &amp;quot;source=${localEnv:HOME}/.aws,target=/root/.aws,type=bind,readonly&amp;quot; ] } アプリケーションで使うパッケージはPoetryでインストールすることにしている。
PoetryでPythonの依存パッケージを管理する - sambaiz-net
KernalをvenvのPythonに切り替えるためにipykernelをインストールする必要がある。
[tool.poetry.dependencies] python = &amp;quot;^3.8&amp;quot; PyAthena = &amp;quot;^1.11.1&amp;quot; pandas = &amp;quot;^1.1.1&amp;quot; ipykernel = &amp;quot;^5.</description>
    </item>
    
    <item>
      <title>SageMakerでTensorFlowのモデルを学習させる</title>
      <link>https://www.sambaiz.net/article/293/</link>
      <pubDate>Mon, 10 Aug 2020 13:39:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/293/</guid>
      <description>以前PyTorchのモデルを学習させたが、そのTensorFlow版。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
コード 全体のコードはGitHubにある。
モデル モデルはTitanicのを使う。
TensorFlow2のKeras APIでTitanicのモデルを作る - sambaiz-net
make_csv_dataset()はbatch_sizeが必須になっているが、 これをそのままfilter()しようとすると、ValueError: predicate return type must be convertible to a scalar boolean tensor.になってしまうのでunbatch()している。 SageMakerのServing containerを用いる場合はsave_format=&amp;quot;tf&amp;quot;にしてSavedModel形式で保存する必要がある。
$ cat model.py import tensorflow as tf import logging class Model: def __init__(self, logger: logging.Logger, dropout: float): self.logger = logger self.model = tf.keras.Sequential([ tf.keras.layers.DenseFeatures(self._feature_columns()), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dropout(dropout), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dense(2, activation=tf.nn.sigmoid), ]) self.model.compile(optimizer=&#39;adam&#39;, loss=&#39;binary_crossentropy&#39;, metrics=[&#39;accuracy&#39;]) def _feature_columns(self): return [ tf.feature_column.numeric_column(&#39;age&#39;), tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(&#39;sex&#39;, 2)), tf.feature_column.numeric_column(&#39;fare&#39;) ] def _fill(self, feature: str, value): def __fill(x): if x[feature] == -1.</description>
    </item>
    
    <item>
      <title>SageMakerで学習したPyTorchのモデルをElastic Inferenceを有効にしてデプロイする</title>
      <link>https://www.sambaiz.net/article/290/</link>
      <pubDate>Sun, 26 Jul 2020 02:43:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/290/</guid>
      <description>学習させたモデルをSageMakerのホスティングサービスにデプロイする。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
推論時に呼ばれる関数 推論時には次の関数が呼ばれる。
 model_fn(model_dir): モデルをロードする input_fn(request_body, request_content_type): リクエストボディのデシリアライズ predict_fn(input_data, model): モデルで推論する output_fn(prediction, content_type): Content-Typeに応じたシリアライズ  input_fn() と output_fn() はJSON, CSV, NPYに対応した実装が、predict_fn() はモデルを呼び出す実装がデフォルトとして用意されていて、 model_fn() も後述するElastic Inferenceを使う場合model.ptというファイルをロードするデフォルト実装が使われる。 ただしその場合モデルがtorch.jit.save()でTorchScriptとして保存してある必要がある。
今回は predict_fn() のみ実装した。
$ cat inference.py import torch def predict_fn(input_data, model): device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;) model = model.to(device) input_data = input_data.to(device) model.eval() with torch.jit.optimized_execution(True, {&amp;quot;target_device&amp;quot;: &amp;quot;eia:0&amp;quot;}): output = model(input_data) return output.max(1)[1] デプロイ Training jobがモデルを保存したS3のパスを取ってきてPyTorchModelを作る。
from sagemaker.pytorch.model import PyTorchModel training_job_name = &#39;pytorch-training-2020-07-25-08-41-45-674&#39; training_job = sess.</description>
    </item>
    
    <item>
      <title>SageMakerでPyTorchのモデルを学習させる</title>
      <link>https://www.sambaiz.net/article/287/</link>
      <pubDate>Fri, 24 Jul 2020 22:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/287/</guid>
      <description>AWSの機械学習サービスSageMakerでPyTorchのモデルを学習させる。
コード まず学習させるモデルとそれを呼び出すエントリーポイントになるコードを書く。全体のコードはGitHubにある。 実際の環境と同じSageMakerのコンテナをローカルで動かしてVSCodeのRemote Developmentで接続して開発すると入っていないパッケージは警告が出たりして良い。
VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する - sambaiz-net
モデル 以前作ったMNISTのモデルを使う。
PyTorchでMNISTする - sambaiz-net
$ cat model.py import torch from torch import nn, cuda import torch.nn.functional as F import torch.distributed as dist import torch.optim as optim class Model(nn.Module): def __init__(self, dropout): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 64, 5) # -&amp;gt; 24x24 self.pool1 = nn.MaxPool2d(2) # -&amp;gt; 12x12 self.conv2 = nn.Conv2d(64, 128, 5) # -&amp;gt; 8x8 self.dropout = nn.Dropout(p=dropout) self.dense = nn.</description>
    </item>
    
    <item>
      <title>VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する</title>
      <link>https://www.sambaiz.net/article/289/</link>
      <pubDate>Sun, 19 Jul 2020 19:34:04 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/289/</guid>
      <description>SageMakerで学習させるモデルを開発するにあたって、Notebooks上ではコードを書きづらいのでVS Codeで書いているのだが、ローカルに依存パッケージをインストールして実行しているため エディタ上では警告が出ていなくても、実際の環境にはパッケージがなかったりすることがある。
そんな場合に便利なのがVS CodeのRemote Development。 これはローカルのVS CodeからリモートのVS Code Serverに接続してその環境で開発することができるエクステンションで、 Dockerコンテナのほか、SSHでリモートマシンやVMに接続したり、WindowsならWSLにも接続して開発環境を揃えることができる。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
設定 .devcontainer/に次のファイルを置く。
PyTorch  Dockerfile  aws/deep-learning-containersの Deep Learning Containers Imagesから選び、ECRからpullするため認証情報を登録しておく。
$ aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin https://763104351884.dkr.ecr.us-east-1.amazonaws.com これをベースに、開発用ツールを入れてVSCodeのDevelopment Container Scriptsを実行する。
$ cat .devcontainer/Dockerfile FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.1-cpu-py36-ubuntu16.04 RUN conda install -y yapf flake8 mypy # VS Code Development Container Scripts # https://github.com/microsoft/vscode-dev-containers/tree/v0.128.0/script-library ARG INSTALL_ZSH=&amp;quot;true&amp;quot; ARG USERNAME=&amp;quot;vscode&amp;quot; ARG USER_UID=&amp;quot;1000&amp;quot; ARG USER_GID=&amp;quot;${USER_UID}&amp;quot; ARG UPGRADE_PACKAGES=&amp;quot;true&amp;quot; ARG COMMON_SCRIPT_SOURCE=&amp;quot;https://raw.</description>
    </item>
    
    <item>
      <title>VPCエンドポイント</title>
      <link>https://www.sambaiz.net/article/274/</link>
      <pubDate>Sat, 23 May 2020 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/274/</guid>
      <description>VPCエンドポイントは PrivateLink対応のサービスおよび、S3やDynamoとAWSネットワーク内で接続するためのエンドポイント。 インターネットに出ない分セキュアでゲートウェイへの負荷も抑えられる。 料金は時間あたりとトラフィック量による。
VPCエンドポイントを使うためにアプリケーション側に手を入れる必要はなく、 S3とDynamoがサポートしているGatewayのエンドポイントではルートテーブルによって、 その他多くのサービスがサポートしているInterfaceのエンドポイントでは名前解決の時点で向き先が変わるようになっている。
まずVPCのDNS ResolutionとDNS HostnamesをtrueにしてPrivate DNSで名前解決されるようにしておく必要がある。 エンドポイントを作成する際の設定項目は対象サービスと、VPCとSubnet、SGとサービスによってはPolicy。 サービスと1:1対応しているわけではなく、例えばECSの場合は次の3つのエンドポイントが必要。
com.amazonaws.region.ecs-agent com.amazonaws.region.ecs-telemetry com.amazonaws.region.ecs 作成するとSubnet内にエンドポイントとそれに紐づくENIが立ち、インターネットゲートウェイなしでもAPIが叩けるようになった。</description>
    </item>
    
    <item>
      <title>DAX (DynamoDB Accelerator)の特性と挙動確認</title>
      <link>https://www.sambaiz.net/article/260/</link>
      <pubDate>Wed, 26 Feb 2020 23:21:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/260/</guid>
      <description>DAXとは DAXはDynamoDBの前段に置かれるマネージドなインメモリキャッシュで、 Read速度の向上(数ms-&amp;gt;数百μs)とテーブルのRead Capacityの節約に効果がある。
DynamoDBとSDKのAPIの互換性があるため置き換えるだけで使えるようになっている。 クライアントの実装としてはHTTPではない独自のプロトコルで通信している点が異なる。
クラスタ作成時に指定するのはノード数とインスタンスタイプで、 ノード数はスループットに、インスタンスタイプはスループットとメモリ量(キャッシュヒット率)に影響する。 複数のノードがある場合、一つがWriteするプライマリーノードになり、他はリードレプリカになる。 なのでノード数を増やしてもWriteのスループットは上がらない。プライマリーノードに問題が発生したら自動でフェイルオーバーする。 ノードは最大10個まで増減できるが、インスタンスタイプは変更できない。最大10個というのは足りるのかと思ったが、数百万RPS捌けるようなので十分そうだ。
インスタンスに対して時間課金が発生し、可用性のために3ノード以上にすることが推奨されている。 そのため、リクエストがそれほどなかったり、キャッシュミスばかりだとインスタンス代の方が高くつくこともあるが、 そこそこReadするなら目に見えてコスト削減されるはずだ。ただしどれくらい次の整合性を許容できるかによる。
キャッシュの整合性 DAXはDynamoDBとは異なり、結果整合性のある読み込み(Eventually Consistent Reads)のみをサポートしているので、プライマリノードにキャッシュされ、全てのノードにレプリケーションが完了するまでの間は異なる結果を返す可能性がある。また、DAXからDynamoDBへのリクエストも結果整合性のある読み込みで行われる。
リクエストの結果は、Itemがなかった場合のネガティブキャッシュも含めて、 Item Cache(GetItem,BatchGetItem)とQuery Cache(Query,Scan)にキャッシュされ、 それぞれパラメータグループで設定されたTTLが過ぎるか、LRUアルゴリズムによって破棄される。 TTLのデフォルトは5分。
書き込みリクエストが来るとまずDynamoDBに書き込んで成功したことを確認してからキャッシュしてレスポンスを返す。 この際Item Cacheは更新されるが、Query Cacheは更新されずTTL/LRUによって破棄されるまで同じ値を返し続けてしまう。 もし問題がある場合は、TTLを短くするかDynamoDBを直接見に行くことになるが、そうするとDAXの効果が薄れてしまう。 また、大量のデータを書き込むと、その分レイテンシが増加したり、それらがすべてキャッシュに乗ることで既存のものがLRUで追い出されてキャッシュヒット率が悪くなることがあり、それらを回避するため直接書き込むという選択肢もあるが、Item Cacheが更新されないことを許容する必要がある。
取れるメトリクス CPU使用率や、キャッシュヒット数、各リクエスト数や接続数が取れる。
ノードごとのCPU使用率も取れる。Writeやキャッシュミスによるプライマリーノードの負荷の高まりに注意。 レイテンシが大きくなり接続数が増える悪循環に陥る。
プライマリーノードのCPU使用率が80%程度でテーブルのキャパシティに余裕があってもリクエストがスロットリングされることがあり、一つ上のインスタンスタイプでクラスタを作り直したところ解消した。 この際、いきなり全リクエストが新クラスタに送られることで膨大なキャッシュミスが発生し、 テーブルのキャパシティを超過したりプライマリーノードに負荷が集中しないように、 Route53で割合で解決されるようにしたが、これはうまく流れてくれた。
挙動確認 DAX/DynamoへリクエストするだけだったらLambdaでも良いが、負荷をかけるのでECS上にアプリケーションをデプロイすることにした。 以前作ったBoilerplateベースで、コードはここ。
ECSでアプリケーションを動かすBoilerplateを作った - sambaiz-net
TableとDAXのClusterを作成。
createDynamoTable() { return new dynamodb.Table(this, &#39;DynamoTable&#39;, { tableName: &amp;quot;dax-test-table&amp;quot;, partitionKey: { name: &#39;id&#39;, type: dynamodb.AttributeType.STRING }, readCapacity: 50, writeCapacity: 50, }); } createDaxCluster(subnetIds: string[]) { const subnetGroup = new dax.</description>
    </item>
    
    <item>
      <title>ECSでアプリケーションを動かすBoilerplateを作った</title>
      <link>https://www.sambaiz.net/article/259/</link>
      <pubDate>Mon, 24 Feb 2020 16:17:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/259/</guid>
      <description>https://github.com/sambaiz/ecs-boilerplate
ECS上でアプリケーションを動かすBoilerplateを作った。CDKでデプロイする。以前Digdagを動かしたときのを汎用的にしたもの。
CDKでECS+Fargate上にDigdagを立ててCognito認証を挟む - sambaiz-net
new ECSStack(app, &#39;ECSBoilerplateSampleStack&#39;, { /* // If vpcAttributes is not specified, new VPC is created. vpcAttributes: { vpcId: &#39;&#39;, availabilityZones: [], publicSubnetIds: [], privateSubnetIds: [], }, // DNS record. Even if this is not specified, you can access with ELB domain (***.elb.amazonaws.com) route53: { zoneId: &#39;&#39;, zoneName: &#39;example.com&#39;, recordName: &#39;foo&#39;, }, // Certificate Manager ARN. Required if accessing with HTTPS acmArn: &#39;arn:aws:acm:****&#39; // default values containerPort: 8080, cpu: 256, memoryLimitMiB: 512, minCapacity: 1, maxCapacity: 5, scaleCPUPercent: 80 */ }); CDKがECRへのpushまでやってくれるのでcdk deployすれば動き始め、削除するときもStackを消せばよい。</description>
    </item>
    
    <item>
      <title>LA,ディズニーランドからre:Inventに参加しグランドキャニオンへドライブしてきた</title>
      <link>https://www.sambaiz.net/article/250/</link>
      <pubDate>Sun, 22 Dec 2019 23:45:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/250/</guid>
      <description>一昨年、去年と参加したGoogle I/Oのチケットが今年は当たらなかったので、前から行ってみたかったAWSのre:Inventに参加することにした。 会場はラスベガスで、会期は12/2-6。前後の土日ともう2日つなげて現地時間10日間の旅程にした。 去年、カナダのバンクーバーから西海岸のシアトル、ポートランド、サンフランシスコは巡ったので、今回はまだ訪れていないロサンゼルスからスタートすることにした。会期後はグランドキャニオンまで足を伸ばす。
準備 例年通りExpediaで航空券と宿を取り、加えてラスベガスからグランドキャニオンへ行くためにレンタカーを予約した。 I/O会期中のシリコンバレー近辺とは異なり、ラスベガスのホテルは本当に安くて、OYO(元フーターズ)が一泊2千円で取れた。 re:Inventは65000人規模のイベントなんだが現地のUberドライバー曰く、最大20万人規模のイベントもやったりするらしいのでキャパシティは十分そうだ。 多くの人が申し込むJTBのツアーはホテルのグレードを考えてもかなり割高に感じた。
LAでは北のハリウッド、南のアナハイム、ディズニーランドまで行くことを考えてダウンタウンに宿を取った。 価格だけで選ぶとスキッド・ロウといった危ないエリアの近くになりかねないので注意が必要だ。 ディズニーのチケットも事前に購入した。2パークいけるパークホッパーチケットに、色々な特典を含むMaxPassを付けて$214。 高い日のPeak料金ではあるんだが、家族連れで来たら大変じゃないかと思う。
ラスベガスといえばカジノとショーの街だというし、シルク・ドゥ・ソレイユ Oのチケットを買った。 チケットを印刷するのを忘れていたので現地のFedex Officeで印刷した。ファイルを添付してメールを送るとコードが送られてくるので、それを印刷機に入力するだけで簡単。
アメリカでの運転は初めてで往復できるか不安だったので、グランドキャニオンの南、フラッグスタッフという町の空港で乗り捨てられるAlamoで予約した。 日本でも5年は走っていないペーパードライバーなので、2時間出張教習してもらい、後はタイムズのカーシェアで練習した。 それと免許センターで国際免許を発行した。特に試験とかはなくて手数料だけ払えばもらえる。警察署でも発行してくれるようだが即日発行ではないようだ。
今回のSIMはこれ。
Amazon.co.jp： 【AT&amp;amp;T】ハワイ・アメリカ本土 プリペイドSIM 30日 データ容量8GB 大容量通話付き: 家電・カメラ
回線はAT&amp;amp;Tで、30日、8GB、テザリング可で電話もできる(香港の番号だが国際通話ができる)と、申し分ないスペックに対して安すぎるのが若干不安だったが、 ドキュメント通り現地でSIMを挿してAPNを作成したらすぐにつながり、その後も全く問題なかった。すごい。
LA/ディズニーランド 行きの航空券が関空乗り継ぎだった。国内線スタートの良いところは搭乗時間の締め切りが国際線よりはるかに緩いことで、実際それに救われた。
10時間ほどのフライトでLAXに到着。Lax-itというライドシェア用の乗り場を目指す。 国際線ターミナルからは逆のところにあるので、ひっきりなしに走っているシャトルバスに乗る。 Uberをこの辺りに呼ぶとLax-it内のポート番号が表示される仕様だ。
ホテルにチェックイン後、Cole&amp;rsquo;sという店のDip sandwitchを食べに行く。サンドイッチを肉汁のスープに浸して食べる。 カフェみたいなのをイメージしていったら酒場で緊張した。
地下鉄でハリウッドに移動。運賃はTapというカードにチャージする仕組み。 持ってなかったら$2くらい余分に払うと自販機から出てくる。距離に関係なく同一運賃。 このカードでMetroのバスにも乗れるが、バスではチャージできないそうなので乗る場合は少し余分に入れておく。 ただ、結局バスは乗らなかった。ディズニーランド行きのバスもあっておそらく最安なんだが、 Localなのでとても時間がかかるし、サウス・ロサンゼルスというこれまた治安悪いエリアを突っ切るのが怖かったからだ。
ハリウッドではWalk of Fameの有名人の名前プレートを見ながら散歩していた。途中ハリウッドサインが見えたが想像していたより遠い。
せっかくなのでビバリーヒルズのロデオドライブにも行ってきた。表参道みたいな感じであまり用がなかった。
次の日はディズニーランドに行く前にGrand Central MarketのEggslutで朝食を取った。8時開店で8時半には着いたんだが、既に20人くらいの行列ができている人気店だ。 よく分からなかったのでslutというのを注文した。食べるまで気づかなかったんだが星野珈琲のモーニングのあれだ。美味しい。
Uberでディズニーランドに向かう。そういえばアプリ入れてないなと思ってPlayストアで調べたがひっかからない。もしやと思って調べてみると
は？？しょうがないのでアカウントの地域をアメリカに切り替えて入れた。1年間変更できず、日本向けのアプリをダウンロードできなくなった。納得いかない。 ともあれ、これでアプリからファストパスが取れるようになった。ファストパスは30分に1回取れるが、既に持ってるものを消費しなくてはいけない。 人気のアトラクションはかなり先の時間になってしまうか取れなくなってしまうので、 東京と同様のアトラクションが結構あるディズニーランドパークよりカリフォルニアアドベンチャーの方を先に回った方が良さそう。 ただ、パークの方でもスターウォーズのエリアはかなり作り込んであって新鮮だし、写真を撮ってくれる人もいるので暗くなる前に行くべきかもしれない。 撮ってもらった写真はMaxPassならアプリからダウンロードできる。一人って言ったら、&amp;ldquo;Oh, Solo&amp;quot;って言われたのにうまく反応できなくて悔しい。
カリフォルニアアドベンチャーの方はカーズのアトラクションが一番人気で、ファストパスを取るならそれが最有力だと思う。 スピード感がありながらフワッとする感じはなくて楽しかった。 撮影ポイントがあって出たところのモニターにアプリに入力するコードが表示されているんだが、 切り替わるのが早すぎるので、カメラで撮るなりして一旦コードだけ控えておいたほうが良い。 あとミッキーの顔が書かれた観覧車にも乗った。Swingするのに乗ったら、それこそバイキングかってぐらい揺らしてきて 泣きそうになった。実際泣き出す子もいると思う。
パークの城が東京で見るのと違うなと思ったら、こっちの城はシンデレラ城じゃなくて眠れる森の美女の城らしい。
行きのUberは$40くらいだったのに対して帰りは$60くらいかかった。需要と供給だ。
ラスベガス/re:Invent re:Invent前日にLAXからLASへ。 もう降りたところからスロットマシンが置いてあってさすがカジノの街だ。ここのUber乗り場はパーキングにある。</description>
    </item>
    
    <item>
      <title>ECS(EC2)のCloudFormation最小構成</title>
      <link>https://www.sambaiz.net/article/247/</link>
      <pubDate>Fri, 15 Nov 2019 20:12:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/247/</guid>
      <description>EC2でECSのServiceを動かすCFnテンプレートを書く。以前Fargateで動かしたものを一部再利用する。
ECS FargateでSidecarのFluentdでログをS3に送る構成をCloudFormationで構築する - sambaiz-net
EC2で動かす場合、自分でリソースが不足しないようにインスタンスのスケールを気遣うことになるが、VPC外での実行やprivilegedをtrueにするなどEC2でしかできないことがある。あと同リソースで比較すると安い。
まずはEC2インスタンス以外のリソースを書く。LaunchType以外はFargateのときとほぼ同じ。 LBなしでバッチのようなものを動かすことを想定した最小構成。
ECSCluster: Type: AWS::ECS::Cluster Properties: ClusterName: &#39;test-cluster&#39; LogGroup: Type: AWS::Logs::LogGroup Properties: LogGroupName: &#39;test-task-log-group&#39; RetentionInDays: 1 TaskDefinition: Type: AWS::ECS::TaskDefinition Properties: RequiresCompatibilities: - EC2 Cpu: &#39;256&#39; Memory: &#39;512&#39; ContainerDefinitions: - Name: &#39;app&#39; Image: &#39;busybox&#39; EntryPoint: - &#39;sh&#39; - &#39;-c&#39; Command: - &#39;while true; do echo &amp;quot;{\&amp;quot;foo\&amp;quot;:1000,\&amp;quot;time\&amp;quot;:\&amp;quot;2019-05-09T20:00:00+09:00\&amp;quot;}&amp;quot;; sleep 10; done&#39; Essential: &#39;true&#39; LogConfiguration: LogDriver: &#39;awslogs&#39; Options: awslogs-group: !Ref LogGroup awslogs-region: &#39;ap-northeast-1&#39; awslogs-stream-prefix: &#39;app&#39; Environment: - Name: &#39;TZ&#39; Value: &#39;Asia/Tokyo&#39; Volumes: - Name: &#39;varlog&#39; ECSService: Type: AWS::ECS::Service Properties: Cluster: !</description>
    </item>
    
    <item>
      <title>PR上でCDKのレビューやデプロイを行うツールcdkbotを作った</title>
      <link>https://www.sambaiz.net/article/235/</link>
      <pubDate>Thu, 29 Aug 2019 22:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/235/</guid>
      <description>sambaiz/cdkbot
PRのコメントで/diffや/deployと打つとcdk diffやcdk deployが走る。 diffを見てレビューし、良ければ/deployでデプロイし完了するとmergeされる。
以前CircleCIでmerge時にdeployされる仕組みを作った。
CDK/CircleCI/GitHubでAWSリソース管理リポジトリを作る - sambaiz-net
ただ、この仕組みだと CFnの実行時エラーのためにデプロイできない状態のものがmasterブランチにmergeされてしまい、その修正のために何回も試行錯誤のPRを出すことになったり、 Stack間の依存がある場合リソースを削除するとcdk deployによって依存解決された順序だと失敗してしまうという問題があった。 cdkbotでは必要ならデプロイするStackを選べて、完了してからmergeすることでこれらの問題を解決した。 また、AWS外のCIにとても強い権限を与えていたがそれも必要なくなった。
単純にブランチの状態でデプロイしてしまうと古い状態に巻き戻ってしまう可能性があるので、内部でbaseブランチをmergeしていたり、 ラベルによってそのPRがデプロイ可能かどうかを制御していたりする。 最低限デプロイできるようになってから、この辺りの仕組みを整えるまでに存外に時間がかかった。
Serverless Application Repositoryに公開してあるので簡単にインストールできる。
AWS SAMでLambdaの関数をデプロイしServerless Application Repositoryに公開する - sambaiz-net
 追記 (2019-10-26): ap-northeast-1に対応していないのと、ECSのリソースを作成できないため、Serverless Application Repositoryに公開するのはやめた。makeでインストールできる。 Lambda環境でできない処理をECSで実行する - sambaiz-net
 外部コマンド gitやnpmといった外部コマンドを実行する必要があるが、標準では入っていないのでLambda Layerで入れている。
Lambda上でnpm installできるLayerを作った - sambaiz-net
Go moduleのキャッシュ Dockerコンテナ内でテストを実行しているが、毎回go moduleの解決が走ることで時間はかかるし、テザリングの容量に大打撃を受けたので、 ローカルのキャッシュをコピーするようにした。
test: docker build -t cdkbot-npmbin ./npm-lambda-layer docker build -t cdkbot-test -f ./test/Dockerfile . docker rm -f cdkbot-test || true docker run -itd --name cdkbot-test cdkbot-test /bin/sh docker cp .</description>
    </item>
    
    <item>
      <title>CDKでECS&#43;Fargate上にDigdagを立ててCognito認証を挟む</title>
      <link>https://www.sambaiz.net/article/234/</link>
      <pubDate>Wed, 31 Jul 2019 03:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/234/</guid>
      <description>AWSでワークフローエンジンDigdagを立てるにあたりスケールを見越してECS+Fargateで動かす。 全体のコードはGitHubにある。
FargateでECSを使う - sambaiz-net
リソースはCDKで作る。最近GAになったので高レベルのクラスを積極的に使っている。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
$ npm run cdk -- --version 1.2.0 (build 6b763b7) VPC FargateなのでVPCが必要。 テンプレートを書くとSubnetやRouteTable、NATGatewayなど記述量が多くなるところだが、CDKだとこれだけで済む。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
const vpc = new ec2.Vpc(this, &#39;VPC&#39;, { cidr: props.vpcCidr, natGateways: 1, maxAzs: 2, subnetConfiguration: [ { name: &#39;digdag-public&#39;, subnetType: ec2.SubnetType.PUBLIC, }, { name: &#39;digdag-private&#39;, subnetType: ec2.SubnetType.PRIVATE, }, { name: &#39;digdag-db&#39;, subnetType: ec2.SubnetType.ISOLATED, } ] }) DB DigdagはPostgreSQLを使う。
const db = new rds.DatabaseCluster(this, &#39;DBCluster&#39;, { engine: rds.</description>
    </item>
    
    <item>
      <title>Lambda上でnpm installできるLayerを作った</title>
      <link>https://www.sambaiz.net/article/233/</link>
      <pubDate>Tue, 23 Jul 2019 23:44:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/233/</guid>
      <description>Lambda上でnpm installするためにnpmとnode, npmrc入りのLambda Layerを作った。 GitHubにある。
Lambda Layerでバイナリやライブラリを切り出す - sambaiz-net
まずは/usr/bin/npmをそのまま入れて実行してみた。
FROM lambci/lambda-base:build WORKDIR /opt RUN curl -sL https://rpm.nodesource.com/setup_12.x | bash - &amp;amp;&amp;amp; \ yum install -y nodejs &amp;amp;&amp;amp; \ mkdir bin &amp;amp;&amp;amp; \ cp /usr/bin/node bin/node &amp;amp;&amp;amp; \ cp /usr/bin/npm bin/ &amp;amp;&amp;amp; \ zip -yr /tmp/npm-layer.zip ./* $ docker build -t npmbin . $ docker run npmbin cat /tmp/npm-layer.zip &amp;gt; npm-layer.zip &amp;amp;&amp;amp; unzip npm-layer.zip -d layer 相対パスでの参照に失敗したようだが対象のパスが見当たらない。
internal/modules/cjs/loader.js:628 throw err; ^ Error: Cannot find module &#39;.</description>
    </item>
    
    <item>
      <title>Lambda Layerでバイナリやライブラリを切り出す</title>
      <link>https://www.sambaiz.net/article/232/</link>
      <pubDate>Mon, 22 Jul 2019 21:09:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/232/</guid>
      <description>Lambdaで実行したい外部コマンドがある場合、通常バイナリをパッケージに含めることになりデプロイに時間がかかってしまう。
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;os/exec&amp;quot; &amp;quot;github.com/aws/aws-lambda-go/events&amp;quot; &amp;quot;github.com/aws/aws-lambda-go/lambda&amp;quot; ) func handler(request events.APIGatewayProxyRequest) (events.APIGatewayProxyResponse, error) { cmd := exec.Command(&amp;quot;git&amp;quot;, &amp;quot;clone&amp;quot;, &amp;quot;https://github.com/sambaiz/foobar.git&amp;quot;, &amp;quot;/tmp/repo&amp;quot;) output, err := cmd.CombinedOutput() if err != nil { return events.APIGatewayProxyResponse{ Body: fmt.Sprintf(&amp;quot;%s %s&amp;quot;, string(output), err.Error()), StatusCode: 500, }, nil } return events.APIGatewayProxyResponse{ Body: string(output), StatusCode: 200, }, nil } func main() { lambda.Start(handler) } exec: &amp;quot;git&amp;quot;: executable file not found in $PATH Lambda Layerを使うと ライブラリやバイナリを切り出すことができ、複数Functionで共有することもできる。 ディレクトリをzipにしてLayerに指定すると中身が/optに展開され、/opt/binにはPATHが、/opt/libにはLD_LIBRARY_PATHが通るほか、 言語ごとのパッケージ置き場がある。</description>
    </item>
    
    <item>
      <title>AWS SAMとGoでPRのコメントに対して返事を返すGitHub Appを作る</title>
      <link>https://www.sambaiz.net/article/231/</link>
      <pubDate>Fri, 19 Jul 2019 21:21:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/231/</guid>
      <description>GitHub Appはリポジトリにインストールできるアプリケーションで、 Access TokenやOAuth Appと異なり ユーザーとは独立した権限を与えて実行することができる。
今回はPRの特定のコメントに反応して返事を返すAppを作る。
AWS SAMでデプロイする。全体のコードはGitHubにある。
AWS SAMでLambdaの関数をデプロイしServerless Application Repositoryに公開する - sambaiz-net
GitHub Appの作成 Settings &amp;gt; Developer settings から作成できる。いろいろ項目はあるが、NameとHomepage URL、Webhook URLを入れればひとまず作成はできる。 Webhook URLはあとで決まるので適当な値を入れておく。必須にはなっていないがリクエストを検証するため適当なWebhook secretも入れる。 PermissionsはPull requestsではなくIssueのRead &amp;amp; Writeが必要で、 さらにそうすると表示されるようになるSubscribe to eventsのIssue commentにチェックを入れる。
作成すると秘密鍵がダウンロードできるのでSecretsmanagerに上げておき、LambdaのRoleにもこれを取得できるRoleを付ける。
AWS Systems Manager (SSM)のParameter Storeに認証情報を置き参照する - sambaiz-net
$ aws secretsmanager create-secret --name GitHubCdkAppSecretKey --secret-string $(cat private-key.pem) Policies: - PolicyName: read-cdk-github-app-secret-key PolicyDocument: Version: &amp;quot;2012-10-17&amp;quot; Statement: - Effect: Allow Action: secretsmanager:GetSecretValue Resource: &amp;lt;secret-key arn&amp;gt; Webhooksのリクエスト内容 設定でチェックを入れたeventが起きると次のようなリクエストが送られてくる。 Headerの X-GitHub-Event によってbodyの中身が決まり、 X-Hub-Signature と、bodyと設定したWebhook secretから生成したHMACのMAC値を比較することで リクエストを検証することができる。</description>
    </item>
    
    <item>
      <title>Cognito UserPoolのPreSignUp時に呼ばれるLambdaで登録ユーザーを制限する</title>
      <link>https://www.sambaiz.net/article/228/</link>
      <pubDate>Sun, 07 Jul 2019 17:10:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/228/</guid>
      <description>サードパーティのIdPからCognitoにSignUpできるようにする場合、特定のドメインのメールアドレスといったような制限をかけたいことがある。 PreSignUp時のLambdaでこれを弾いてやることでUserPoolに入らないようにすることができる。
Lambda CognitoEventUserPoolsPreSignupを受け取って返す。
package main import ( &amp;quot;errors&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;github.com/aws/aws-lambda-go/events&amp;quot; &amp;quot;github.com/aws/aws-lambda-go/lambda&amp;quot; ) func handler(event events.CognitoEventUserPoolsPreSignup) (events.CognitoEventUserPoolsPreSignup, error) { fmt.Printf(&amp;quot;PreSignup of user: %s\n&amp;quot;, event.UserName) if event.Request.UserAttributes[&amp;quot;email&amp;quot;] != &amp;quot;godgourd@gmail.com&amp;quot; { return event, errors.New(&amp;quot;Forbidden&amp;quot;) } return event, nil } func main() { lambda.Start(handler) } リソース UserPoolのLambdaConfigでトリガーを設定できる。 CognitoからLambdaを呼べるPermissionが必要。
UserPool: Type: AWS::Cognito::UserPool Properties: ... LambdaConfig: PreSignUp: !GetAtt PresignupLambdaFunction.Arn UserPoolLambdaInvokePermission: Type: AWS::Lambda::Permission Properties: Action: lambda:invokeFunction Principal: cognito-idp.amazonaws.com FunctionName: !GetAtt PresignupLambdaFunction.Arn SourceArn: arn:aws:cognito-idp:&amp;lt;region&amp;gt;:&amp;lt;account_id&amp;gt;:userpool/* なおALBのActionでCognito認証を入れると登録失敗時に500エラーになってしまう。これを回避するには自前でやるしかないのかもしれない。
LambdaとALBでCognito認証をかけて失敗したらログイン画面に飛ばす - sambaiz-net</description>
    </item>
    
    <item>
      <title>LambdaとALBでCognito認証をかけて失敗したらログイン画面に飛ばす</title>
      <link>https://www.sambaiz.net/article/227/</link>
      <pubDate>Wed, 03 Jul 2019 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/227/</guid>
      <description>ALBのTargetとしてLambdaが選択できるようになり、 若干の時間課金が発生する代わりに柔軟にルーティングできるAPI Gatewayのように使えるようになった。 ActionとしてCognito認証を入れて認証に失敗したらログイン画面を表示させる。
API GatewayでCognitoの認証をかけて必要ならログイン画面に飛ばす処理をGoで書く - sambaiz-net
ACMで証明書を発行する HTTPSでListenするため証明書が必要。 AWS Certificate Manager (ACM)でAWSで使える証明書を無料で発行でき参照できる。 外部で取ったドメインでもよい。 検証方法はDNSとメールとで選ぶことができて、DNSで行う場合Route53ならワンクリックで検証用のCNAMEレコードを作成できる。 検証までやや時間がかかるのでちゃんと通ってるかnslookupで確認しといた方がよい。
Application Load Balancer (ALB) 次の要素から構成されるL7のロードバランサー。
 Listener: 指定したプロトコルとポートでリクエストを受ける。 ListenerRule: パスやHeaderなどの値を条件にどのTargetGroupにルーティングするかのルール。 TargetGroup: ルーティングする1つ以上のTarget。Instance, IP, Lambdaが選べる。  Ruleの作成 Serverless FrameworkではALBのenentを付けるだけでLambdaに向くRuleが作成されるが、そこにはCognitoを追加できなさそうなので使っていない。OnUnauthenticatedRequestで認証失敗時の挙動を選択できる。UserPoolとSecretありのClientはあらかじめ作っておく。コールバックURLにはhttps://&amp;lt;domain&amp;gt;/oauth2/idpresponseを追加する。
API Gatewayだとタイムアウトの上限が30秒なのに対してALBはLambdaの上限まで待てる。
$ cat serverless.yml service: alb-cognito-auth-example frameworkVersion: &amp;quot;&amp;gt;=1.28.0 &amp;lt;2.0.0&amp;quot; provider: name: aws runtime: go1.x region: ap-northeast-1 package: exclude: - ./** include: - ./bin/** functions: privateapi: handler: bin/privateapi timeout: 30 resources: Resources: ALBTargetGroup: DependsOn: InvokeLambdaPermissionForALB Type: AWS::ElasticLoadBalancingV2::TargetGroup Properties: Name: &amp;quot;private-lambda-target-group&amp;quot; TargetType: &amp;quot;lambda&amp;quot; Targets: - Id: !</description>
    </item>
    
    <item>
      <title>API GatewayでCognitoの認証をかけて必要ならログイン画面に飛ばす処理をGoで書く</title>
      <link>https://www.sambaiz.net/article/226/</link>
      <pubDate>Wed, 03 Jul 2019 20:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/226/</guid>
      <description>ブラウザから直接API GatewayのエンドポイントにアクセスしたときにCognitoのTokenで認証し、失敗したらログイン画面を表示させる。 API GatewayでCognitoの認証をかける場合、AuthorizerでUserPoolを指定するのが最も簡単なパターンだが、 これだとHeaderにTokenを付けてアクセスする必要があり認証に失敗するとUnauthorizedが返る。
Cognito UserPoolとAPI Gatewayで認証付きAPIを立てる - sambaiz-net
なおAPI GatewayではなくALBをLambdaの前段に挟めば今回やることが簡単に実現できる。
LambdaとALBでCognito認証をかけて失敗したらログイン画面に飛ばす - sambaiz-net
準備 UserPoolとClientを作成する。 CloudFormationで作成する場合SchemaのMutableのデフォルトがfalseで、変えると作り直されてしまうことに注意。
Resources: Userpool: Type: AWS::Cognito::UserPool Properties: AdminCreateUserConfig: AllowAdminCreateUserOnly: false Schema: - Mutable: true Name: email Required: true - Mutable: true Name: name Required: true UsernameAttributes: - email UserPoolName: testpool UserpoolClient: Type: AWS::Cognito::UserPoolClient Properties: UserPoolId: Ref: Userpool ClientName: testclient GenerateSecret: true その後、GoogleのOAuth Client IDを作成し、フェデレーションの設定を行ってGoogleアカウントでもログインできるようにした。 これはID Poolのフェデレーティッドアイデンティティとは異なる機能。 UserPoolのドメインや、外部IdPとのAttributes Mapping、Clientの設定はCloudFormationではできないので手で行う。
 追記 (2020-12-06): 今はCloudFormationで行えるようになっている。
CDKでCognito UserPoolとClientを作成しトリガーやFederationを設定する - sambaiz-net</description>
    </item>
    
    <item>
      <title>AWS DeepRacerを始める</title>
      <link>https://www.sambaiz.net/article/224/</link>
      <pubDate>Mon, 10 Jun 2019 23:06:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/224/</guid>
      <description>AWS DeepRacerは自走する1/18スケールのレーシングカーで、 SageMakerやRoboMakerなどを使って強化学習し、実機を走らせたりバーチャルのDeepRacerリーグで競うことができる。 カメラの画像の処理や、強化学習のアルゴリズムの実装の必要はなく、報酬関数だけで動いてくれるので敷居が低い。
強化学習とDQN(Deep Q-network) - sambaiz-net
設定項目 Action space 取りうるアクションである速度とステアリングの組み合わせのリスト。次の項目から生成される。
 Maximum steering angle (1 - 30) Steering angle granularity (3, 5, 7) Maximum speed (0.8 - 8) Speed granularity (1, 2, 3) Loss type (Mean square error, Huber) Number of experience episodes between each policy-updating iteration (5 - 100)  Reward function 強化学習の報酬関数。次の入力パラメータを用いて実装する。
{ &amp;quot;all_wheels_on_track&amp;quot;: Boolean, # flag to indicate if the vehicle is on the track &amp;quot;x&amp;quot;: float, # vehicle&#39;s x-coordinate in meters &amp;quot;y&amp;quot;: float, # vehicle&#39;s y-coordinate in meters &amp;quot;distance_from_center&amp;quot;: float, # distance in meters from the track center &amp;quot;is_left_of_center&amp;quot;: Boolean, # Flag to indicate if the vehicle is on the left side to the track center or not.</description>
    </item>
    
    <item>
      <title>CDK/CircleCI/GitHubでAWSリソース管理リポジトリを作る</title>
      <link>https://www.sambaiz.net/article/223/</link>
      <pubDate>Mon, 20 May 2019 09:23:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/223/</guid>
      <description>AWS CDKでリソースを記述し、PullRequestに対して自動でcdk diffで変更があるものを表示して、mergeしたときにcdk deployする。 全体のコードはGitHubにある。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
 追記 (2019-08-29): このフローで起こったいくつかの問題を解決するため新しいツールを作った。 PR上でCDKのレビューやデプロイを行うツールcdkbotを作った - sambaiz-net
 CI Userの作成 まずcdkコマンドを実行するためのCI Userを作成する。これはCDK管理外のスタックで、AWSコンソール上から手動で上げる。
AWSのAssumeRole - sambaiz-net
AssumeRoleしかできないCIUserからCIAssumeRoleをassumeすることにした。
AWSTemplateFormatVersion: &#39;2010-09-09&#39; Resources: CIAssumeRole: Type: &#39;AWS::IAM::Role&#39; Properties: RoleName: &#39;CIAssumeRole&#39; ManagedPolicyArns: - &#39;arn:aws:iam::aws:policy/AdministratorAccess&#39; AssumeRolePolicyDocument: Version: &#39;2012-10-17&#39; Statement: - Effect: &#39;Allow&#39; Principal: AWS: - !Ref AWS::AccountId Action: - &#39;sts:AssumeRole&#39; CIGroup: Type: &#39;AWS::IAM::Group&#39; Properties: GroupName: &#39;CI&#39; CIPolicies: Type: &#39;AWS::IAM::Policy&#39; Properties: PolicyName: &#39;CI&#39; PolicyDocument: Statement: - Effect: Allow Action: &#39;sts:AssumeRole&#39; Resource: !</description>
    </item>
    
    <item>
      <title>AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする</title>
      <link>https://www.sambaiz.net/article/222/</link>
      <pubDate>Sun, 19 May 2019 01:43:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/222/</guid>
      <description>AWS CDK(Cloud Development Kit)はTypeScriptやJavaなどのコードから CloudFormationのテンプレートを生成して差分を確認しデプロイできる公式のツール。まだdeveloper preview。
$ npm i -g aws-cdk $ cdk --version 0.33.0 (build 50d71bf) $ mkdir cdk-vpc $ cd cdk-vpc $ cdk init app --language=typescript CloudFormationのリソースと対応するCfnFooや、それを内部で作成する高レベル(L2)のResource ClassFooが実装されている。 ただし、現状CfnFooに対応するResource Classが存在しないものや、複数のリソースを内部で作成するResource Classが存在する。 例えば、ec2.VpcはCfnVPCだけではなく、Public/Private Subnet、NATGatewayまでまとめて一般的な構成で作る。Resource Classはまだ変更が多い。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
型があり補完が効くので通常のテンプレートと比べて書きやすいし、ループしたりすることもできる。
import * as cdk from &#39;@aws-cdk/cdk&#39; import * as ec2 from &#39;@aws-cdk/aws-ec2&#39; interface Export { vpc: ec2.Vpc } export class VPCStack extends cdk.Stack { protected deployEnv: string export: Export constructor(scope: cdk.</description>
    </item>
    
    <item>
      <title>ECS FargateでSidecarのFluentdでログをS3に送る構成をCloudFormationで構築する</title>
      <link>https://www.sambaiz.net/article/221/</link>
      <pubDate>Thu, 09 May 2019 23:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/221/</guid>
      <description>DAEMONを動かすことはできず、 fluentd logdriverもサポートされていないFargateで、 サイドカーとしてFluentdのコンテナを動かしてアプリケーションのログをS3に送る。 全体のコードはGitHubにある。
FargateでECSを使う - sambaiz-net
Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る - sambaiz-net
Fluentd 必要なプラグインと設定ファイルを入れたイメージを作る。
FROM fluent/fluentd:v1.4-1 USER root COPY ./fluent.conf /fluentd/etc/ # install plugin RUN apk add --update-cache --virtual .build-deps sudo build-base ruby-dev \ &amp;amp;&amp;amp; gem install fluent-plugin-s3 -v 1.0.0 --no-document \ &amp;amp;&amp;amp; gem install uuidtools \ &amp;amp;&amp;amp; gem sources --clear-all \ &amp;amp;&amp;amp; apk del .build-deps \ &amp;amp;&amp;amp; rm -rf /var/cache/apk/* \ /home/fluent/.gem/ruby/*/cache/*.gem # set timezone (Alpine) RUN apk --update-cache add tzdata &amp;amp;&amp;amp; \ cp /usr/share/zoneinfo/Asia/Tokyo /etc/localtime &amp;amp;&amp;amp; \ apk del tzdata &amp;amp;&amp;amp; \ rm -rf /var/cache/apk/* fluent.</description>
    </item>
    
    <item>
      <title>DatadogのAWS integrationとAlertの設定をTerraformで行う</title>
      <link>https://www.sambaiz.net/article/219/</link>
      <pubDate>Sat, 04 May 2019 19:25:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/219/</guid>
      <description>DatadogのAWS integrationとAlertの設定をTerraformで行い、バージョン管理やレビューできるようにする。 全体のコードはGitHubに置いてある。
AWS Integration まずdatadog_integration_awsでAWS integrationの設定を作成してExternalIDを取得し、Policy/Roleを作成する。必要な権限はドキュメントを参照。
resource &amp;quot;datadog_integration_aws&amp;quot; &amp;quot;test&amp;quot; { account_id = &amp;quot;${var.aws_account_id}&amp;quot; role_name = &amp;quot;${var.aws_integration_role_name}&amp;quot; filter_tags = [&amp;quot;datadog:1&amp;quot;] } data &amp;quot;aws_iam_policy_document&amp;quot; &amp;quot;datadog_aws_integration_assume_role&amp;quot; { statement { actions = [&amp;quot;sts:AssumeRole&amp;quot;] principals { type = &amp;quot;AWS&amp;quot; identifiers = [&amp;quot;arn:aws:iam::464622532012:root&amp;quot;] } condition { test = &amp;quot;StringEquals&amp;quot; variable = &amp;quot;sts:ExternalId&amp;quot; values = [ &amp;quot;${datadog_integration_aws.test.external_id}&amp;quot;, ] } } } Datadog providerにはないSlackなどその他のintegrationは手動で設定する必要がある。 また、ログを集める場合Serverless Application Repositoryから公式のDatadog-Log-Forwarderを入れて AWS IntegrationのところにLambdaのARNを入れるのも手動。
 追記 (2020-12-07): 今はDatadog Forwaderとなり、Serverless Application Repositoryからではなく、直接CloudFormationのスタックを上げるようになっているのでaws_cloudformation_stackでデプロイできる。AWS IntegrationのRoleに必要なPolicyを与えてCollect LogsタブのLambda Cloudwatch Logsにチェックを入れると、Optionally limit resource collectionを設定しているならそのtagを持つ、全てのFunctionのStreamに自動でForwarderへのSubscription Filterが作成され転送が始まる。チェックを外すと削除されるが、この際もtagを見ているようで、条件を変更するとSubscription Filterが一部残ることがあった。ログだけではなくlambdaからのカスタムメトリクスの中継や、estimated_costなどの拡張メトリクスの送信も行う。</description>
    </item>
    
    <item>
      <title>SageMaker NotebookでGitリポジトリにSSHでpush/pullできるようにする</title>
      <link>https://www.sambaiz.net/article/211/</link>
      <pubDate>Mon, 04 Mar 2019 22:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/211/</guid>
      <description>Sagemaker NotebookはAWSの機械学習のワークフローを提供するSageMakerの一部である マネージドなJupyter Notebooksで、可視化などはもちろん、ここから複数インタンスでの学習ジョブを実行したりすることができる。
Git統合 によってノートブック作成時にGitHubなどのリポジトリを指定すると前もって持ってきてくれるようになったが、 今のところHTTPSエンドポイントにしか対応していないようで、ユーザー名・パスワードまたはトークンといった個人に紐づく認証情報が必要になる。 今回はこの機能を使わずに、ライフサイクル設定でssh鍵を置き、これでpush/pullできるようにする。
パスフレーズなしの鍵を作って公開鍵を対象リポジトリのDeployKeyに登録してread/writeできるようにする。
$ mkdir sagemaker-sshkey $ cd sagemaker-sshkey $ ssh-keygen -t rsa -b 4096 -f id_rsa -N &amp;quot;&amp;quot; $ pbcopy &amp;lt; id_rsa.pub 秘密鍵はSSMのParameter Storeに登録する。
AWS Systems Manager (SSM)のParameter Storeに認証情報を置き参照する - sambaiz-net
$ aws ssm put-parameter --name &amp;quot;sagemaker-sshkey&amp;quot; --value &amp;quot;`cat id_rsa`&amp;quot; --type String --overwrite $ aws ssm get-parameters --names &amp;quot;sagemaker-sshkey&amp;quot; ライフサイクル設定でノートブック開始時に次のスクリプトが実行されるようにする。 このスクリプトはrootで実行される。Parameter Storeが読める権限をNotebookのIAMに付けておく。
#!/bin/bash set -e su - ec2-user &amp;lt;&amp;lt;EOF cd /home/ec2-user aws ssm get-parameters --names &amp;quot;sagemaker-sshkey&amp;quot; | jq -r &amp;quot;.</description>
    </item>
    
    <item>
      <title>AWS SAMでLambdaの関数をデプロイしServerless Application Repositoryに公開する</title>
      <link>https://www.sambaiz.net/article/207/</link>
      <pubDate>Sun, 10 Feb 2019 16:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/207/</guid>
      <description>AWS SAM (Serverless Application Model)はAWS公式の サーバーレスアプリケーションのビルドツール。 CloudFormationのテンプレートを設定ファイルに書くことでLambda関数と共にイベントトリガーや他のリソースも含めてデプロイでき、 その点でServerless Frameworkと立ち位置が近いが、向こうがLambda以外のサーバーレス環境にも対応していたり、 プラグインによって機能拡張できるようになっている一方、こちらは比較的薄いツールになっている。 ただ、Serverless Application Repositoryで公開するにはSAMの形式にする必要があり、 Serverless FrameworkにもSAMのテンプレートを出力するプラグインがある。
Serverless FrameworkでLambdaをデプロイする - sambaiz-net
SAM CLIのインストール $ brew tap aws/tap $ brew install aws-sam-cli $ sam --version SAM CLI, version 0.11.0 init initすると次の構成のディレクトリが作られる。
$ sam init --runtime go1.x -n test-sam $ cd test-sam/ $ ls Makefile	README.md	hello-world	template.yaml template.yamlの中に関数の設定やCloudFormationのテンプレートを書く。
$ cat template.yaml AWSTemplateFormatVersion: &#39;2010-09-09&#39; Transform: AWS::Serverless-2016-10-31 Description: &amp;gt; test-sam Sample SAM Template for test-sam # More info about Globals: https://github.</description>
    </item>
    
    <item>
      <title>CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う</title>
      <link>https://www.sambaiz.net/article/206/</link>
      <pubDate>Sun, 03 Feb 2019 17:31:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/206/</guid>
      <description>Aurora ServerlessはオートスケールするAuroraで、 使ったAurora Capacity Unit (ACU)によって料金が発生するため、 使用頻度が少なかったり変動するアプリケーションにおいて安くRDBを使うことができる。 インスタンスを立てると最低でも月3000円くらいかかるが、Serverlessだとほとんどストレージ分から運用することができて趣味でも使いやすい。 ただしLambdaと同様に常に同等のリソースを使っている状態だとインスタンスと比べて割高になる。
今回はLambdaで使う。 Serverlessと名前には付いているが用途としてはLambdaに限らず、 むしろコンテナの数が容易に増え得るLambdaは同時接続数が問題になるRDBと一般に相性が良くない。 現在Betaの、コネクションを張らずにHTTPSでクエリを投げられるData APIはこの問題を解消すると思われるが、トランザクションが張れなかったり、レスポンスサイズに制限があるようだ。今回はコンソール上から初期クエリを流すためにData APIを有効にしている。
他の選択肢として、DynamoDBは現状最有力で最近トランザクションもサポートされたがSQLのように複雑なクエリは投げられない。 Athenaはクエリは投げられるがそこそこ時間がかかるし、INSERT/UPDATEはできずクエリごとに料金が発生する。
Serverless Frameworkを使ってリソースを作成しデプロイする。リポジトリはここ。
Serverless FrameworkでLambdaをデプロイする - sambaiz-net
VPCの作成 Aurora Serverlessの制限の一つとしてVPC内からしか接続しかできないというものがある。ということでVPCから作成していく。以前Terraformで作ったのと同じリソースをCloudFormationで作る。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
LambdaをVPC内で動かすとコンテナ起動時にENIも作成するため立ち上がりの際時間がかかる。必要なら定期的に呼び出して削除されないようにする。 また、今回はテストのため/24でVPCを切っているが、小さいとENIのIPアドレスが枯渇する可能性がある。
 VPC  TestVPC: Type: AWS::EC2::VPC Properties: CidrBlock: 172.32.0.0/24 Tags: - Key: Name Value: test-vpc  Subnet  Aurora Serverlessのために少なくとも2つのサブネットが必要。
TestPublicSubnet: Type: AWS::EC2::Subnet Properties: VpcId: !Ref TestVPC CidrBlock: 172.32.0.0/25 AvailabilityZone: us-east-1d Tags: - Key: Name Value: test-public-subnet1 TestPrivateSubnet1: Type: AWS::EC2::Subnet Properties: VpcId: !</description>
    </item>
    
    <item>
      <title>AWS Systems Manager (SSM)のParameter Storeに認証情報を置き参照する</title>
      <link>https://www.sambaiz.net/article/204/</link>
      <pubDate>Mon, 07 Jan 2019 23:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/204/</guid>
      <description>DBのパスワードやAPIトークンといった認証情報をバージョン管理するコードや設定ファイル上に書くとOSS化など公開範囲を広げるときにやや困るし漏れるリスクが高まるのでなるべく避けたい。 そこでSSMのParameter Storeに値を置き、実行時やデプロイ時に参照する。
SSMのParameter StoreとSecrets Manager Systems Manager (SSM)はAWSのリソースを可視化したり操作を自動化したりするサービス群で、 設定を持つParameter Storeはその一つ。値は暗号化して持つこともできる。 料金はかからない。
SSMのParameter Storeと似たような別のAWSのサービスに Secrets Managerというのがあって、RDSなどと連携してLambdaによって定期的に新しい値を生成しローテーションさせることができる。 ただし料金がシークレットの件数($0.4/月)とAPIコール($0.05/10000回)でかかる。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
今はParamter StoreとSecrets Managerが統合されていて、Parameter StoreのAPIでどちらも参照できるようだ。 今回はローテーションしないので単純に料金がかからないParameter Storeの方に書き込むことにする。 ただし、Parameter Storeは現状一度に大量のリクエストが飛ぶような使い方をするとRate exceededになってしまう問題がある。
実行時の値取得 実行時に値を取得するのはこんな感じ。
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;github.com/aws/aws-sdk-go/aws&amp;quot; &amp;quot;github.com/aws/aws-sdk-go/aws/awserr&amp;quot; &amp;quot;github.com/aws/aws-sdk-go/aws/session&amp;quot; &amp;quot;github.com/aws/aws-sdk-go/service/ssm&amp;quot; ) type Parameter struct { ssm *ssm.SSM } func newParameter(sess *session.Session) *Parameter { return &amp;amp;Parameter{ ssm: ssm.New(sess), } } func (s *Parameter) Get(name string, decrypt bool) (string, error) { param, err := s.</description>
    </item>
    
    <item>
      <title>AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する</title>
      <link>https://www.sambaiz.net/article/203/</link>
      <pubDate>Tue, 01 Jan 2019 17:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/203/</guid>
      <description>AWS GlueはマネージドなETL(Extract/Transform/Load)サービスで、Sparkを使ってS3などにあるデータを読み込み加工して変換出力したり、AthenaやRedshift Spectrumで参照できるデータカタログを提供する。 今回はS3のCSVを読み込んで加工し、列指向フォーマットParquetに変換しパーティションを切って出力、その後クローラを回してデータカタログにテーブルを作成してAthenaで参照できることを確認する。
料金はジョブがDPU(4vCPU/16GBメモリ)時間あたり$0.44(最低2DPU/10分)かかる。 また、クローラも同様にDPUで課金される。結構高い。
なお、AthenaのCTASでもParquetを出力することができる。 出力先にファイルがないようにする必要があったり重いクエリは失敗することがあるが手軽で良い。
import * as athena from &#39;athena-client&#39; const clientConfig: athena.AthenaClientConfig = { bucketUri: &#39;s3://*****/*****&#39; skipFetchResult: true, }; const awsConfig: athena.AwsConfig = { region: &#39;us-east-1&#39;, }; const client = athena.createClient(clientConfig, awsConfig); (async () =&amp;gt; { await client.execute(` CREATE TABLE ***** WITH ( format = &#39;PARQUET&#39;, external_location = &#39;s3://*****&#39; ) AS ( SELECT ~~ ) })(); 開発用エンドポイント ジョブの立ち上がりにやや時間がかかるため開発用エンドポイントを立ち上げておくとDPUが確保されて効率よく開発できる。 立ち上げている間のDPUの料金がかかる。つまりずっとジョブを実行し続けているようなもので結構高くつくので終わったら閉じるようにしたい。
ローカルやEC2から自分で開発用エンドポイントにsshしてNotebookを立てることもできるが、 コンソールから立ち上げたNotebookは最初からつながっていて鍵の登録も必要なくて楽。
ssh -i private-key-file-path -NTL 9007:169.</description>
    </item>
    
    <item>
      <title>FargateでECSを使う</title>
      <link>https://www.sambaiz.net/article/196/</link>
      <pubDate>Fri, 09 Nov 2018 00:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/196/</guid>
      <description>ECSはAWSのコンテナオーケストレーションサービス。 クラスタはEC2上に立てることもできるが、その場合Auto Scalingグループの設定やスケールイン時のdrainなどを考慮する必要がある。 Fargateで起動するとサーバーレスで実行でき、バックエンドの管理が必要がなくなる。 料金は割り当てたvCPUとメモリによって、最低1分の1秒単位で課金される。 Lambdaと同じくリソースあたりでいうとオンデマンドのEC2と比較して割高。ただし柔軟にリソースが指定できる分いくらか差は縮まる。
特にバッチ処理のように常にリソースが必要ないTaskは都度インスタンスを立ち上げるのも面倒なので良いと思う。 Lambdaと比較すると、実行環境を自由に作れるのと実行時間に制限がないというところが良いが、 Taskを作るトリガーは現状cronだけなのでそれ以外のイベントで実行したい場合はLambdaと組み合わせる必要がある。
AWSにはKubernetesクラスタを立てられるEKSもあるが、こちらはまだFargateに対応していない。 もしかしたら今月末のre:Inventで何か発表されるかもしれない。
Clusterの作成 まずはClusterを作成する。
$ aws ecs create-cluster --cluster-name test Taskの登録 イメージやポートマッピング、ヘルスチェックや割り当てるリソースといったContainer definition を含むTask definitionを書く。 Fargateの場合networkModeはawsvpc固定になる。
{ &amp;quot;family&amp;quot;: &amp;quot;test-task&amp;quot;, &amp;quot;networkMode&amp;quot;: &amp;quot;awsvpc&amp;quot;, &amp;quot;containerDefinitions&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;nginx&amp;quot;, &amp;quot;image&amp;quot;: &amp;quot;nginx:1.15&amp;quot;, &amp;quot;portMappings&amp;quot;: [ { &amp;quot;containerPort&amp;quot;: 80, &amp;quot;hostPort&amp;quot;: 80, &amp;quot;protocol&amp;quot;: &amp;quot;tcp&amp;quot; } ], &amp;quot;essential&amp;quot;: true } ], &amp;quot;requiresCompatibilities&amp;quot;: [ &amp;quot;FARGATE&amp;quot; ], &amp;quot;cpu&amp;quot;: &amp;quot;256&amp;quot;, &amp;quot;memory&amp;quot;: &amp;quot;512&amp;quot; } Taskを登録する。
$ aws ecs register-task-definition --cli-input-json file://$(pwd)/task.json $ aws ecs list-task-definitions &amp;quot;taskDefinitionArns&amp;quot;: [ &amp;quot;arn:aws:ecs:ap-northeast-1:*****:task-definition/test-task:1&amp;quot; ] } Taskを実行 run-taskでTaskを実行できる。 外からアクセスできるようにPublicIPを割り当てている。</description>
    </item>
    
    <item>
      <title>Macでの開発環境構築メモ</title>
      <link>https://www.sambaiz.net/article/163/</link>
      <pubDate>Sat, 14 Apr 2018 14:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/163/</guid>
      <description>新しいMBPを買ったので開発環境の構築でやったことを残しておく
設定  アクセシビリティから3本指スクロールを有効にする ホットコーナーの左上にLaunchPad、右上にデスクトップを割り当てている 画面をなるべく広く使うためにDockは左に置いて自動的に隠す  bash_profile パッケージマネージャ以外で持ってきたバイナリは${HOME}/binに置くことにする。
touch ~/.bash_profile mkdir ${HOME}/bin echo &amp;quot;export PATH=\$PATH:${HOME}/bin&amp;quot; &amp;gt;&amp;gt; ~/.bash_profile HomeBrew &amp;amp; Cask /usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot; brew tap caskroom/cask 一般的なアプリケーション/コマンドのインストール XcodeとUnityとLINEは手動で入れる。
brew cask install google-chrome kap visual-studio-code slack kindle brew install jq gibo mysql wget Git git config --global user.name sambaiz git config --global user.email godgourd@gmail.com Docker &amp;amp; K8s brew cask install docker virtualbox minikube brew install docker kubernetes-helm fish bash前提で書かれたスクリプトも多いので、デフォルトシェルにはしない。</description>
    </item>
    
    <item>
      <title>Cognito UserPoolとAPI Gatewayで認証付きAPIを立てる</title>
      <link>https://www.sambaiz.net/article/157/</link>
      <pubDate>Sun, 25 Feb 2018 23:46:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/157/</guid>
      <description>UserPoolを作成。デフォルト設定はこんな感じ。 必須項目や、確認メールの文面などを自由にカスタマイズでき、 登録時などのタイミングでLambdaを発火させることもできる。
作成したUserPoolにアプリクライアントを追加する。 ブラウザで使うのでクライアントシークレットはなし。
クライアント側 amazon-cognito-identity-jsを使う。
依存するjsを持ってくる。
$ wget https://raw.githubusercontent.com/aws/amazon-cognito-identity-js/master/dist/amazon-cognito-identity.min.js $ wget https://raw.githubusercontent.com/aws/amazon-cognito-identity-js/master/dist/aws-cognito-sdk.min.js Sign UpからAPIを呼ぶところまでのボタンを並べた。 SignInするとOIDC標準のトークンがそのページのドメインのLocal Storageに書かれる。
OpenID ConnectのIDトークンの内容と検証 - sambaiz-net
CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.idToken CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.accessToken CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.refreshToken CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.clockDrift CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.LastAuthUser APIを呼ぶときはidTokenをAuthorization Headerに乗せる。
&amp;lt;button id=&amp;quot;signUp&amp;quot;&amp;gt;Sign Up&amp;lt;/button&amp;gt; &amp;lt;p&amp;gt;&amp;lt;label&amp;gt;Code:&amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;code&amp;quot;&amp;gt;&amp;lt;/label&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;button id=&amp;quot;confirm&amp;quot;&amp;gt;Confirm&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;quot;signIn&amp;quot;&amp;gt;Sign In&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;quot;whoAmI&amp;quot;&amp;gt;Who am I?&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;quot;requestAPI&amp;quot;&amp;gt;Request API with token&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;quot;signOut&amp;quot;&amp;gt;Sign Out&amp;lt;/button&amp;gt; &amp;lt;script src=&amp;quot;aws-cognito-sdk.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script src=&amp;quot;amazon-cognito-identity.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script&amp;gt; const USER_NAME = &amp;quot;*****&amp;quot;; const USER_PASSWORD = &amp;quot;*****&amp;quot;; const USER_EMAIL = &amp;quot;*****&amp;quot;; class CognitoUserPoolAuth { constructor(UserPoolId, clientId, apiEndpoint) { const poolData = { UserPoolId : UserPoolId, ClientId : clientId }; this.</description>
    </item>
    
    <item>
      <title>Serverless FrameworkでLambdaをデプロイする</title>
      <link>https://www.sambaiz.net/article/155/</link>
      <pubDate>Sun, 11 Feb 2018 23:20:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/155/</guid>
      <description>Serverless FrameworkでLambda Functionをデプロイする。 Apexが基本的にFunction自体のデプロイしかしないのに対して、こちらはeventの設定なども諸々やってくれて強い。
ApexでLambdaをデプロイする - sambaiz-net
$ npm install -g serverless $ serverless version 1.26.0 ApexではFunctionごとにディレクトリが作られたが、ServerlessではServiceごとに作られ、 一つのService内で複数のFunctionを定義できる。handlerは同じでも異なっていてもよい。
Apexの形式の場合、共通の処理をWebpackなどで各Functionに持って来たり、 同じような処理の複数のFunctionを立てる際はコピーする必要があったが、 こちらは必要最小限の変更でそれらを行うことができる。
templateからServiceをcreateする。
$ serverless create --template aws-nodejs --path testservice $ ls testservice/ handler.js	serverless.yml 設定ファイルserverless.yml にはLambdaの基本的なもののほかに、VPCやevent、IAM Roleなども書けて、これらはdeploy時に作られるCloudFormationのstackによって管理される。必要なら生のCloudFormationの設定も書ける。
ApexでもTerraformによって管理することができるが、書くのはこちらの方がはるかに楽。
ApexでデプロイしたLambdaのトリガーをTerraformで管理する - sambaiz-net
$ cat sesrverless.yml service: testservice provider: name: aws profile: foobar region: ap-northeast-1 runtime: nodejs6.10 memorySize: 512 timeout: 10 functions: hello: handler: handler.hello events: - http: path: hello/world method: get cors: true deployすると{service}-{stage}-{function}のFunctionが作られる。 今回の場合はtestservice-prd-test。stageをymlでも指定しなかった場合はデフォルト値のdevになる。</description>
    </item>
    
    <item>
      <title>DatadogのLambda Integrationで気象データを送ってアラートを飛ばす</title>
      <link>https://www.sambaiz.net/article/152/</link>
      <pubDate>Mon, 05 Feb 2018 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/152/</guid>
      <description>最近朝が寒くて布団から出るのがつらい。雨や雪が降っていたら尚更のこと、それならそれで現状を把握する必要がある。 そこで、無料から使える気象API OpenWeatherMapのデータをdatadogに送って、特に寒い日や雨雪が降るような朝にはアラートを飛ばすことにした。
インスタンスが立っていたらDataDog AgentのDogStatsD経由で送ることができ、 そうでなければ通常はAPIを呼ぶことになるんだけど、Lambdaでは、AWS Integrationを設定すると有効になるLambda Integrationによって MONITORING|unix_epoch_timestamp|value|metric_type|my.metric.name|#tag1:value,tag2のフォーマットでconsole.logするだけでメトリクスが送られるようになっている。
 追記 (2020-12-07): 今はDatadog Forwaderを通して送ることができる。
 const axios = require(&#39;axios&#39;); const CITY = &#39;Shibuya&#39;; const API_KEY = &#39;*****&#39;; const WEATHER_API = `http://api.openweathermap.org/data/2.5/weather?q=${CITY}&amp;amp;units=metric&amp;amp;appid=${API_KEY}`; const METRIC_COUNTER = &#39;counter&#39;; const METRIC_GAUGE = &#39;gauge&#39;; const monitor = (metricName, metricType, value, tags) =&amp;gt; { const unixEpochTimestamp = Math.floor(new Date().getTime()); console.log(`MONITORING|${unixEpochTimestamp}|${value}|${metricType}|${metricName}|#${tags.join(&#39;,&#39;)}`); }; exports.handler = async (event, context, callback) =&amp;gt; { const data = (await axios.get(WEATHER_API)).data const namePrefix = &#39;livinginfo.</description>
    </item>
    
    <item>
      <title>Athenaのmigrationやpartitionするathena-adminを作った</title>
      <link>https://www.sambaiz.net/article/145/</link>
      <pubDate>Sun, 24 Dec 2017 23:31:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/145/</guid>
      <description>https://github.com/sambaiz/athena-admin
AthenaはS3をデータソースとするマネージドなデータ分析基盤。Prestoベースで標準SQLを実行できる。
料金はスキャンしたデータ量にかかり、$5/TB。1MB切り上げで、10MB以下のクエリは10MBになる。 データ量に対してかなり安く使えるものの、フルスキャンしてしまうとBigQueryと同様にお金が溶けてしまうので、大抵はパーティションを切ることになるのだけど都度locationを指定してADD PARTITIONを実行するのは大変。さらにスキーマを変更するのにもALTER TABLE ADD COLUMNSなどはないのでテーブルを作り直すことになるが、当然パーティションも全部作り直すことになる。
ではどうしようもないかというとMSCK REPAIR TABLEというのがあって、 これはS3のObjectのdt=YYYY-MM-DDのようなkey=valueのprefixを認識してパーティションを作るもの。作り直す際もこれ1クエリで終わる。それなら最初からそういう風に置けばよいのではというところだけど、勝手にYYYY/MM/DD/HHのprefixを付けてしまうFirehoseのようなのもある。
今回作ったathena-adminは以下のような定義ファイルから、 パーティションのkey=valueのprefixが付くように置き換えたり、変更があったらmigrationする。 このファイルを書き換えるだけで基本的にどうにかなるし、バージョン管理すればテーブル定義の変更を追うことができる。
{ &amp;quot;general&amp;quot;: { &amp;quot;athenaRegion&amp;quot;: &amp;quot;ap-northeast-1&amp;quot;, &amp;quot;databaseName&amp;quot;: &amp;quot;aaaa&amp;quot;, &amp;quot;saveDefinitionLocation&amp;quot;: &amp;quot;s3://saveDefinitionBucket/aaaa.json&amp;quot; }, &amp;quot;tables&amp;quot;: { &amp;quot;sample_data&amp;quot;: { &amp;quot;columns&amp;quot;: { &amp;quot;user_id&amp;quot;: &amp;quot;int&amp;quot;, &amp;quot;value&amp;quot;: { &amp;quot;score&amp;quot;: &amp;quot;int&amp;quot;, &amp;quot;category&amp;quot;: &amp;quot;string&amp;quot; } /* &amp;quot;struct&amp;lt;score:int,category:string&amp;gt;&amp;quot; のように書くこともできる */ }, &amp;quot;srcLocation&amp;quot;: &amp;quot;s3://src/location/&amp;quot;, &amp;quot;partition&amp;quot;: { &amp;quot;prePartitionLocation&amp;quot;: &amp;quot;s3://pre/partition/&amp;quot;, /* optional */ &amp;quot;regexp&amp;quot;: &amp;quot;(\\d{4})/(\\d{2})/(\\d{2})/&amp;quot;, /* optional */ &amp;quot;keys&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;dt&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;format&amp;quot;: &amp;quot;{1}-{2}-{3}&amp;quot;, /* optional */ } ] } } } } 使い方はこんな感じ。使い方によってはmigrate()だけ呼ぶこともあると思う。 replaceObjects()にはmatchedHandlerというのを渡すこともできて、 UTCからJSTに変換するといったこともできる。</description>
    </item>
    
    <item>
      <title>ApexでデプロイしたLambdaのトリガーをTerraformで管理する</title>
      <link>https://www.sambaiz.net/article/144/</link>
      <pubDate>Sun, 12 Nov 2017 22:23:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/144/</guid>
      <description>Apexでfunctionをデプロイするとトリガーが登録されないのであとで追加することになる。 これを手作業で行うこともできるのだけど、せっかくなのでアプリケーションと一緒に管理したい。 そんなときのためにterraformコマンドをラップしたapex infraが用意されている。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
functionsと同列にinfrastructureディレクトリを作成してtfファイルを置く。 その下に環境ごとのディレクトリを作成することもできて、その場合は--envで指定した環境のものが使われる。
- functions - infrastructure main.tf variables.tf - modules - cloudwatch_schedule main.tf variables.tf project.json functionをデプロイするとそのARNが変数で取れるようになる。
$ apex list --tfvars apex_function_hello=&amp;quot;arn:aws:lambda:ap-northeast-1:*****:function:usetf_hello&amp;quot; 今回設定するトリガーはCloudwatch Eventのスケジューリング。作成するリソースは以下の通り。
 aws_cloudwatch_event_ruleでイベントルール(今回はschedule)を作成 aws_cloudwatch_event_targetでルールにターゲット(今回はLambda)を設定 aws_lambda_permissionでルールに対象Lambdaをinvokeする権限を付ける  $ cat infrastructure/modules/cloudwatch_schefule/variables.tf variable &amp;quot;lambda_function_name&amp;quot; {} variable &amp;quot;lambda_function_arn&amp;quot; {} variable &amp;quot;schedule_expression&amp;quot; { description = &amp;quot;cloudwatch schedule expression e.g. \&amp;quot;cron(0/5 * * * ? *)\&amp;quot;&amp;quot; } $ cat infrastructure/modules/cloudwatch_schefule/main.tf resource &amp;quot;aws_cloudwatch_event_rule&amp;quot; &amp;quot;lambda&amp;quot; { name = &amp;quot;lambda_rule_${var.lambda_function_name}&amp;quot; description = &amp;quot;invoke lambda ${var.</description>
    </item>
    
    <item>
      <title>Redashでデータを可視化する</title>
      <link>https://www.sambaiz.net/article/141/</link>
      <pubDate>Mon, 23 Oct 2017 23:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/141/</guid>
      <description>RedashはOSSのデータ可視化ツール。 BIツールのようにパラメータを変えながら指標を探っていくというよりは、分かっている指標を見るのに使うイメージ。 比較的機能が少ない分処理がわかりやすく、 クエリが自動生成されないため時間がかかるものを実行する前にある程度気づけるのが良いと思う。
docker-composeで立ち上げることもできるけど、 AWSの各リージョンにAMIが用意されているのでそれで立ち上げる。
sshで入って以下のようなのを必要に応じて設定する。 メールを送る場合はSESでメールアドレスをVerifyしてやるのが簡単。 GSuiteを使っている場合、OAuthのClientID、Secretを発行しドメインを登録するとそれで認証できる。
$ ssh ubuntu@***** $ sudo vi /opt/redash/.env export REDASH_MAIL_SERVER=&amp;quot;email-smtp.us-east-1.amazonaws.com&amp;quot; export REDASH_MAIL_USE_TLS=&amp;quot;true&amp;quot; export REDASH_MAIL_USERNAME=&amp;quot;*****&amp;quot; export REDASH_MAIL_PASSWORD=&amp;quot;*****&amp;quot; export REDASH_MAIL_DEFAULT_SENDER=&amp;quot;*****&amp;quot; # Email address to send from export REDASH_GOOGLE_CLIENT_ID=&amp;quot;&amp;quot; export REDASH_GOOGLE_CLIENT_SECRET=&amp;quot;&amp;quot; $ cd /opt/redash/current $ sudo -u redash bin/run ./manage.py org set_google_apps_domains {{domains}} $ sudo supervisorctl restart all HTTPS対応するのに/etc/nginx/sites-available/redashを編集する。crtとkeyの場所は変える。
upstream rd_servers { server 127.0.0.1:5000; } server { server_tokens off; listen 80 default; access_log /var/log/nginx/rd.access.log; gzip on; gzip_types *; gzip_proxied any; location /ping { proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://rd_servers; } location / { return 301 https://$host$request_uri; } } server { listen 443 ssl; # Make sure to set paths to your certificate .</description>
    </item>
    
    <item>
      <title>ApexでLambdaをデプロイする</title>
      <link>https://www.sambaiz.net/article/140/</link>
      <pubDate>Sun, 22 Oct 2017 16:06:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/140/</guid>
      <description>ApexでLambdaをデプロイする。 とても簡単に使えるし、変なこともしないので良い感じ。
 Serverless Frameworkだとeventの設定までカバーできてより便利。
Serverless FrameworkでLambdaをデプロイする - sambaiz-net
 インストール。ダウンロードして実行できるようにしている。
$ curl https://raw.githubusercontent.com/apex/apex/master/install.sh | sh  IAMFullAccess AWSLambdaFullAccess  を付けたIAMのプロファイルを登録しておく。
$ aws configure --profile apex $ aws configure list --profile apex Name Value Type Location ---- ----- ---- -------- profile apex manual --profile access_key ****************OVGQ shared-credentials-file secret_key ****************oi5t shared-credentials-file region ap-northeast-1 config-file ~/.aws/config apex initしてnameとdescriptionを入れるとIAMが登録され、 ディレクトリ構造が作られる。
$ apex init --profile apex Project name: try-apex Project description: test [+] creating IAM try-apex_lambda_function role [+] creating IAM try-apex_lambda_logs policy [+] attaching policy to lambda_function role.</description>
    </item>
    
    <item>
      <title>Lambda上でPuppeteer/Headless Chromeを動かすStarter Kitを作った</title>
      <link>https://www.sambaiz.net/article/132/</link>
      <pubDate>Sun, 10 Sep 2017 23:45:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/132/</guid>
      <description>PuppeteerでHeadless Chromeを動かすコードを Lambda上で動かすStarter Kitを作った。
puppeteer-lambda-starter-kit
Chromeの準備 Puppeteerのインストール時に落としてくるChromeをLambda上で動かそうとしても Lambdaにないshared libraryに依存しているため失敗する。
error while loading shared libraries: libpangocairo-1.0.so.0: cannot open shared object file: No such file or directory Lambda上でHeadless Chromeを動かす例がないか調べたらserverless-chromeというのがあって、 Headless用の設定でChromeをビルドしていた。 ほかにはchromelessというのもあるけど これはserverless-chromeに 依存している。 最小構成でPuppeteerを使いたかったので、今回はこれらを使わず一から作ることにした。
serverless-chromeにもビルドしたものが置いてあるが、少しバージョンが古いようだったので最新版でビルドした。 基本的には書いてある 通りやればうまくいく。他のプロセスとのshared memoryとして/dev/shmを使っているのを、/tmpに置き換える ようにしないと、実行時のpage.goto()でFailed Provisional Load: ***, error_code: -12になる。
ビルドしたheadless_shellには問題になった依存は含まれていないようだ。
$ ldd headless_shell linux-vdso.so.1 =&amp;gt; (0x00007ffcb6fed000) libpthread.so.0 =&amp;gt; /lib64/libpthread.so.0 (0x00007f5f17dbe000) libdl.so.2 =&amp;gt; /lib64/libdl.so.2 (0x00007f5f17bba000) librt.so.1 =&amp;gt; /lib64/librt.so.1 (0x00007f5f179b1000) libnss3.so =&amp;gt; /usr/lib64/libnss3.so (0x00007f5f17692000) libnssutil3.so =&amp;gt; /usr/lib64/libnssutil3.so (0x00007f5f17466000) libsmime3.so =&amp;gt; /usr/lib64/libsmime3.</description>
    </item>
    
    <item>
      <title>TerraformでVPCを管理するmoduleを作る</title>
      <link>https://www.sambaiz.net/article/121/</link>
      <pubDate>Sun, 23 Jul 2017 02:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/121/</guid>
      <description>Terraform
$ brew install terraform $ terraform -v Terraform v0.9.11 Terraformの設定要素 provider IaaS(e.g. AWS)、PaaS(e.g. Heroku)、SaaS(e.g. CloudFlare)など。
AWS Providerはこんな感じ。 ここに直接access_keyやsecret_keyを書くこともできるけど、誤って公開されてしまわないように環境変数か variableで渡す。
provider &amp;quot;aws&amp;quot; { # access_key = &amp;quot;${var.access_key}&amp;quot; # secret_key = &amp;quot;${var.secret_key}&amp;quot; region = &amp;quot;us-east-1&amp;quot; } $ export AWS_ACCESS_KEY_ID=&amp;quot;anaccesskey&amp;quot; $ export AWS_SECRET_ACCESS_KEY=&amp;quot;asecretkey&amp;quot; varibale CLIでオーバーライドできるパラメーター。typeにはstringのほかにmapやlistを渡すことができ、 何も渡さないとdefault値のものが、それもなければstringになる。
variable &amp;quot;key&amp;quot; { type = &amp;quot;string&amp;quot; default = &amp;quot;value&amp;quot; description = &amp;quot;description&amp;quot; } 値を渡す方法はTF_VAR_をprefixとする環境変数、-var、-var-fileがある。 また、moduleのinputとして渡されることもある。
$ export TF_VAR_somelist=&#39;[&amp;quot;ami-abc123&amp;quot;, &amp;quot;ami-bcd234&amp;quot;]&#39; $ terraform apply -var foo=bar -var foo=baz $ terraform apply -var-file=foo.</description>
    </item>
    
    <item>
      <title>fluentdのAggregatorをELBで負荷分散し、Blue/Green Deploymentする</title>
      <link>https://www.sambaiz.net/article/113/</link>
      <pubDate>Sun, 25 Jun 2017 00:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/113/</guid>
      <description>デプロイやスループットの調整を簡単にするため、BeanstalkでAggregatorを立ち上げた。
負荷分散 TCPの24224(設定による)が通るようにEC2,ELBのSGとリスナーの設定をする必要があって、 ELBのSGのアウトバウンドの設定が見落とされがち。ELBのクロスゾーン分散は有効になっている。
まず、ELBに3台、それぞれ別のAZ(1b, 1c, 1d)に配置されている状態でログを送り始めるとそれぞれ均等にログが届いた。 その状態から4台(1b * 2, 1c, 1d)にすると、2つのインスタンス(1b, 1c)のみに均等にログが届くようになった。 4台になると(1b, 1c)と(1b, 1d)に分けられてELBのノードがそれらの組に紐づいたということだと思う。 各ノードにはDNSラウンドロビンするようになっている。実際restartすると今度は別の組の方に送られた。
では、なぜ一度送り始めると同じ方にしか飛ばないかというと、forwardプラグインのexpire_dns_cacheがデフォルトでnilになっていて、 heartbeatが届いている間は無期限にDNSキャッシュするようになっているため。これに0(キャッシュしない)か秒数を指定すると、 その間隔で他の組のインスタンスにもログが届くようになった。 expire_dns_cacheしなくても複数のインスタンスからラウンドロビンされるため全体でいえば分散される。
heartbeat ELB配下のEC2を全て落としてもheartbeatに失敗しないため、standyに移行せずELBのバックエンド接続エラーになってログがロストしてしまうようだ。 ログにも出ず、以下のようにactive-standbyの設定をしてもstandbyに移行しない。 全てのインスタンスが同時に落ちるというのは滅多に起きないだろうけど、少なくとも検知できるようにはしておく。
&amp;lt;server&amp;gt; name td1 host autoscale-td1.us-east-1.elasticbeanstalk.com port 24224 &amp;lt;/server&amp;gt; &amp;lt;server&amp;gt; name td2 host autoscale-td2.us-east-1.elasticbeanstalk.com port 24224 standby &amp;lt;/server&amp;gt; Blue/Green Deployment Blue-Green Deploymentというのは、2つの系を用意し、activeでない方にデプロイし、 スワップして反映させるもの。ダウンタイムなしで問題が起きた際にもすぐに切り戻すことができる。 スワップして向き先を変えるにはexpire_dns_cacheを設定する必要がある。
Auto Scaling 増えるのはいいとして減るときに、 送り先で一時的に障害が起きていたりするとバッファをflushできずにログがロストする可能性がある。 それでいうとログの送り元でも同じことが起こりうるんだけど、通常Aggregatorにしか送らないので比較的問題になりにくい。
これを避けたい場合、Auto Scalingグループの設定で スケールインから保護を有効にして これから立ち上がるインスタンスはスケールインしなくすることができる。 それまでに立ち上がっていたインスタンスには適用されないので注意。
スケールインしないということは最大の台数で止まってしまうので、 ピークを過ぎたらスワップしてバッファが全て掃けたことを確認してからTerminateすることになる。 これを日常的にやるのは面倒なので、実際は予期しない流量の増加に備えて一応設定しておき、 普段はしきい値にひっかからないような最低台数で待ち構えておくことにするかな。
あとはヘルスチェックによって潰される可能性はなくもないけど、それはもうやむなし・・・。
参考 AWS ELBの社内向け構成ガイドを公開してみる 負荷分散編 – Cross-Zone Routingを踏まえて ｜ Developers.</description>
    </item>
    
    <item>
      <title>fluentdでKinesis streamsに送るときの性能確認</title>
      <link>https://www.sambaiz.net/article/108/</link>
      <pubDate>Mon, 05 Jun 2017 23:48:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/108/</guid>
      <description>localでのstreamsとproducerのbenchmark aws-fluent-plugin-kinesisの make benchmarkはlocalにDummyServerを立ち上げて送っている。
空でもいいのでroleをつけておく必要がある。
$ git clone https://github.com/awslabs/aws-fluent-plugin-kinesis.git $ cd aws-fluent-plugin-kinesis $ yum install -y ruby-devel gcc $ echo &#39;gem &amp;quot;io-console&amp;quot;&#39; &amp;gt;&amp;gt; Gemfile $ make $ make benchmark RATEを指定しなければデフォルトで秒間1000レコードが送られる設定。 fluentdを起動してから10秒後にプロセスをkillし、そのレコード数などを出力している。
t2.microでデフォルト(RATE=1000)で実行した結果がこれ。 固める分producerの方はややパフォーマンスが落ちる。
bundle exec rake benchmark TYPE=streams Results: requets: 20, raw_records: 9400, records: 9400 bundle exec rake benchmark TYPE=producer Results: requets: 14, raw_records: 1005, records: 8900 RATE=3000のとき。producerではraw_recordsが1/100、リクエスト数は1/5。 streamsだとシャードを増やしていく必要があるけど、producerの方は当分大丈夫そうだ。
bundle exec rake benchmark TYPE=streams Results: requets: 57, raw_records: 27600, records: 27600 bundle exec rake benchmark TYPE=producer Results: requets: 12, raw_records: 241, records: 25200 RATE=10000のとき。raw_records, requestの圧縮率はさらに上がり、 パフォーマンスの差が大きくなってきている。</description>
    </item>
    
    <item>
      <title>BeanstalkでのパッケージのバージョンがAMIでのバージョンと異なる原因</title>
      <link>https://www.sambaiz.net/article/106/</link>
      <pubDate>Sun, 04 Jun 2017 23:40:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/106/</guid>
      <description>User-Dataとは EC2インスタンス起動時に、シェルスクリプトを走らせたりcloud-initディレクティブを適用できる機能。 コンソールではインスタンスの詳細の設定の、高度な詳細のところから設定できる。
BeanstalkでのUser-Data 実はBeanstalkでも使われていて、CloudFormationで設定されている。
&amp;quot; /bin/bash /tmp/ebbootstrap.sh &amp;quot;, ... &amp;quot;Fn::FindInMap&amp;quot;: [ &amp;quot;AWSEBOptions&amp;quot;, &amp;quot;options&amp;quot;, &amp;quot;UserDataScript&amp;quot; ] &amp;quot; &amp;gt; /tmp/ebbootstrap.sh &amp;quot;, ... &amp;quot;AWSEBOptions&amp;quot;: { &amp;quot;options&amp;quot;: { &amp;quot;UserDataScript&amp;quot;: &amp;quot;https://s3-ap-northeast-1.amazonaws.com/elasticbeanstalk-env-resources-ap-northeast-1/stalks/eb_node_js_4.0.1.90.2/lib/UserDataScript.sh&amp;quot;, &amp;quot;guid&amp;quot;: &amp;quot;f08557fc43ac&amp;quot;, } } このshellの中では、時計を同期させたり、awsebユーザーを作成したりするほかに、 非Beanstalk AMI(is_baked=false)ではyum updateが走るようになっている。 そのため、AMIでのバージョンとBeanstalkで立ち上がったときのバージョンが異なることがあるようだ。
GUID=$7 function update_yum_packages { if is_baked update_yum_packages_$GUID; then log yum update has already been done. else log Updating yum packages. yum --exclude=aws-cfn-bootstrap update -y || echo Warning: cannot update yum packages. Continue... mark_installed update_yum_packages_$GUID # Update system-release RPM package will reset the .</description>
    </item>
    
    <item>
      <title>Node.jsでの文字コードの変換</title>
      <link>https://www.sambaiz.net/article/89/</link>
      <pubDate>Tue, 28 Mar 2017 21:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/89/</guid>
      <description>node-iconvを使う。
$ npm install iconv SHIFT_JISからUTF-8への変換はこんな感じ。
const Iconv = require(&#39;iconv&#39;).Iconv; const before = new Buffer([ 0x8b, 0x8d, 0x8e, 0x4d, 0x26, 0x82, 0xb2, 0x94, 0xd1 ]); const iconv = new Iconv(&#39;SHIFT_JIS&#39;, &#39;UTF-8&#39;); console.log(`before: ${before.toString(&#39;hex&#39;)} ${before.toString()}`) const after = iconv.convert(before); console.log(`after: ${after.toString(&#39;hex&#39;)} ${after.toString()}`); before: 8b8d8e4d2682b294d1 ���M&amp;amp;���� after: e7899be79abf26e38194e9a3af 牛皿&amp;amp;ご飯 文字コードによっては変換後に表せないことがある。 例えば、UTF-8からSHIFT_JISへの変換でサロゲートペア🍚を渡すと変換できず、エラーになる。
throw errnoException(&#39;EILSEQ&#39;, &#39;Illegal character sequence.&#39;); //IGNOREを付けることで そのような文字があった場合でもエラーにしないようにできる。
const Iconv = require(&#39;iconv&#39;).Iconv; const before = &amp;quot;牛皿&amp;amp;🍚&amp;quot;; const iconv = new Iconv(&#39;UTF-8&#39;, &#39;SHIFT_JIS//IGNORE&#39;); console.</description>
    </item>
    
    <item>
      <title>FluentdとKPL(Kinesis Producer Library)でログをまとめてスループットを稼ぐ</title>
      <link>https://www.sambaiz.net/article/84/</link>
      <pubDate>Wed, 15 Mar 2017 23:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/84/</guid>
      <description>KPL(Kinesis Producer Library)とは Developing Amazon Kinesis Streams Producers Using the Amazon Kinesis Producer Library - Amazon Kinesis Streams
Kinesisに送るとき、自動リトライしてくれたり、レコードをまとめてスループットを向上してくれたりするアプリケーション。Protobufを使っている。 普通に送るとどんなに小さくてもシャード*1000レコード/秒しか最大でPUTできないのを、KPLを使ってまとめることで増やすことができる。
fluentdで送る aws-fluent-plugin-kinesisでkinesis_producerを指定するとKPLを使って送信する。
&amp;lt;kinesis_producer&amp;gt;の中にKPLの設定を書くことができる。
&amp;lt;kinesis_producer&amp;gt; record_max_buffered_time 10 &amp;lt;/kinesis_producer&amp;gt; record_max_bufferd_time はバッファされたレコードが送られるまでの最大時間(ms)。デフォルトは100ms。この時間が経つか、他のリミットに当たったらレコードは送られる。
 AggregationMaxCount: 一つのレコードにまとめる最大レコード数 AggregationMaxSize: まとめたレコードの最大バイト数 CollectionMaxCount: PutRecordsで送る最大アイテム数 CollectionMaxSize: PutRecordsで送るデータ量  CloudWatchに送るmetrics_levelはデフォルトでdetailedになっていて、 コンソールのメトリクスからstream名で検索すると KinesisProducerLibraryにUserRecordsPerKinesisRecordや、UserRecordsDataPut、BufferingTime、RequestTimeなどいろいろ表示される。
とりあえず試しにこんな設定で送ってみる。
&amp;lt;match hoge.log&amp;gt; @type kinesis_producer region ap-northeast-1 stream_name teststream include_time_key true flush_interval 1 buffer_chunk_limit 1m try_flush_interval 0.1 queued_chunk_flush_interval 0.01 num_threads 15 &amp;lt;/match&amp;gt; Lambdaで読む まとめられたレコードをkinesis-aggregationで分解して読む。 今回はNode.jsでやる。
$ npm install --save aws-kinesis-agg 注意する必要があるのはドキュメントの情報が古くて、 関数の引数が足りないこと。第二引数のcomputeChecksumsが抜けているので気付かないと一つずつずれていくことになる。</description>
    </item>
    
    <item>
      <title>fluentdでKinesis Streamsに送ってLambdaで読んでS3に保存する</title>
      <link>https://www.sambaiz.net/article/73/</link>
      <pubDate>Sun, 26 Feb 2017 18:56:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/73/</guid>
      <description>aws-fluent-plugin-kinesisでKinesis Streamsに送り、Lambdaで読んでS3に保存する。 要するにFirehoseのようなことをやりたいのだけれどTokyoリージョンにまだ来ないので自分でやる。
fluentdで送る $ td-agent-gem install fluent-plugin-kinesis try_flush_intervalとqueued_chunk_flush_intervalはドキュメントには載っていないが、 以下のページによるとそれぞれqueueに次のchunkがないときとあるときのflushする間隔。 いずれもデフォルトは1だが、これを減らすことでもっと頻繁に吐き出されるようになるらしい。
Fluentd の out_forward と BufferedOutput
あとシャードに振り分けるためのpartition_key を指定できる。デフォルトはランダム。
&amp;lt;source&amp;gt; @type tail path /var/log/td-agent/hoge.log pos_file /etc/td-agent/log.pos tag hoge.log format json time_key timestamp # 2017-01-01T01:01:01+0900 time_format %Y-%m-%dT%H:%M:%S%z &amp;lt;/source&amp;gt; &amp;lt;match hoge.log&amp;gt; @type kinesis_streams region ap-northeast-1 stream_name teststream include_time_key true flush_interval 1 buffer_chunk_limit 1m try_flush_interval 0.1 queued_chunk_flush_interval 0.01 num_threads 15 &amp;lt;/match&amp;gt; いくつか送ってみる。
for i in `seq 1 1000` do echo &#39;{&amp;quot;hoge&amp;quot;: &amp;quot;fuga&amp;quot;, &amp;quot;timestamp&amp;quot;: &amp;quot;2017-01-01T01:01:01+0900&amp;quot;}&#39; &amp;gt;&amp;gt; /var/log/td-agent/hoge.</description>
    </item>
    
    <item>
      <title>AWSのAssumeRole</title>
      <link>https://www.sambaiz.net/article/72/</link>
      <pubDate>Sat, 25 Feb 2017 20:40:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/72/</guid>
      <description>AWS Security Token Serviceによる、 RoleArn(arn:aws:iam::&amp;lt;account id&amp;gt;:role/&amp;lt;role name&amp;gt;)から一時的なCredentialを取得する仕組み。 前もって発行したAPIキーとは違い、有効期限が存在するため続けて呼ぶ場合は失効する前に再発行する必要がある。
ではRoleArnを知っていたら誰でも取得できるかというと、もちろんそうではなく、 ロールの信頼関係、&amp;quot;Action&amp;quot;: &amp;quot;sts:AssumeRole&amp;quot;のPrincipalのところで信頼する対象を設定する。 例えば、Serviceでec2.amazonaws.comを指定してEC2がAssumeRoleするのを許可したり、 AWSで(他の)アカウントやユーザーを指定してそのAPIキーでこのRoleのCredentialを取得できるようにしたりといった感じ。
{ &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;, &amp;quot;Statement&amp;quot;: [ { &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;, &amp;quot;Principal&amp;quot;: { &amp;quot;Service&amp;quot;: &amp;quot;ec2.amazonaws.com&amp;quot; }, &amp;quot;Action&amp;quot;: &amp;quot;sts:AssumeRole&amp;quot; } ] } EC2にロールを設定すると、実はそのロールについてAssumeRoleして自動でCredentialを取得している。 EC2にロールを設定するにはロールとは別に インスタンスプロファイルを作成 する必要があるが、コンソールでEC2のサービスロールを作ると同名のインスタンスプロファイルが自動で作成される。 さらに、AssumeRoleのServiceとしてec2.amazonaws.comが追加されている。
$ curl http://169.254.169.254/latest/meta-data/iam/info { &amp;quot;Code&amp;quot; : &amp;quot;Success&amp;quot;, &amp;quot;LastUpdated&amp;quot; : &amp;quot;2017-02-25T10:56:33Z&amp;quot;, &amp;quot;InstanceProfileArn&amp;quot; : &amp;quot;arn:aws:iam::*****:instance-profile/assume_role_test&amp;quot;, &amp;quot;InstanceProfileId&amp;quot; : &amp;quot;*****&amp;quot; } $ curl http://169.254.169.254/latest/meta-data/iam/security-credentials/assume_role_test { &amp;quot;Code&amp;quot; : &amp;quot;Success&amp;quot;, &amp;quot;LastUpdated&amp;quot; : &amp;quot;2017-02-25T10:56:23Z&amp;quot;, &amp;quot;Type&amp;quot; : &amp;quot;AWS-HMAC&amp;quot;, &amp;quot;AccessKeyId&amp;quot; : &amp;quot;*****&amp;quot;, &amp;quot;SecretAccessKey&amp;quot; : &amp;quot;*****&amp;quot;, &amp;quot;Token&amp;quot; : &amp;quot;*****&amp;quot;, &amp;quot;Expiration&amp;quot; : &amp;quot;2017-02-25T17:26:07Z&amp;quot; } 参考 IAMロール徹底理解 〜 AssumeRoleの正体 ｜ Developers.</description>
    </item>
    
    <item>
      <title>ELBのスケーリングとsurge queue</title>
      <link>https://www.sambaiz.net/article/68/</link>
      <pubDate>Tue, 21 Feb 2017 19:48:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/68/</guid>
      <description>バックエンドだけではなくELB自体もスケーリングし、内部node数はdigで調べることができる。 このnode数は自分ではコントロールできず、基本的に意識することはない。
$ dig ****.ap-northeast-1.elb.amazonaws.com ;; ANSWER SECTION: *****.elb.amazonaws.com. 60 IN A xxx.xxx.xxx.xxx *****.elb.amazonaws.com. 60 IN A yyy.yyy.yyy.yyy nodeが増えるのにはある程度時間がかかるので、 アクセスが急増(5分間で50%以上のトラフィック増加が目安) したら捌ききれず、503を返すことがある。 前もって多量のアクセスが来ることが分かっていて、 AWSサポートがBusiness以上なら pre-warming申請することでnodeが増えた状態で待ち構えられる。
バックエンドのアプリケーションがリクエストを処理できない場合、ELBのsurge queueに溜まっていく。 この数はCloudWatchのSurgeQueueLength(キュー長の急増)メトリクスで確認できる。 また、SurgeQueueLengthの最大値1024を超えるとリクエストは拒否され、その数はSpoiloverCount(過剰数)メトリクスに出る。
参考 ELBの挙動とCloudWatchメトリクスの読み方を徹底的に理解する ｜ Developers.IO
Elastic Load Balancing でのレイテンシーのトラブルシューティング</description>
    </item>
    
    <item>
      <title>Kinesis Streams/Firehose/Analyticsを試す</title>
      <link>https://www.sambaiz.net/article/67/</link>
      <pubDate>Mon, 20 Feb 2017 21:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/67/</guid>
      <description>https://aws.amazon.com/jp/kinesis/
リアルタイムのストリーミングデータを扱うサービス群。 いまのところTokyoリージョンではKinesis Streamsしか使えない。
Kinesis Firehose AWSのデータストアに送るストリーム。自分でデータを読む処理を書かなくてよく、スケーリングも勝手にやってくれるので簡単に使える。
https://aws.amazon.com/jp/kinesis/firehose/faqs/
Q: 送信先とは何ですか? 送信先はデータが配信されるデータストアです。Amazon Kinesis Firehose では、 現在送信先として Amazon S3、Amazon Redshift、Amazon Elasticsearch Service がサポートされています。 料金は取り込まれたデータ量による。 一見そんなに高くならないように見えるが、5KB単位で切り上げられるのでレコードのサイズが小さくて数が多い場合に注意が必要。
今回はS3に送ってみる。
圧縮方法を設定したり、Lambdaを噛ませたりすることができる。
StatusがActiveになったらKinesis Agentで送ってみる。 CloudWatchとFirehoseにPutする権限が必要。Firehoseはkinesis:ではなくfirehose:なので注意。
$ sudo yum install –y aws-kinesis-agent /etc/aws-kinesis/agent.jsonを編集する。リージョンごとのエンドポイントは ここ にある。
{ &amp;quot;awsAccessKeyId&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;awsSecretAccessKey&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;firehose.endpoint&amp;quot;: &amp;quot;https://firehose.us-east-1.amazonaws.com&amp;quot;, &amp;quot;flows&amp;quot;: [ { &amp;quot;filePattern&amp;quot;: &amp;quot;/tmp/hoge.log&amp;quot;, &amp;quot;deliveryStream&amp;quot;: &amp;quot;hogefugastream&amp;quot; } ] } $ sudo service aws-kinesis-agent start $ sudo chkconfig aws-kinesis-agent on $ echo &amp;quot;aaa&amp;quot; &amp;gt;&amp;gt; /tmp/hoge.log $ tail /var/log/aws-kinesis-agent/aws-kinesis-agent.</description>
    </item>
    
    <item>
      <title>GoでDynamoDBを使う</title>
      <link>https://www.sambaiz.net/article/63/</link>
      <pubDate>Sun, 12 Feb 2017 23:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/63/</guid>
      <description>テーブルを作成する プライマリキー テーブルの操作のガイドライン - Amazon DynamoDB
プライマリキーとしてパーティションキー(ハッシュキー)とオプションのソートキー(レンジキー)を設定する。 DynamoDBはこのパーティションキーに基づいて、複数のパーティションに分散して保存する。 テーブルにプロビジョニングされたスループット性能はパーティション間で均等に使われるので、 ソートキーを設定する場合にこれを最大限に活用するためには、 あるパーティションにリクエストが集中しないよう、パーティションキーに特定の値ばかり集中しないようなフィールドを 選ぶ必要がある。
セカンダリインデックス パーティションキーのグローバルセカンダリインデックス(GSI)と ソートキーのローカルセカンダリインデックス(LSI)がある。 射影されるフィールドを選択でき、ここに含まれないフィールドは返さない。 ただし、すべてをインデックスに書き込むのはコストが高いのでなるべく絞る。
キャパシティユニット  1読み込みキャパシティユニット: 4kbを超えないデータを1秒に1~2回(整合性による)読み込める 1書き込みキャパシティユニット: 1kbを超えないデータを1秒に1回書き込める  ユニットに応じて1時間あたりで課金される。
未使用のキャパシティがある場合、最大5分保持してバーストに備えてくれる。
読み書きする aws-sdk-goを直接使ってもいいけど、簡単に扱えるラッパー guregu/dynamo を使うことにした。
type Data struct { ID int64 `dynamo:&amp;quot;id&amp;quot;` Name string Age int } db := dynamo.New(session.New(), &amp;amp;aws.Config{Region: aws.String(&amp;quot;ap-northeast-1&amp;quot;)}) table := db.Table(&amp;quot;testtable&amp;quot;) Create &amp;amp; Update d := Data{ID: 1, Name: &amp;quot;hogefuga&amp;quot;, Age: 123} if err := table.Put(d).Run(); err != nil { return err } if err := table.</description>
    </item>
    
    <item>
      <title>EC2のインスタンスストア</title>
      <link>https://www.sambaiz.net/article/58/</link>
      <pubDate>Mon, 06 Feb 2017 21:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/58/</guid>
      <description>http://docs.aws.amazon.com/ja_jp/AWSEC2/latest/UserGuide/InstanceStorage.html
EC2ではインスタンスタイプによってはEBSに加えてインスタンスストアが使える。しかも追加料金なし。 対象はストレージが&amp;quot;EBSのみ&amp;quot;でないもの。
https://aws.amazon.com/jp/ec2/instance-types/
インスタンスストアはインスタンスが停止したり、障害が起きると消える一時ストレージ。再起動では消えない。 ホストに物理的にアタッチされているので、バッファやキャッシュなどの頻繁に読み書きされ、消えてもいいデータに最適。 他のインスタンスにアタッチすることはできない。容量や性能もインスタンスタイプに依存する。
インスタンスストアボリュームの追加は インスタンスの起動時に、新しいボリュームを追加し、ボリュームタイプをインスタンスストアにすることで行うことができる。
今回はSSDストレージ1 x 4のm3.mediumで試す。これは4gbのボリュームが一つ追加できるという意味。
まずはインスタンスストアを追加してないインスタンス。 lsblkというのはlist block devicesの略。
$ df -h ファイルシス サイズ 使用 残り 使用% マウント位置 /dev/xvda1 7.8G 1.2G 6.6G 15% / ... $ dd if=/dev/zero of=hoge bs=1M count=1000 $ ls -sh 合計 1001M 1001M hoge $ df -h ファイルシス サイズ 使用 残り 使用% マウント位置 /dev/xvda1 7.8G 2.2G 5.6G 28% / ... $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk └─xvda1 202:1 0 8G 0 part / それに対してインスタンスストア(/dev/xvdb)を追加したインスタンス。</description>
    </item>
    
    <item>
      <title>複数EC2インスタンスを立ち上げてvegetaで負荷試験する</title>
      <link>https://www.sambaiz.net/article/43/</link>
      <pubDate>Sun, 18 Dec 2016 20:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/43/</guid>
      <description>vegetaで負荷をかける。
インスタンスを立ち上げるスクリプト コードはここ。 sambaiz/loadtest
まずキーペアを作成し、EC2インスタンスを立ち上げて、全てのインスタンスが使えるようになるまで待つ。
aws ec2 create-key-pair --key-name LoadTestKeyPare --query &#39;KeyMaterial&#39; --output text &amp;gt; LoadTestKeyPare.pem chmod 400 LoadTestKeyPare.pem aws ec2 run-instances --image-id $AMI_ID --count $INSTANCE_NUM --instance-type t2.micro --key-name LoadTestKeyPare --security-group-ids $SECURITY_GROUP_IDS --subnet-id $SUBNET_ID ... aws ec2 wait instance-status-ok --instance-ids $INSTANCE_IDS このAMIは事前にPackerでつくったもの。vegetaをインストールしてファイルディスクリプタの上限を増やしている。
{ &amp;quot;variables&amp;quot;: { &amp;quot;aws_access_key&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;aws_secret_key&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `aws_access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `aws_secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;ap-northeast-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-0c11b26d&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.micro&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ec2-user&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;loadtest {{timestamp}}&amp;quot; }], &amp;quot;provisioners&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;, &amp;quot;inline&amp;quot;: [ &amp;quot;wget https://github.</description>
    </item>
    
    <item>
      <title>PackerでAMIを作る</title>
      <link>https://www.sambaiz.net/article/24/</link>
      <pubDate>Tue, 18 Oct 2016 22:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/24/</guid>
      <description>https://www.packer.io/
いろんなプラットフォームのイメージを作ることができるツール。 これでfluentdのログサーバーのAMIを作る。
$ brew install packer # mac $ packer -v 0.10.1 設定ファイルはこんな感じ。variablesの値は{{user ... }}のところで使われる。 buildersに作るイメージの情報を書いて、provisionersで環境を作る。
provisionersにはchefやansibleなども指定できるが、 継ぎ足し継ぎ足しで秘伝のタレ化したAMIも最初は、コマンドいくつか実行するだけなのでとりあえず手作業で作った、後でなんとかするなんてものもあったりして、 そういうものは無理にchefなどで始めず、手軽にshellでpacker buildするといいと思う。 手作業よりも楽だしソースが別にあるので使われていないAMIを消すのも簡単。
fileではpermissionがないところに置くことができないので、一旦置いてshellで移動する。
{ &amp;quot;variables&amp;quot;: { &amp;quot;aws_access_key&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;aws_secret_key&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `aws_access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `aws_secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;ap-northeast-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-1a15c77b&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.small&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ec2-user&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;fluentd-logserver {{timestamp}}&amp;quot; }], &amp;quot;provisioners&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;file&amp;quot;, &amp;quot;source&amp;quot;: &amp;quot;td-agent.conf&amp;quot;, &amp;quot;destination&amp;quot;: &amp;quot;/home/ec2-user/td-agent.conf&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;, &amp;quot;inline&amp;quot;: [ &amp;quot;curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent2.sh | sh&amp;quot;, &amp;quot;sudo mv /home/ec2-user/td-agent.</description>
    </item>
    
  </channel>
</rss>
