<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Aws on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/aws/</link>
    <description>Recent content in Aws on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Sun, 03 Feb 2019 17:31:00 +0900</lastBuildDate>
    
	<atom:link href="https://www.sambaiz.net/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Serverless FrameworkでVPCを作成してLambdaをデプロイしAurora Serverlessを使う</title>
      <link>https://www.sambaiz.net/article/206/</link>
      <pubDate>Sun, 03 Feb 2019 17:31:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/206/</guid>
      <description>Aurora ServerlessはオートスケールするAuroraで、 使ったAurora Capacity Unit (ACU)によって料金が発生するため、 使用頻度が少なかったり変動するアプリケーションにおいて安くRDBを使うことができる。 インスタンスを立てると最低でも月3000円くらいかかるが、Serverlessだとほとんどストレージ分から運用することができて趣味でも使いやすい。 ただしLambdaと同様に常に同等のリソースを使っている状態だとインスタンスと比べて割高になる。
今回はLambdaで使う。 Serverlessと名前には付いているが用途としてはLambdaに限らず、 むしろコンテナの数が容易に増え得るLambdaは同時接続数が問題になるRDBと一般に相性が良くない。 現在Betaの、コネクションを張らずにHTTPSでクエリを投げられるData APIはこの問題を解消すると思われるが、トランザクションが張れなかったり、レスポンスサイズに制限があるようだ。今回はコンソール上から初期クエリを流すためにData APIを有効にしている。
他の選択肢として、DynamoDBは現状最有力で最近トランザクションもサポートされたがSQLのように柔軟なクエリは投げられない。 Athenaはクエリは投げられるがそこそこ時間がかかるし、INSERTはできずクエリごとに料金が発生する。
Serverless Frameworkを使ってリソースを作成しデプロイする。リポジトリはここ。
Serverless FrameworkでLambdaをデプロイする - sambaiz-net
VPCの作成 Aurora Serverlessの制限の一つとしてVPC内からしか接続しかできないというものがある。ということでVPCから作成していく。以前Terraformで作ったのと同じリソースをCloudFormationで作る。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
LambdaをVPC内で動かすとコンテナ起動時にENIも作成するため立ち上がりの際時間がかかる。必要なら定期的に呼び出して削除されないようにする。 また、今回はテストのため/24でVPCを切っているが、小さいとENIのIPアドレスが枯渇する可能性がある。
 VPC  TestVPC: Type: AWS::EC2::VPC Properties: CidrBlock: 172.32.0.0/24 Tags: - Key: Name Value: test-vpc   Subnet  Aurora Serverlessのために少なくとも2つのサブネットが必要。
TestPublicSubnet: Type: AWS::EC2::Subnet Properties: VpcId: !Ref TestVPC CidrBlock: 172.32.0.0/25 AvailabilityZone: us-east-1d Tags: - Key: Name Value: test-public-subnet1 TestPrivateSubnet1: Type: AWS::EC2::Subnet Properties: VpcId: !</description>
    </item>
    
    <item>
      <title>AWS Systems Manager (SSM)のParameter Storeに認証情報を置き参照する</title>
      <link>https://www.sambaiz.net/article/204/</link>
      <pubDate>Mon, 07 Jan 2019 23:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/204/</guid>
      <description>DBのパスワードやAPIトークンといった認証情報をバージョン管理するコードや設定ファイル上に書くとOSS化など公開範囲を広げるときにやや困るし漏れるリスクが高まるのでなるべく避けたい。 そこでSSMのParameter Storeに値を置き、実行時やデプロイ時に参照する。
SSMのParameter StoreとSecrets Manager Systems Manager (SSM)はAWSのリソースを可視化したり操作を自動化したりするサービス群で、 設定を持つParameter Storeはその一つ。値は暗号化して持つこともできる。 料金はかからない。
SSMのParameter Storeと似たような別のAWSのサービスに Secrets Managerというのがあって、 こちらはLambdaによって定期的に新しい値を生成しローテーションさせることができる。特にRDSなら自前の関数を用意する必要もない。 ただし料金がシークレットの件数($0.4/月)とAPIコール($0.05/10000回)でかかる。
今はParamter StoreとSecrets Managerが統合されていて、Parameter StoreのAPIでどちらも参照できるようだ。 今回はローテーションしないので単純に料金がかからないParameter Storeの方に書き込むことにする。 ただし、Parameter Storeは現状一度に大量のリクエストが飛ぶような使い方をするとRate exceededになってしまう問題がある。
実行時の値取得 実行時に値を取得するのはこんな感じ。
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;github.com/aws/aws-sdk-go/aws&amp;quot; &amp;quot;github.com/aws/aws-sdk-go/aws/awserr&amp;quot; &amp;quot;github.com/aws/aws-sdk-go/aws/session&amp;quot; &amp;quot;github.com/aws/aws-sdk-go/service/ssm&amp;quot; ) type Parameter struct { ssm *ssm.SSM } func newParameter(sess *session.Session) *Parameter { return &amp;amp;Parameter{ ssm: ssm.New(sess), } } func (s *Parameter) Get(name string, decrypt bool) (string, error) { param, err := s.</description>
    </item>
    
    <item>
      <title>AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する</title>
      <link>https://www.sambaiz.net/article/203/</link>
      <pubDate>Tue, 01 Jan 2019 17:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/203/</guid>
      <description>AWS GlueはマネージドなETL(Extract/Transform/Load)サービスで、Sparkを使ってS3などにあるデータを読み込み加工して変換出力したり、AthenaやRedshift Spectrumで参照できるデータカタログを提供する。 今回はS3のCSVを読み込んで加工し、列指向フォーマットParquetに変換しパーティションを切って出力、その後クローラを回してデータカタログにテーブルを作成してAthenaで参照できることを確認する。
料金はジョブがDPU(4vCPU/16GBメモリ)時間あたり$0.44(最低2DPU/10分)かかる。 また、クローラも同様にDPUで課金される。
開発用エンドポイント ジョブの立ち上がりにやや時間がかかるため開発用エンドポイントを立ち上げておくとDPUが確保されて効率よく開発できる。 立ち上げている間のDPUの料金がかかる。つまりずっとジョブを実行し続けているようなもので結構高くつくので終わったら閉じるようにしたい。
ローカルやEC2から自分で開発用エンドポイントにsshしてNotebookを立てることもできるが、 コンソールから立ち上げたNotebookは最初からつながっていて鍵の登録も必要なくて楽。
ssh -i private-key-file-path -NTL 9007:169.254.76.1:9007 glue@dev-endpoint-public-dns  NotebookのインスタンスのIAMロールを作成するとCloudwatch Logsまわりに加えてGlueのDevEndpointとAssetの取得権限が付与されている。
{ &amp;quot;Action&amp;quot;: [ &amp;quot;s3:ListBucket&amp;quot; ], &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;, &amp;quot;Resource&amp;quot;: [ &amp;quot;arn:aws:s3:::aws-glue-jes-prod-us-east-1-assets&amp;quot; ] } { &amp;quot;Action&amp;quot;: [ &amp;quot;s3:GetObject&amp;quot; ], &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;, &amp;quot;Resource&amp;quot;: [ &amp;quot;arn:aws:s3:::aws-glue-jes-prod-us-east-1-assets*&amp;quot; ] } { &amp;quot;Action&amp;quot;: [ &amp;quot;glue:UpdateDevEndpoint&amp;quot;, &amp;quot;glue:GetDevEndpoint&amp;quot;, &amp;quot;glue:GetDevEndpoints&amp;quot; ], &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;, &amp;quot;Resource&amp;quot;: [ &amp;quot;arn:aws:glue:us-east-1:*****:devEndpoint/test*&amp;quot; ] }  たまに立ち上げに失敗したり次のようなエラーで実行できないことがあったが、立ち上げ直したらうまくいった。
The code failed because of a fatal error: Error sending http request and maximum retry encountered.</description>
    </item>
    
    <item>
      <title>FargateでECSを使う</title>
      <link>https://www.sambaiz.net/article/196/</link>
      <pubDate>Fri, 09 Nov 2018 00:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/196/</guid>
      <description>ECSはAWSのコンテナオーケストレーションサービス。 クラスタはEC2上に立てることもできるが、その場合Auto Scalingグループの設定やスケールイン時のdrainなどを考慮する必要がある。 Fargateで起動するとサーバーレスで実行でき、バックエンドの管理が必要がなくなる。 料金は割り当てたvCPUとメモリによって、最低1分の1秒単位で課金される。 Lambdaと同じくリソースあたりでいうとオンデマンドのEC2と比較して割高。ただし柔軟にリソースが指定できる分いくらか差は縮まる。
特にバッチ処理のように常にリソースが必要ないTaskは都度インスタンスを立ち上げるのも面倒なので良いと思う。 Lambdaと比較すると、実行環境を自由に作れるのと実行時間に制限がないというところが良いが、 Taskを作るトリガーは現状cronだけなのでそれ以外のイベントで実行したい場合はLambdaと組み合わせる必要がある。
AWSにはKubernetesクラスタを立てられるEKSもあるが、こちらはまだFargateに対応していない。 もしかしたら今月末のre:Inventで何か発表されるかもしれない。
Clusterの作成 まずはClusterを作成する。
$ aws ecs create-cluster --cluster-name test  Taskの登録 イメージやポートマッピング、ヘルスチェックや割り当てるリソースといったContainer definition を含むTask definitionを書く。 Fargateの場合networkModeはawsvpc固定になる。
{ &amp;quot;family&amp;quot;: &amp;quot;test-task&amp;quot;, &amp;quot;networkMode&amp;quot;: &amp;quot;awsvpc&amp;quot;, &amp;quot;containerDefinitions&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;nginx&amp;quot;, &amp;quot;image&amp;quot;: &amp;quot;nginx:1.15&amp;quot;, &amp;quot;portMappings&amp;quot;: [ { &amp;quot;containerPort&amp;quot;: 80, &amp;quot;hostPort&amp;quot;: 80, &amp;quot;protocol&amp;quot;: &amp;quot;tcp&amp;quot; } ], &amp;quot;essential&amp;quot;: true } ], &amp;quot;requiresCompatibilities&amp;quot;: [ &amp;quot;FARGATE&amp;quot; ], &amp;quot;cpu&amp;quot;: &amp;quot;256&amp;quot;, &amp;quot;memory&amp;quot;: &amp;quot;512&amp;quot; }  Taskを登録する。
$ aws ecs register-task-definition --cli-input-json file://$(pwd)/task.</description>
    </item>
    
    <item>
      <title>Macでの開発環境構築メモ</title>
      <link>https://www.sambaiz.net/article/163/</link>
      <pubDate>Sat, 14 Apr 2018 14:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/163/</guid>
      <description>新しいMBPを買ったので開発環境の構築でやったことを残しておく
設定  アクセシビリティから3本指スクロールを有効にする ホットコーナーの左上にLaunchPad、右上にデスクトップを割り当てている 画面をなるべく広く使うためにDockは左に置いて自動的に隠す  bash_profile パッケージマネージャ以外で持ってきたバイナリは${HOME}/binに置くことにする。
touch ~/.bash_profile mkdir ${HOME}/bin echo &amp;quot;export PATH=\$PATH:${HOME}/bin&amp;quot; &amp;gt;&amp;gt; ~/.bash_profile  HomeBrew &amp;amp; Cask /usr/bin/ruby -e &amp;quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;quot; brew tap caskroom/cask  一般的なアプリケーション/コマンドのインストール XcodeとUnityとLINEは手動で入れる。
brew cask install google-chrome kap visual-studio-code slack kindle brew install jq gibo mysql wget  Git git config --global user.name sambaiz git config --global user.email godgourd@gmail.com  Docker &amp;amp; K8s brew cask install docker virtualbox minikube brew install docker kubernetes-helm  fish bash前提で書かれたスクリプトも多いので、デフォルトシェルにはしない。</description>
    </item>
    
    <item>
      <title>Cognito UserPoolとAPI Gatewayで認証付きAPIを立てる</title>
      <link>https://www.sambaiz.net/article/157/</link>
      <pubDate>Sun, 25 Feb 2018 23:46:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/157/</guid>
      <description>UserPoolを作成。デフォルト設定はこんな感じ。 必須項目や、確認メールの文面などを自由にカスタマイズでき、 登録時などのタイミングでLambdaを発火させることもできる。
作成したUserPoolにアプリクライアントを追加する。 ブラウザで使うのでクライアントシークレットはなし。
クライアント側 amazon-cognito-identity-jsを使う。
依存するjsを持ってくる。
$ wget https://raw.githubusercontent.com/aws/amazon-cognito-identity-js/master/dist/amazon-cognito-identity.min.js $ wget https://raw.githubusercontent.com/aws/amazon-cognito-identity-js/master/dist/aws-cognito-sdk.min.js  Sign UpからAPIを呼ぶところまでのボタンを並べた。 SignInするとOIDC標準のトークンがそのページのドメインのLocal Storageに書かれる。
OpenID ConnectのIDトークンの内容と検証 - sambaiz-net
CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.idToken CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.accessToken CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.refreshToken CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.clockDrift CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.LastAuthUser  APIを呼ぶときはidTokenをAuthorization Headerに乗せる。
&amp;lt;button id=&amp;quot;signUp&amp;quot;&amp;gt;Sign Up&amp;lt;/button&amp;gt; &amp;lt;p&amp;gt;&amp;lt;label&amp;gt;Code:&amp;lt;input type=&amp;quot;text&amp;quot; id=&amp;quot;code&amp;quot;&amp;gt;&amp;lt;/label&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;button id=&amp;quot;confirm&amp;quot;&amp;gt;Confirm&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;quot;signIn&amp;quot;&amp;gt;Sign In&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;quot;whoAmI&amp;quot;&amp;gt;Who am I?&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;quot;requestAPI&amp;quot;&amp;gt;Request API with token&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;quot;signOut&amp;quot;&amp;gt;Sign Out&amp;lt;/button&amp;gt; &amp;lt;script src=&amp;quot;aws-cognito-sdk.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script src=&amp;quot;amazon-cognito-identity.min.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script&amp;gt; const USER_NAME = &amp;quot;*****&amp;quot;; const USER_PASSWORD = &amp;quot;*****&amp;quot;; const USER_EMAIL = &amp;quot;*****&amp;quot;; class CognitoUserPoolAuth { constructor(UserPoolId, clientId, apiEndpoint) { const poolData = { UserPoolId : UserPoolId, ClientId : clientId }; this.</description>
    </item>
    
    <item>
      <title>Serverless FrameworkでLambdaをデプロイする</title>
      <link>https://www.sambaiz.net/article/155/</link>
      <pubDate>Sun, 11 Feb 2018 23:20:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/155/</guid>
      <description>Serverless FrameworkでLambda Functionをデプロイする。 Apexが基本的にFunction自体のデプロイしかしないのに対して、こちらはeventの設定なども諸々やってくれて強い。
ApexでLambdaをデプロイする - sambaiz-net
$ npm install -g serverless $ serverless version 1.26.0  ApexではFunctionごとにディレクトリが作られたが、ServerlessではServiceごとに作られ、 一つのService内で複数のFunctionを定義できる。handlerは同じでも異なっていてもよい。
Apexの形式の場合、共通の処理をWebpackなどで各Functionに持って来たり、 同じような処理の複数のFunctionを立てる際はコピーする必要があったが、 こちらは必要最小限の変更でそれらを行うことができる。
templateからServiceをcreateする。
$ serverless create --template aws-nodejs --path testservice $ ls testservice/ handler.js	serverless.yml  設定ファイルserverless.yml にはLambdaの基本的なもののほかに、VPCやevent、IAM Roleなども書けて、これらはdeploy時に作られるCloudFormationのstackによって管理される。必要なら生のCloudFormationの設定も書ける。
ApexでもTerraformによって管理することができるが、書くのはこちらの方がはるかに楽。
ApexでデプロイしたLambdaのトリガーをTerraformで管理する - sambaiz-net
$ cat sesrverless.yml service: testservice provider: name: aws profile: foobar region: ap-northeast-1 runtime: nodejs6.10 memorySize: 512 timeout: 10 functions: hello: handler: handler.hello events: - http: path: hello/world method: get cors: true  deployすると{service}-{stage}-{function}のFunctionが作られる。 今回の場合はtestservice-prd-test。stageをymlでも指定しなかった場合はデフォルト値のdevになる。</description>
    </item>
    
    <item>
      <title>DatadogのLambda Integrationで気象データを送ってアラートを飛ばす</title>
      <link>https://www.sambaiz.net/article/152/</link>
      <pubDate>Mon, 05 Feb 2018 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/152/</guid>
      <description>最近朝が寒くて布団から出るのがつらい。雨や雪が降っていたら尚更のこと、それならそれで現状を把握する必要がある。 そこで、無料から使える気象API OpenWeatherMapのデータをdatadogに送って、特に寒い日や雨雪が降るような朝にはアラートを飛ばすことにした。
インスタンスが立っていたらDataDog AgentのDogStatsD経由で送ることができ、 そうでなければ通常はAPIを呼ぶことになるんだけど、Lambdaでは、AWS Integrationを設定すると有効になるLambda Integrationによって MONITORING|unix_epoch_timestamp|value|metric_type|my.metric.name|#tag1:value,tag2のフォーマットでconsole.logするだけでメトリクスが送られるようになっている。
const axios = require(&#39;axios&#39;); const CITY = &#39;Shibuya&#39;; const API_KEY = &#39;*****&#39;; const WEATHER_API = `http://api.openweathermap.org/data/2.5/weather?q=${CITY}&amp;amp;units=metric&amp;amp;appid=${API_KEY}`; const METRIC_COUNTER = &#39;counter&#39;; const METRIC_GAUGE = &#39;gauge&#39;; const monitor = (metricName, metricType, value, tags) =&amp;gt; { const unixEpochTimestamp = Math.floor(new Date().getTime()); console.log(`MONITORING|${unixEpochTimestamp}|${value}|${metricType}|${metricName}|#${tags.join(&#39;,&#39;)}`); }; exports.handler = async (event, context, callback) =&amp;gt; { const data = (await axios.get(WEATHER_API)).data const namePrefix = &#39;livinginfo.weather&#39; monitor(`${namePrefix}.temperature`, METRIC_GAUGE, data.main.temp, []) monitor(`${namePrefix}.</description>
    </item>
    
    <item>
      <title>Athenaのmigrationやpartitionするathena-adminを作った</title>
      <link>https://www.sambaiz.net/article/145/</link>
      <pubDate>Sun, 24 Dec 2017 23:31:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/145/</guid>
      <description>https://github.com/sambaiz/athena-admin
AthenaはS3をデータソースとするマネージドなデータ分析基盤。Prestoベースで標準SQLを実行できる。
料金はスキャンしたデータ量にかかり、$5/TB。1MB切り上げで、10MB以下のクエリは10MBになる。 データ量に対してかなり安く使えるものの、フルスキャンしてしまうとBigQueryと同様にお金が溶けてしまうので、大抵はパーティションを切ることになるのだけど都度locationを指定してADD PARTITIONを実行するのは大変。さらにスキーマを変更するのにもALTER TABLE ADD COLUMNSなどはないのでテーブルを作り直すことになるが、当然パーティションも全部作り直すことになる。
ではどうしようもないかというとMSCK REPAIR TABLEというのがあって、 これはS3のObjectのdt=YYYY-MM-DDのようなkey=valueのprefixを認識してパーティションを作るもの。作り直す際もこれ1クエリで終わる。それなら最初からそういう風に置けばよいのではというところだけど、勝手にYYYY/MM/DD/HHのprefixを付けてしまうFirehoseのようなのもある。
今回作ったathena-adminは以下のような定義ファイルから、 パーティションのkey=valueのprefixが付くように置き換えたり、変更があったらmigrationする。 このファイルを書き換えるだけで基本的にどうにかなるし、バージョン管理すればテーブル定義の変更を追うことができる。
{ &amp;quot;general&amp;quot;: { &amp;quot;athenaRegion&amp;quot;: &amp;quot;ap-northeast-1&amp;quot;, &amp;quot;databaseName&amp;quot;: &amp;quot;aaaa&amp;quot;, &amp;quot;saveDefinitionLocation&amp;quot;: &amp;quot;s3://saveDefinitionBucket/aaaa.json&amp;quot; }, &amp;quot;tables&amp;quot;: { &amp;quot;sample_data&amp;quot;: { &amp;quot;columns&amp;quot;: { &amp;quot;user_id&amp;quot;: &amp;quot;int&amp;quot;, &amp;quot;value&amp;quot;: { &amp;quot;score&amp;quot;: &amp;quot;int&amp;quot;, &amp;quot;category&amp;quot;: &amp;quot;string&amp;quot; } /* &amp;quot;struct&amp;lt;score:int,category:string&amp;gt;&amp;quot; のように書くこともできる */ }, &amp;quot;srcLocation&amp;quot;: &amp;quot;s3://src/location/&amp;quot;, &amp;quot;partition&amp;quot;: { &amp;quot;prePartitionLocation&amp;quot;: &amp;quot;s3://pre/partition/&amp;quot;, /* optional */ &amp;quot;regexp&amp;quot;: &amp;quot;(\\d{4})/(\\d{2})/(\\d{2})/&amp;quot;, /* optional */ &amp;quot;keys&amp;quot;: [ { &amp;quot;name&amp;quot;: &amp;quot;dt&amp;quot;, &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;, &amp;quot;format&amp;quot;: &amp;quot;{1}-{2}-{3}&amp;quot;, /* optional */ } ] } } } }  使い方はこんな感じ。使い方によってはmigrate()だけ呼ぶこともあると思う。 replaceObjects()にはmatchedHandlerというのを渡すこともできて、 UTCからJSTに変換するといったこともできる。</description>
    </item>
    
    <item>
      <title>ApexでデプロイしたLambdaのトリガーをTerraformで管理する</title>
      <link>https://www.sambaiz.net/article/144/</link>
      <pubDate>Sun, 12 Nov 2017 22:23:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/144/</guid>
      <description>Apexでfunctionをデプロイするとトリガーが登録されないのであとで追加することになる。 これを手作業で行うこともできるのだけど、せっかくなのでアプリケーションと一緒に管理したい。 そんなときのためにterraformコマンドをラップしたapex infraが用意されている。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
functionsと同列にinfrastructureディレクトリを作成してtfファイルを置く。 その下に環境ごとのディレクトリを作成することもできて、その場合は--envで指定した環境のものが使われる。
- functions - infrastructure main.tf variables.tf - modules - cloudwatch_schedule main.tf variables.tf project.json  functionをデプロイするとそのARNが変数で取れるようになる。
$ apex list --tfvars apex_function_hello=&amp;quot;arn:aws:lambda:ap-northeast-1:*****:function:usetf_hello&amp;quot;  今回設定するトリガーはCloudwatch Eventのスケジューリング。作成するリソースは以下の通り。
 aws_cloudwatch_event_ruleでイベントルール(今回はschedule)を作成 aws_cloudwatch_event_targetでルールにターゲット(今回はLambda)を設定 aws_lambda_permissionでルールに対象Lambdaをinvokeする権限を付ける  $ cat infrastructure/modules/cloudwatch_schefule/variables.tf variable &amp;quot;lambda_function_name&amp;quot; {} variable &amp;quot;lambda_function_arn&amp;quot; {} variable &amp;quot;schedule_expression&amp;quot; { description = &amp;quot;cloudwatch schedule expression e.g. \&amp;quot;cron(0/5 * * * ? *)\&amp;quot;&amp;quot; } $ cat infrastructure/modules/cloudwatch_schefule/main.tf resource &amp;quot;aws_cloudwatch_event_rule&amp;quot; &amp;quot;lambda&amp;quot; { name = &amp;quot;lambda_rule_${var.</description>
    </item>
    
    <item>
      <title>Redashでデータを可視化する</title>
      <link>https://www.sambaiz.net/article/141/</link>
      <pubDate>Mon, 23 Oct 2017 23:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/141/</guid>
      <description>RedashはOSSのデータ可視化ツール。 BIツールのようにパラメータを変えながら指標を探っていくというよりは、分かっている指標を見るのに使うイメージ。 比較的機能が少ない分処理がわかりやすく、 クエリが自動生成されないため時間がかかるものを実行する前にある程度気づけるのが良いと思う。
docker-composeで立ち上げることもできるけど、 AWSの各リージョンにAMIが用意されているのでそれで立ち上げる。
sshで入って以下のようなのを必要に応じて設定する。 メールを送る場合はSESでメールアドレスをVerifyしてやるのが簡単。 GSuiteを使っている場合、OAuthのClientID、Secretを発行しドメインを登録するとそれで認証できる。
$ ssh ubuntu@***** $ sudo vi /opt/redash/.env export REDASH_MAIL_SERVER=&amp;quot;email-smtp.us-east-1.amazonaws.com&amp;quot; export REDASH_MAIL_USE_TLS=&amp;quot;true&amp;quot; export REDASH_MAIL_USERNAME=&amp;quot;*****&amp;quot; export REDASH_MAIL_PASSWORD=&amp;quot;*****&amp;quot; export REDASH_MAIL_DEFAULT_SENDER=&amp;quot;*****&amp;quot; # Email address to send from export REDASH_GOOGLE_CLIENT_ID=&amp;quot;&amp;quot; export REDASH_GOOGLE_CLIENT_SECRET=&amp;quot;&amp;quot; $ cd /opt/redash/current $ sudo -u redash bin/run ./manage.py org set_google_apps_domains {{domains}} $ sudo supervisorctl restart all  HTTPS対応するのに/etc/nginx/sites-available/redashを編集する。crtとkeyの場所は変える。
upstream rd_servers { server 127.0.0.1:5000; } server { server_tokens off; listen 80 default; access_log /var/log/nginx/rd.access.log; gzip on; gzip_types *; gzip_proxied any; location /ping { proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://rd_servers; } location / { return 301 https://$host$request_uri; } } server { listen 443 ssl; # Make sure to set paths to your certificate .</description>
    </item>
    
    <item>
      <title>ApexでLambdaをデプロイする</title>
      <link>https://www.sambaiz.net/article/140/</link>
      <pubDate>Sun, 22 Oct 2017 16:06:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/140/</guid>
      <description>ApexでLambdaをデプロイする。 とても簡単に使えるし、変なこともしないので良い感じ。
Serverless Frameworkだとeventの設定までカバーできてより便利。
Serverless FrameworkでLambdaをデプロイする - sambaiz-net
インストール。ダウンロードして実行できるようにしている。
$ curl https://raw.githubusercontent.com/apex/apex/master/install.sh | sh   IAMFullAccess AWSLambdaFullAccess  を付けたIAMのプロファイルを登録しておく。
$ aws configure --profile apex $ aws configure list --profile apex Name Value Type Location ---- ----- ---- -------- profile apex manual --profile access_key ****************OVGQ shared-credentials-file secret_key ****************oi5t shared-credentials-file region ap-northeast-1 config-file ~/.aws/config  apex initしてnameとdescriptionを入れるとIAMが登録され、 ディレクトリ構造が作られる。
$ apex init --profile apex Project name: try-apex Project description: test [+] creating IAM try-apex_lambda_function role [+] creating IAM try-apex_lambda_logs policy [+] attaching policy to lambda_function role.</description>
    </item>
    
    <item>
      <title>Lambda上でPuppeteer/Headless Chromeを動かすStarter Kitを作った</title>
      <link>https://www.sambaiz.net/article/132/</link>
      <pubDate>Sun, 10 Sep 2017 23:45:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/132/</guid>
      <description>PuppeteerでHeadless Chromeを動かすコードを Lambda上で動かすStarter Kitを作った。
puppeteer-lambda-starter-kit
Chromeの準備 Puppeteerのインストール時に落としてくるChromeをLambda上で動かそうとしても Lambdaにないshared libraryに依存しているため失敗する。
error while loading shared libraries: libpangocairo-1.0.so.0: cannot open shared object file: No such file or directory  Lambda上でHeadless Chromeを動かす例がないか調べたらserverless-chromeというのがあって、 Headless用の設定でChromeをビルドしていた。 ほかにはchromelessというのもあるけど これはserverless-chromeに 依存している。 最小構成でPuppeteerを使いたかったので、今回はこれらを使わず一から作ることにした。
serverless-chromeにもビルドしたものが置いてあるが、少しバージョンが古いようだったので最新版でビルドした。 基本的には書いてある 通りやればうまくいく。他のプロセスとのshared memoryとして/dev/shmを使っているのを、/tmpに置き換える ようにしないと、実行時のpage.goto()でFailed Provisional Load: ***, error_code: -12になる。
ビルドしたheadless_shellには問題になった依存は含まれていないようだ。
$ ldd headless_shell linux-vdso.so.1 =&amp;gt; (0x00007ffcb6fed000) libpthread.so.0 =&amp;gt; /lib64/libpthread.so.0 (0x00007f5f17dbe000) libdl.so.2 =&amp;gt; /lib64/libdl.so.2 (0x00007f5f17bba000) librt.so.1 =&amp;gt; /lib64/librt.so.1 (0x00007f5f179b1000) libnss3.so =&amp;gt; /usr/lib64/libnss3.so (0x00007f5f17692000) libnssutil3.so =&amp;gt; /usr/lib64/libnssutil3.so (0x00007f5f17466000) libsmime3.</description>
    </item>
    
    <item>
      <title>TerraformでVPCを管理するmoduleを作る</title>
      <link>https://www.sambaiz.net/article/121/</link>
      <pubDate>Sun, 23 Jul 2017 02:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/121/</guid>
      <description>Terraform
$ brew install terraform $ terraform -v Terraform v0.9.11  Terraformの設定要素 provider IaaS(e.g. AWS)、PaaS(e.g. Heroku)、SaaS(e.g. CloudFlare)など。
AWS Providerはこんな感じ。 ここに直接access_keyやsecret_keyを書くこともできるけど、誤って公開されてしまわないように環境変数か variableで渡す。
provider &amp;quot;aws&amp;quot; { # access_key = &amp;quot;${var.access_key}&amp;quot; # secret_key = &amp;quot;${var.secret_key}&amp;quot; region = &amp;quot;us-east-1&amp;quot; }  $ export AWS_ACCESS_KEY_ID=&amp;quot;anaccesskey&amp;quot; $ export AWS_SECRET_ACCESS_KEY=&amp;quot;asecretkey&amp;quot;  varibale CLIでオーバーライドできるパラメーター。typeにはstringのほかにmapやlistを渡すことができ、 何も渡さないとdefault値のものが、それもなければstringになる。
variable &amp;quot;key&amp;quot; { type = &amp;quot;string&amp;quot; default = &amp;quot;value&amp;quot; description = &amp;quot;description&amp;quot; }  値を渡す方法はTF_VAR_をprefixとする環境変数、-var、-var-fileがある。 また、moduleのinputとして渡されることもある。
$ export TF_VAR_somelist=&#39;[&amp;quot;ami-abc123&amp;quot;, &amp;quot;ami-bcd234&amp;quot;]&#39; $ terraform apply -var foo=bar -var foo=baz $ terraform apply -var-file=foo.</description>
    </item>
    
    <item>
      <title>fluentdのAggregatorをELBで負荷分散し、Blue/Green Deploymentする</title>
      <link>https://www.sambaiz.net/article/113/</link>
      <pubDate>Sun, 25 Jun 2017 00:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/113/</guid>
      <description>デプロイやスループットの調整を簡単にするため、BeanstalkでAggregatorを立ち上げた。
負荷分散 TCPの24224(設定による)が通るようにEC2,ELBのSGとリスナーの設定をする必要があって、 ELBのSGのアウトバウンドの設定が見落とされがち。ELBのクロスゾーン分散は有効になっている。
まず、ELBに3台、それぞれ別のAZ(1b, 1c, 1d)に配置されている状態でログを送り始めるとそれぞれ均等にログが届いた。 その状態から4台(1b * 2, 1c, 1d)にすると、2つのインスタンス(1b, 1c)のみに均等にログが届くようになった。 4台になると(1b, 1c)と(1b, 1d)に分けられてELBのノードがそれらの組に紐づいたということだと思う。 各ノードにはDNSラウンドロビンするようになっている。実際restartすると今度は別の組の方に送られた。
では、なぜ一度送り始めると同じ方にしか飛ばないかというと、forwardプラグインのexpire_dns_cacheがデフォルトでnilになっていて、 heartbeatが届いている間は無期限にDNSキャッシュするようになっているため。これに0(キャッシュしない)か秒数を指定すると、 その間隔で他の組のインスタンスにもログが届くようになった。 expire_dns_cacheしなくても複数のインスタンスからラウンドロビンされるため全体でいえば分散される。
heartbeat ELB配下のEC2を全て落としてもheartbeatに失敗しないため、standyに移行せずELBのバックエンド接続エラーになってログがロストしてしまうようだ。 ログにも出ず、以下のようにactive-standbyの設定をしてもstandbyに移行しない。 全てのインスタンスが同時に落ちるというのは滅多に起きないだろうけど、少なくとも検知できるようにはしておく。
&amp;lt;server&amp;gt; name td1 host autoscale-td1.us-east-1.elasticbeanstalk.com port 24224 &amp;lt;/server&amp;gt; &amp;lt;server&amp;gt; name td2 host autoscale-td2.us-east-1.elasticbeanstalk.com port 24224 standby &amp;lt;/server&amp;gt;  Blue/Green Deployment Blue-Green Deploymentというのは、2つの系を用意し、activeでない方にデプロイし、 スワップして反映させるもの。ダウンタイムなしで問題が起きた際にもすぐに切り戻すことができる。 スワップして向き先を変えるにはexpire_dns_cacheを設定する必要がある。
Auto Scaling 増えるのはいいとして減るときに、 送り先で一時的に障害が起きていたりするとバッファをflushできずにログがロストする可能性がある。 それでいうとログの送り元でも同じことが起こりうるんだけど、通常Aggregatorにしか送らないので比較的問題になりにくい。
これを避けたい場合、Auto Scalingグループの設定で スケールインから保護を有効にして これから立ち上がるインスタンスはスケールインしなくすることができる。 それまでに立ち上がっていたインスタンスには適用されないので注意。
スケールインしないということは最大の台数で止まってしまうので、 ピークを過ぎたらスワップしてバッファが全て掃けたことを確認してからTerminateすることになる。 これを日常的にやるのは面倒なので、実際は予期しない流量の増加に備えて一応設定しておき、 普段はしきい値にひっかからないような最低台数で待ち構えておくことにするかな。
あとはヘルスチェックによって潰される可能性はなくもないけど、それはもうやむなし・・・。
参考 AWS ELBの社内向け構成ガイドを公開してみる 負荷分散編 – Cross-Zone Routingを踏まえて ｜ Developers.</description>
    </item>
    
    <item>
      <title>fluentdでKinesis streamsに送るときの性能確認</title>
      <link>https://www.sambaiz.net/article/108/</link>
      <pubDate>Mon, 05 Jun 2017 23:48:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/108/</guid>
      <description>localでのstreamsとproducerのbenchmark aws-fluent-plugin-kinesisの make benchmarkはlocalにDummyServerを立ち上げて送っている。
空でもいいのでroleをつけておく必要がある。
$ git clone https://github.com/awslabs/aws-fluent-plugin-kinesis.git $ cd aws-fluent-plugin-kinesis $ yum install -y ruby-devel gcc $ echo &#39;gem &amp;quot;io-console&amp;quot;&#39; &amp;gt;&amp;gt; Gemfile $ make $ make benchmark  RATEを指定しなければデフォルトで秒間1000レコードが送られる設定。 fluentdを起動してから10秒後にプロセスをkillし、そのレコード数などを出力している。
t2.microでデフォルト(RATE=1000)で実行した結果がこれ。 固める分producerの方はややパフォーマンスが落ちる。
bundle exec rake benchmark TYPE=streams Results: requets: 20, raw_records: 9400, records: 9400 bundle exec rake benchmark TYPE=producer Results: requets: 14, raw_records: 1005, records: 8900  RATE=3000のとき。producerではraw_recordsが1/100、リクエスト数は1/5。 streamsだとシャードを増やしていく必要があるけど、producerの方は当分大丈夫そうだ。
bundle exec rake benchmark TYPE=streams Results: requets: 57, raw_records: 27600, records: 27600 bundle exec rake benchmark TYPE=producer Results: requets: 12, raw_records: 241, records: 25200  RATE=10000のとき。raw_records, requestの圧縮率はさらに上がり、 パフォーマンスの差が大きくなってきている。</description>
    </item>
    
    <item>
      <title>BeanstalkでのパッケージのバージョンがAMIでのバージョンと異なる原因</title>
      <link>https://www.sambaiz.net/article/106/</link>
      <pubDate>Sun, 04 Jun 2017 23:40:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/106/</guid>
      <description>User-Dataとは EC2インスタンス起動時に、シェルスクリプトを走らせたりcloud-initディレクティブを適用できる機能。 コンソールではインスタンスの詳細の設定の、高度な詳細のところから設定できる。
BeanstalkでのUser-Data 実はBeanstalkでも使われていて、CloudFormationで設定されている。
&amp;quot; /bin/bash /tmp/ebbootstrap.sh &amp;quot;, ... &amp;quot;Fn::FindInMap&amp;quot;: [ &amp;quot;AWSEBOptions&amp;quot;, &amp;quot;options&amp;quot;, &amp;quot;UserDataScript&amp;quot; ] &amp;quot; &amp;gt; /tmp/ebbootstrap.sh &amp;quot;, ... &amp;quot;AWSEBOptions&amp;quot;: { &amp;quot;options&amp;quot;: { &amp;quot;UserDataScript&amp;quot;: &amp;quot;https://s3-ap-northeast-1.amazonaws.com/elasticbeanstalk-env-resources-ap-northeast-1/stalks/eb_node_js_4.0.1.90.2/lib/UserDataScript.sh&amp;quot;, &amp;quot;guid&amp;quot;: &amp;quot;f08557fc43ac&amp;quot;, } }  このshellの中では、時計を同期させたり、awsebユーザーを作成したりするほかに、 非Beanstalk AMI(is_baked=false)ではyum updateが走るようになっている。 そのため、AMIでのバージョンとBeanstalkで立ち上がったときのバージョンが異なることがあるようだ。
GUID=$7 function update_yum_packages { if is_baked update_yum_packages_$GUID; then log yum update has already been done. else log Updating yum packages. yum --exclude=aws-cfn-bootstrap update -y || echo Warning: cannot update yum packages. Continue... mark_installed update_yum_packages_$GUID # Update system-release RPM package will reset the .</description>
    </item>
    
    <item>
      <title>Node.jsでの文字コードの変換</title>
      <link>https://www.sambaiz.net/article/89/</link>
      <pubDate>Tue, 28 Mar 2017 21:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/89/</guid>
      <description>node-iconvを使う。
$ npm install iconv  SHIFT_JISからUTF-8への変換はこんな感じ。
const Iconv = require(&#39;iconv&#39;).Iconv; const before = new Buffer([ 0x8b, 0x8d, 0x8e, 0x4d, 0x26, 0x82, 0xb2, 0x94, 0xd1 ]); const iconv = new Iconv(&#39;SHIFT_JIS&#39;, &#39;UTF-8&#39;); console.log(`before: ${before.toString(&#39;hex&#39;)} ${before.toString()}`) const after = iconv.convert(before); console.log(`after: ${after.toString(&#39;hex&#39;)} ${after.toString()}`);  before: 8b8d8e4d2682b294d1 ���M&amp;amp;���� after: e7899be79abf26e38194e9a3af 牛皿&amp;amp;ご飯  文字コードによっては変換後に表せないことがある。 例えば、UTF-8からSHIFT_JISへの変換でサロゲートペア🍚を渡すと変換できず、エラーになる。
throw errnoException(&#39;EILSEQ&#39;, &#39;Illegal character sequence.&#39;);  //IGNOREを付けることで そのような文字があった場合でもエラーにしないようにできる。
const Iconv = require(&#39;iconv&#39;).Iconv; const before = &amp;quot;牛皿&amp;amp;🍚&amp;quot;; const iconv = new Iconv(&#39;UTF-8&#39;, &#39;SHIFT_JIS//IGNORE&#39;); console.</description>
    </item>
    
    <item>
      <title>FluentdとKPL(Kinesis Producer Library)でログをまとめてスループットを稼ぐ</title>
      <link>https://www.sambaiz.net/article/84/</link>
      <pubDate>Wed, 15 Mar 2017 23:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/84/</guid>
      <description>KPL(Kinesis Producer Library)とは Developing Amazon Kinesis Streams Producers Using the Amazon Kinesis Producer Library - Amazon Kinesis Streams
Kinesisに送るとき、自動リトライしてくれたり、レコードをまとめてスループットを向上してくれたりするアプリケーション。Protobufを使っている。 普通に送るとどんなに小さくてもシャード*1000レコード/秒しか最大でPUTできないのを、KPLを使ってまとめることで増やすことができる。
fluentdで送る aws-fluent-plugin-kinesisでkinesis_producerを指定するとKPLを使って送信する。
&amp;lt;kinesis_producer&amp;gt;の中にKPLの設定を書くことができる。
&amp;lt;kinesis_producer&amp;gt; record_max_buffered_time 10 &amp;lt;/kinesis_producer&amp;gt;  record_max_bufferd_time はバッファされたレコードが送られるまでの最大時間(ms)。デフォルトは100ms。この時間が経つか、他のリミットに当たったらレコードは送られる。
 AggregationMaxCount: 一つのレコードにまとめる最大レコード数 AggregationMaxSize: まとめたレコードの最大バイト数 CollectionMaxCount: PutRecordsで送る最大アイテム数 CollectionMaxSize: PutRecordsで送るデータ量  CloudWatchに送るmetrics_levelはデフォルトでdetailedになっていて、 コンソールのメトリクスからstream名で検索すると KinesisProducerLibraryにUserRecordsPerKinesisRecordや、UserRecordsDataPut、BufferingTime、RequestTimeなどいろいろ表示される。
とりあえず試しにこんな設定で送ってみる。
&amp;lt;match hoge.log&amp;gt; @type kinesis_producer region ap-northeast-1 stream_name teststream include_time_key true flush_interval 1 buffer_chunk_limit 1m try_flush_interval 0.1 queued_chunk_flush_interval 0.01 num_threads 15 &amp;lt;/match&amp;gt;  Lambdaで読む まとめられたレコードをkinesis-aggregationで分解して読む。 今回はNode.jsでやる。</description>
    </item>
    
    <item>
      <title>fluentdでKinesis Streamsに送ってLambdaで読んでS3に保存する</title>
      <link>https://www.sambaiz.net/article/73/</link>
      <pubDate>Sun, 26 Feb 2017 18:56:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/73/</guid>
      <description>aws-fluent-plugin-kinesisでKinesis Streamsに送り、Lambdaで読んでS3に保存する。 要するにFirehoseのようなことをやりたいのだけれどTokyoリージョンにまだ来ないので自分でやる。
fluentdで送る $ td-agent-gem install fluent-plugin-kinesis  try_flush_intervalとqueued_chunk_flush_intervalはドキュメントには載っていないが、 以下のページによるとそれぞれqueueに次のchunkがないときとあるときのflushする間隔。 いずれもデフォルトは1だが、これを減らすことでもっと頻繁に吐き出されるようになるらしい。
Fluentd の out_forward と BufferedOutput
あとシャードに振り分けるためのpartition_key を指定できる。デフォルトはランダム。
&amp;lt;source&amp;gt; @type tail path /var/log/td-agent/hoge.log pos_file /etc/td-agent/log.pos tag hoge.log format json time_key timestamp # 2017-01-01T01:01:01+0900 time_format %Y-%m-%dT%H:%M:%S%z &amp;lt;/source&amp;gt; &amp;lt;match hoge.log&amp;gt; @type kinesis_streams region ap-northeast-1 stream_name teststream include_time_key true flush_interval 1 buffer_chunk_limit 1m try_flush_interval 0.1 queued_chunk_flush_interval 0.01 num_threads 15 &amp;lt;/match&amp;gt;  いくつか送ってみる。
for i in `seq 1 1000` do echo &#39;{&amp;quot;hoge&amp;quot;: &amp;quot;fuga&amp;quot;, &amp;quot;timestamp&amp;quot;: &amp;quot;2017-01-01T01:01:01+0900&amp;quot;}&#39; &amp;gt;&amp;gt; /var/log/td-agent/hoge.</description>
    </item>
    
    <item>
      <title>AWSのAssumeRole</title>
      <link>https://www.sambaiz.net/article/72/</link>
      <pubDate>Sat, 25 Feb 2017 20:40:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/72/</guid>
      <description>AWS Security Token Serviceによる、 RoleArn(arn:aws:iam::&amp;lt;account id&amp;gt;:role/&amp;lt;role name&amp;gt;)から一時的なCredentialを取得する仕組み。 前もって発行したAPIキーとは違い、有効期限が存在するため続けて呼ぶ場合は失効する前に再発行する必要がある。
ではRoleArnを知っていたら誰でも取得できるかというと、もちろんそうではなく、 ロールの信頼関係、&amp;quot;Action&amp;quot;: &amp;quot;sts:AssumeRole&amp;quot;のPrincipalのところで信頼する対象を設定する。 例えば、Serviceでec2.amazonaws.comを指定してEC2がAssumeRoleするのを許可したり、 AWSで(他の)アカウントやユーザーを指定してそのAPIキーでこのRoleのCredentialを取得できるようにしたりといった感じ。
{ &amp;quot;Version&amp;quot;: &amp;quot;2012-10-17&amp;quot;, &amp;quot;Statement&amp;quot;: [ { &amp;quot;Effect&amp;quot;: &amp;quot;Allow&amp;quot;, &amp;quot;Principal&amp;quot;: { &amp;quot;Service&amp;quot;: &amp;quot;ec2.amazonaws.com&amp;quot; }, &amp;quot;Action&amp;quot;: &amp;quot;sts:AssumeRole&amp;quot; } ] }  EC2にロールを設定すると、実はそのロールについてAssumeRoleして自動でCredentialを取得している。 EC2にロールを設定するにはロールとは別に インスタンスプロファイルを作成 する必要があるが、コンソールでEC2のサービスロールを作ると同名のインスタンスプロファイルが自動で作成される。 さらに、AssumeRoleのServiceとしてec2.amazonaws.comが追加されている。
$ curl http://169.254.169.254/latest/meta-data/iam/info { &amp;quot;Code&amp;quot; : &amp;quot;Success&amp;quot;, &amp;quot;LastUpdated&amp;quot; : &amp;quot;2017-02-25T10:56:33Z&amp;quot;, &amp;quot;InstanceProfileArn&amp;quot; : &amp;quot;arn:aws:iam::*****:instance-profile/assume_role_test&amp;quot;, &amp;quot;InstanceProfileId&amp;quot; : &amp;quot;*****&amp;quot; } $ curl http://169.254.169.254/latest/meta-data/iam/security-credentials/assume_role_test { &amp;quot;Code&amp;quot; : &amp;quot;Success&amp;quot;, &amp;quot;LastUpdated&amp;quot; : &amp;quot;2017-02-25T10:56:23Z&amp;quot;, &amp;quot;Type&amp;quot; : &amp;quot;AWS-HMAC&amp;quot;, &amp;quot;AccessKeyId&amp;quot; : &amp;quot;*****&amp;quot;, &amp;quot;SecretAccessKey&amp;quot; : &amp;quot;*****&amp;quot;, &amp;quot;Token&amp;quot; : &amp;quot;*****&amp;quot;, &amp;quot;Expiration&amp;quot; : &amp;quot;2017-02-25T17:26:07Z&amp;quot; }  参考 IAMロール徹底理解 〜 AssumeRoleの正体 ｜ Developers.</description>
    </item>
    
    <item>
      <title>ELBのスケーリングとsurge queue</title>
      <link>https://www.sambaiz.net/article/68/</link>
      <pubDate>Tue, 21 Feb 2017 19:48:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/68/</guid>
      <description>バックエンドだけではなくELB自体もスケーリングし、内部node数はdigで調べることができる。 このnode数は自分ではコントロールできず、基本的に意識することはない。
$ dig ****.ap-northeast-1.elb.amazonaws.com ;; ANSWER SECTION: *****.elb.amazonaws.com. 60 IN A xxx.xxx.xxx.xxx *****.elb.amazonaws.com. 60 IN A yyy.yyy.yyy.yyy  nodeが増えるのにはある程度時間がかかるので、 アクセスが急増(5分間で50%以上のトラフィック増加が目安) したら捌ききれず、503を返すことがある。 前もって多量のアクセスが来ることが分かっていて、 AWSサポートがBusiness以上なら pre-warming申請することでnodeが増えた状態で待ち構えられる。
バックエンドのアプリケーションがリクエストを処理できない場合、ELBのsurge queueに溜まっていく。 この数はCloudWatchのSurgeQueueLength(キュー長の急増)メトリクスで確認できる。 また、SurgeQueueLengthの最大値1024を超えるとリクエストは拒否され、その数はSpoiloverCount(過剰数)メトリクスに出る。
参考 ELBの挙動とCloudWatchメトリクスの読み方を徹底的に理解する ｜ Developers.IO
Elastic Load Balancing でのレイテンシーのトラブルシューティング</description>
    </item>
    
    <item>
      <title>Kinesis Streams/Firehose/Analyticsを試す</title>
      <link>https://www.sambaiz.net/article/67/</link>
      <pubDate>Mon, 20 Feb 2017 21:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/67/</guid>
      <description>https://aws.amazon.com/jp/kinesis/
リアルタイムのストリーミングデータを扱うサービス群。 いまのところTokyoリージョンではKinesis Streamsしか使えない。
Kinesis Firehose AWSのデータストアに送るストリーム。自分でデータを読む処理を書かなくてよく、スケーリングも勝手にやってくれるので簡単に使える。
https://aws.amazon.com/jp/kinesis/firehose/faqs/
Q: 送信先とは何ですか? 送信先はデータが配信されるデータストアです。Amazon Kinesis Firehose では、 現在送信先として Amazon S3、Amazon Redshift、Amazon Elasticsearch Service がサポートされています。  料金は取り込まれたデータ量による。 一見そんなに高くならないように見えるが、5KB単位で切り上げられるのでレコードのサイズが小さくて数が多い場合に注意が必要。
今回はS3に送ってみる。
圧縮方法を設定したり、Lambdaを噛ませたりすることができる。
StatusがActiveになったらKinesis Agentで送ってみる。 CloudWatchとFirehoseにPutする権限が必要。Firehoseはkinesis:ではなくfirehose:なので注意。
$ sudo yum install –y aws-kinesis-agent  /etc/aws-kinesis/agent.jsonを編集する。リージョンごとのエンドポイントは ここ にある。
{ &amp;quot;awsAccessKeyId&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;awsSecretAccessKey&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;firehose.endpoint&amp;quot;: &amp;quot;https://firehose.us-east-1.amazonaws.com&amp;quot;, &amp;quot;flows&amp;quot;: [ { &amp;quot;filePattern&amp;quot;: &amp;quot;/tmp/hoge.log&amp;quot;, &amp;quot;deliveryStream&amp;quot;: &amp;quot;hogefugastream&amp;quot; } ] }  $ sudo service aws-kinesis-agent start $ sudo chkconfig aws-kinesis-agent on $ echo &amp;quot;aaa&amp;quot; &amp;gt;&amp;gt; /tmp/hoge.</description>
    </item>
    
    <item>
      <title>GoでDynamoDBを使う</title>
      <link>https://www.sambaiz.net/article/63/</link>
      <pubDate>Sun, 12 Feb 2017 23:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/63/</guid>
      <description>テーブルを作成する プライマリキー テーブルの操作のガイドライン - Amazon DynamoDB
プライマリキーとしてパーティションキー(ハッシュキー)とオプションのソートキー(レンジキー)を設定する。 DynamoDBはこのパーティションキーに基づいて、複数のパーティションに分散して保存する。 テーブルにプロビジョニングされたスループット性能はパーティション間で均等に使われるので、 ソートキーを設定する場合にこれを最大限に活用するためには、 あるパーティションにリクエストが集中しないよう、パーティションキーに特定の値ばかり集中しないようなフィールドを 選ぶ必要がある。
セカンダリインデックス パーティションキーのグローバルセカンダリインデックス(GSI)と ソートキーのローカルセカンダリインデックス(LSI)がある。 射影されるフィールドを選択でき、ここに含まれないフィールドは返さない。 ただし、すべてをインデックスに書き込むのはコストが高いのでなるべく絞る。
キャパシティユニット  1読み込みキャパシティユニット: 4kbを超えないデータを1秒に1~2回(整合性による)読み込める 1書き込みキャパシティユニット: 1kbを超えないデータを1秒に1回書き込める  ユニットに応じて1時間あたりで課金される。
未使用のキャパシティがある場合、最大5分保持してバーストに備えてくれる。
読み書きする aws-sdk-goを直接使ってもいいけど、簡単に扱えるラッパー guregu/dynamo を使うことにした。
type Data struct { ID int64 `dynamo:&amp;quot;id&amp;quot;` Name string Age int } db := dynamo.New(session.New(), &amp;amp;aws.Config{Region: aws.String(&amp;quot;ap-northeast-1&amp;quot;)}) table := db.Table(&amp;quot;testtable&amp;quot;)  Create &amp;amp; Update d := Data{ID: 1, Name: &amp;quot;hogefuga&amp;quot;, Age: 123} if err := table.Put(d).Run(); err != nil { return err } if err := table.</description>
    </item>
    
    <item>
      <title>EC2のインスタンスストア</title>
      <link>https://www.sambaiz.net/article/58/</link>
      <pubDate>Mon, 06 Feb 2017 21:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/58/</guid>
      <description>http://docs.aws.amazon.com/ja_jp/AWSEC2/latest/UserGuide/InstanceStorage.html
EC2ではインスタンスタイプによってはEBSに加えてインスタンスストアが使える。しかも追加料金なし。 対象はストレージが&amp;rdquo;EBSのみ&amp;rdquo;でないもの。
https://aws.amazon.com/jp/ec2/instance-types/
インスタンスストアはインスタンスが停止したり、障害が起きると消える一時ストレージ。再起動では消えない。 ホストに物理的にアタッチされているので、バッファやキャッシュなどの頻繁に読み書きされ、消えてもいいデータに最適。 他のインスタンスにアタッチすることはできない。容量や性能もインスタンスタイプに依存する。
インスタンスストアボリュームの追加は インスタンスの起動時に、新しいボリュームを追加し、ボリュームタイプをインスタンスストアにすることで行うことができる。
今回はSSDストレージ1 x 4のm3.mediumで試す。これは4gbのボリュームが一つ追加できるという意味。
まずはインスタンスストアを追加してないインスタンス。 lsblkというのはlist block devicesの略。
$ df -h ファイルシス サイズ 使用 残り 使用% マウント位置 /dev/xvda1 7.8G 1.2G 6.6G 15% / ... $ dd if=/dev/zero of=hoge bs=1M count=1000 $ ls -sh 合計 1001M 1001M hoge $ df -h ファイルシス サイズ 使用 残り 使用% マウント位置 /dev/xvda1 7.8G 2.2G 5.6G 28% / ... $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk └─xvda1 202:1 0 8G 0 part /  それに対してインスタンスストア(/dev/xvdb)を追加したインスタンス。</description>
    </item>
    
    <item>
      <title>複数EC2インスタンスを立ち上げてvegetaで負荷試験する</title>
      <link>https://www.sambaiz.net/article/43/</link>
      <pubDate>Sun, 18 Dec 2016 20:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/43/</guid>
      <description>vegetaで負荷をかける。
インスタンスを立ち上げるスクリプト コードはここ。 sambaiz/loadtest
まずキーペアを作成し、EC2インスタンスを立ち上げて、全てのインスタンスが使えるようになるまで待つ。
aws ec2 create-key-pair --key-name LoadTestKeyPare --query &#39;KeyMaterial&#39; --output text &amp;gt; LoadTestKeyPare.pem chmod 400 LoadTestKeyPare.pem aws ec2 run-instances --image-id $AMI_ID --count $INSTANCE_NUM --instance-type t2.micro --key-name LoadTestKeyPare --security-group-ids $SECURITY_GROUP_IDS --subnet-id $SUBNET_ID ... aws ec2 wait instance-status-ok --instance-ids $INSTANCE_IDS  このAMIは事前にPackerでつくったもの。vegetaをインストールしてファイルディスクリプタの上限を増やしている。
{ &amp;quot;variables&amp;quot;: { &amp;quot;aws_access_key&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;aws_secret_key&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `aws_access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `aws_secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;ap-northeast-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-0c11b26d&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.micro&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ec2-user&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;loadtest {{timestamp}}&amp;quot; }], &amp;quot;provisioners&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;, &amp;quot;inline&amp;quot;: [ &amp;quot;wget https://github.</description>
    </item>
    
    <item>
      <title>PackerでAMIを作る</title>
      <link>https://www.sambaiz.net/article/24/</link>
      <pubDate>Tue, 18 Oct 2016 22:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/24/</guid>
      <description>https://www.packer.io/
いろんなプラットフォームのイメージを作ることができるツール。 これでfluentdのログサーバーのAMIを作る。
$ brew install packer # mac $ packer -v 0.10.1  設定ファイルはこんな感じ。variablesの値は{{user ... }}のところで使われる。 buildersに作るイメージの情報を書いて、provisionersで環境を作る。
provisionersにはchefやansibleなども指定できるが、 継ぎ足し継ぎ足しで秘伝のタレ化したAMIも最初は、
「コマンドいくつか実行するだけなのでとりあえず手作業で作った、後でなんとかする」
なんてものもあったりして、 そういうものは無理にchefなどで始めず、手軽にshellでpacker buildするといいと思う。 手作業よりも楽だし、ソースが別にあるので使われていないAMIを消すのも簡単だ。
fileではpermissionがないところに置くことができないので、一旦置いてshellで移動する。
{ &amp;quot;variables&amp;quot;: { &amp;quot;aws_access_key&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;aws_secret_key&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `aws_access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `aws_secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;ap-northeast-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-1a15c77b&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.small&amp;quot;, &amp;quot;ssh_username&amp;quot;: &amp;quot;ec2-user&amp;quot;, &amp;quot;ami_name&amp;quot;: &amp;quot;fluentd-logserver {{timestamp}}&amp;quot; }], &amp;quot;provisioners&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;file&amp;quot;, &amp;quot;source&amp;quot;: &amp;quot;td-agent.conf&amp;quot;, &amp;quot;destination&amp;quot;: &amp;quot;/home/ec2-user/td-agent.conf&amp;quot; }, { &amp;quot;type&amp;quot;: &amp;quot;shell&amp;quot;, &amp;quot;inline&amp;quot;: [ &amp;quot;curl -L https://toolbelt.</description>
    </item>
    
  </channel>
</rss>