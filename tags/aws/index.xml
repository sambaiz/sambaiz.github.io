<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>aws on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/aws/</link>
    <description>Recent content in aws on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Tue, 18 Jan 2022 21:26:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/aws/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GlueのTPC-DS Connectorでデータを生成する</title>
      <link>https://www.sambaiz.net/article/393/</link>
      <pubDate>Tue, 18 Jan 2022 21:26:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/393/</guid>
      <description>AthenaのTPC-DS Connectorで250GBのデータを生成してS3に出力しようとしたところ最大までLambdaのリソースを上げてもタイムアウトしてしまったのでGlueで行う。
AthenaのFederated QueryでTPC-DS Connectorを用いてデータを生成する - sambaiz-net
GlueのTPC-DS connectorをSubscribeしてActivateする。
 スクリプトはこんな感じで必要なパラメータを外から渡せるようにした。scaleはAthenaのものと同じくGB単位。 アップロード時にカタログにテーブルが追加されるようにしている。
from socket import create_connection import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, [&amp;#34;JOB_NAME&amp;#34;, &amp;#34;BUCKET_NAME&amp;#34;, &amp;#34;DATABASE_NAME&amp;#34;, &amp;#34;SCALE&amp;#34;, &amp;#34;NUM_PARTITIONS&amp;#34;, &amp;#34;FORMAT&amp;#34;, &amp;#34;CONNECTION_NAME&amp;#34;]) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[&amp;#34;JOB_NAME&amp;#34;], args) bucketName = args[&amp;#34;BUCKET_NAME&amp;#34;] tables = [ &amp;#34;call_center&amp;#34;, &amp;#34;catalog_page&amp;#34;, &amp;#34;catalog_returns&amp;#34;, &amp;#34;catalog_sales&amp;#34;, &amp;#34;customer&amp;#34;, &amp;#34;customer_address&amp;#34;, &amp;#34;customer_demographics&amp;#34;, &amp;#34;date_dim&amp;#34;, &amp;#34;dbgen_version&amp;#34;, &amp;#34;household_demographics&amp;#34;, &amp;#34;income_band&amp;#34;, &amp;#34;inventory&amp;#34;, &amp;#34;item&amp;#34;, &amp;#34;promotion&amp;#34;, &amp;#34;reason&amp;#34;, &amp;#34;ship_mode&amp;#34;, &amp;#34;store&amp;#34;, &amp;#34;store_returns&amp;#34;, &amp;#34;store_sales&amp;#34;, &amp;#34;time_dim&amp;#34;, &amp;#34;warehouse&amp;#34;, &amp;#34;web_page&amp;#34;, &amp;#34;web_returns&amp;#34;, &amp;#34;web_sales&amp;#34;, &amp;#34;web_site&amp;#34; ] for table in tables: df = glueContext.</description>
    </item>
    
    <item>
      <title>Redshift Serverlessと他のサーバーレス集計サービス、Glue Data Catalogのテーブルへのクエリ実行</title>
      <link>https://www.sambaiz.net/article/392/</link>
      <pubDate>Sun, 26 Dec 2021 22:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/392/</guid>
      <description>Redshift Serverless Redshift Serverlessは今年のre:Inventで発表された、インスタンスを立てることなく従量課金でペタバイトスケールのDWHであるRedshiftを使える機能。 S3のデータを直接参照できるRedshift SpectrumやRDSへのFederated Query、機械学習のRedshift MLも使える。 分析のような不定期に発生する需要のためにインスタンスを立てておくのはコストの上でハードルが高かったのでこのアップデートは嬉しい。
料金は最低1分のオートスケールするRPU (2vCPU, 16GiBメモリ)時間とストレージに対してかかり、オレゴンリージョンなどが$0.45/RPU hourなのに対して東京はなぜか$0.70/RPU hourと少し割高な設定になっている。2vCPU, 15 GiBメモリのインスタンスdc2.largeがオンデマンドで$0.314/hourなので、利用頻度が少なかったり不定期なために平均40%以上リソースが使えていない場合のコストは抑えられそうだ。RIが最大まで効いている場合は$0.110/hourとなり15%まで閾値が下がるが、スケールを気にしなくて良いのは運用の上でもメリットがある。また、Redshift Spectrumのロード量に対する課金は発生しない。
他のサーバーレス集計サービス サーバーレスで集計を行える他のサービスとしてはPrestoのマネージドサービスであるAthenaやSparkのマネージドサービスであるGlueがあって、前者は手軽にクエリを実行できロード量による課金も分かりやすいが、実行時間や同時実行数などの制約があり、後者は時間のかかる集計も行うことができるがジョブを作る必要があってアドホックな実行にはあまり向かない。
これらはHive Metastoreに対応しており、その互換サービスであるGlue Data Catalogでスキーマを共有できるので用途に応じて使い分けることもできるが、使える文法に差があったり、同じクエリを実行しても異なる結果を返すことがあったりと、移植する際には注意する必要がある。
Athena(Presto)とGlue(Spark)で同じクエリを実行した際に異なる値が返る原因 - sambaiz-net
また、いずれもRDSに加えてDynamoDBやMongoDBなどのコネクタが用意されており、カスタムコネクタを用いることで他のデータソースにも接続することができる。
AthenaのFederated QueryでTPC-DS Connectorを用いてデータを生成する - sambaiz-net
GlueのカスタムコネクタでBigQueryに接続する - sambaiz-net
 同じく今年のre:Inventで発表されたEMR Serverlessもある。 巨大なデータに対して重い集計やrepartition()を行ったりするとOOMやNo space left on deviceになることがあるが、Glueだとスケールアップできないため物理的に解決が難しいことがあり、そのような場合はEMRを検討することになる。
サードパーティにも目を向ければ各クラウドにホストされるSnowflakeがある。コストやスケールの容易さのためにRedshiftからSnowflakeに移行した事例もあり良い評判を目にするが、立ち位置が近いRedshift Serverlessが発表されたことでこれからの技術選定に影響があるかもしれない。料金はプランによって単価が異なるSnowflake Creditとストレージに対してかかる。
他にはAnthosを用いてマルチクラウドでクエリエンジンを動かすBigQuery Omniもある。今年の10月にGAになったばかりで、対応リージョンがまだ少なく、料金も従量課金でないので他と同様に使うイメージではないが、データを転送することなくAWS上でBigQueryのクエリを実行できるのは待望の機能なので今後の動きが気になる。
Glue Data Catalogのテーブルにクエリを実行する CDKでData CatalogのDatabaseとTable、それらにアクセスできるRoleを作成する。
CDKでGlue Data CatalogのDatabase,Table,Partition,Crawlerを作成する - sambaiz-net
これも今年のre:Inventで発表があったが、ついにCDKのv2がstableになったのでv2で書く。 各サービスのパッケージ @aws-cdk/aws-xxx が aws-cdk-lib に統合され、alphaのものだけ旧パッケージ名に-alphaを付けたものになった。
$ npx aws-cdk@2.x init app --language typescript $ npm install --save aws-cdk-lib $ npm install --save @aws-cdk/aws-glue-alpha Redshift Serverlessで用いるRoleはPrincipalに redshift.</description>
    </item>
    
    <item>
      <title>AthenaのFederated QueryでTPC-DS Connectorを用いてデータを生成する</title>
      <link>https://www.sambaiz.net/article/391/</link>
      <pubDate>Sat, 25 Dec 2021 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/391/</guid>
      <description>AthenaのFederated Queryは データソースコネクタとなるLambdaを通してDynamoDBやRDSといったS3以外のデータソースにクエリを実行できる機能。
 今回はAWS公式のリポジトリにあるTPC-DS Connectorを用いて、意思決定支援(Decision Support)におけるデータベースのベンチマークであるTPC-DSのデータを生成する。
公式のリポジトリにあるとはいえカスタム扱いなので自分でビルドする必要がある。基本的にREADME通り進めれば良いが、jdk16ではビルドに失敗したのでjdk8を入れて実行した。
$ brew tap homebrew/cask-versions $ brew install --cask corretto8 export JAVA_HOME=`/usr/libexec/java_home -v 1.8` export PATH=${JAVA_HOME}/bin:${PATH} $ java -version openjdk version &amp;#34;1.8.0_312&amp;#34; OpenJDK Runtime Environment Corretto-8.312.07.1 (build 1.8.0_312-b07) OpenJDK 64-Bit Server VM Corretto-8.312.07.1 (build 25.312-b07, mixed mode) publish.shの引数でもリージョンを取るが、これとは別に.aws/configなどで指定されていないとAWS SDKの方で読めずに失敗する。
$ git clone https://github.com/awslabs/aws-athena-query-federation.git -b v2021.51.1 --depth 1 $ cd aws-athena-query-federation/ $ mvn clean install -DskipTests=true $ cd athena-tpcds/ $ mvn clean install -DskipTests=true # export AWS_REGION=us-west-2 $ .</description>
    </item>
    
    <item>
      <title>SparkのWeb UIでJobのStageとExecutorによるTask分散、SQLのplanを確認する</title>
      <link>https://www.sambaiz.net/article/382/</link>
      <pubDate>Thu, 30 Sep 2021 13:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/382/</guid>
      <description>SparkのWeb UIはJobやExecutorをモニタリングするためのツール。
aws-glue-samplesから maven:3.6-amazoncorretto-8 ベースでSparkを動かすDockerfileを持ってきて、 History Serverを起動する。Glueで出力されたEventLogのパスと認証情報を渡している。
 $ git clone https://github.com/aws-samples/aws-glue-samples.git $ cd aws-glue-samples/utilities/Spark_UI/glue-3_0/ $ docker build -t glue/sparkui:latest . $ docker run -it -e SPARK_HISTORY_OPTS=&amp;#34;$SPARK_HISTORY_OPTS-Dspark.history.fs.logDirectory=s3a://path_to_eventlog -Dspark.hadoop.fs.s3a.access.key=$AWS_ACCESS_KEY_ID-Dspark.hadoop.fs.s3a.secret.key=$AWS_SECRET_ACCESS_KEY&amp;#34; -p 18080:18080 glue/sparkui:latest &amp;#34;/opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer&amp;#34; これで http://localhost:18080 にアクセスしApplicationを選択するとJob実行やExecutor追加のタイムラインが表示される。
 各Jobをクリックすると次のようにStageとDAG(Directed Acyclic Graph)が表示される。 WholeStageCodeGenは高速化のため処理ごとではなくStage単位でCode Generationする処理。 ただ生成されるコードが大きいとJVMがJITコンパイルしなくなるのでかえって遅くなることもあるそうだ。
Stageはコストが大きいシャッフルを必要としない実行単位なので少ない方が良い。 Joinのためにかさんでいるのであれば、パラメータを調整するなどしてシャッフルしないBroadcast Hash Joinにできないか検討する。
Apache SparkのRDD, DataFrame, DataSetとAction, Transformation - sambaiz-net
 さらにStageをクリックすると各ExecutorごとのTaskのタイムラインが表示される。 ほとんどの時間をComputing Timeに割けていて、Taskの実行時間やExecutorへのInput Recordsの統計も概ね均一で、うまく分散できているように見える。
 SQLのタブを見るとOptimizerによって最適化されたクエリのLogical planや、 実際に実行されるPhysical plan、そのどこにどれくらい時間がかかっているかを確認できる。
 参考 Apache Sparkコミッターが教える、Spark SQLの詳しい仕組みとパフォーマンスチューニング Part2 - ログミーTech</description>
    </item>
    
    <item>
      <title>Glue DataBrewでデータを可視化して分析するProjectと機械学習の前処理を行うJobをCDKで作成する</title>
      <link>https://www.sambaiz.net/article/381/</link>
      <pubDate>Mon, 27 Sep 2021 16:42:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/381/</guid>
      <description>Glue DataBrewは、データを可視化してパラメータ間の相関を見たり、カテゴリー変数のエンコードや、欠損値や外れ値を置換する処理をコードなしで実行できるマネージドサービス。KaggleのHouse Prices Competitonの学習データで試してみる。全体のコードはGitHubにある。
KaggleのHouse Prices CompetitionのKernelからデータの探り方を学ぶ - sambaiz-net
料金は30分のセッションごとに$1と、Jobのノード時間あたり$0.48かかる。 ノードには通常のGlueのJobのDPUと同じく4vCPUと16GBのメモリが含まれ、時間あたりのコストの差は$0.04とそれほど大きくない。 使い分けとしては、非エンジニアが使う場合はGUIでデータを加工できるDataBrewを、独自の変換やカスタムコネクターを要する処理は通常のJobで行うと良いとのこと。
GlueのカスタムコネクタでBigQueryに接続する - sambaiz-net
Datasetの作成 DatasetのソースとしてData CatalogのほかにRedshiftやRDS、S3に直接接続することもできる。
CDKでGlue Data CatalogのDatabase,Table,Partition,Crawlerを作成する - sambaiz-net
今回はS3にファイルを上げてそれを参照する。
createDataBucket() { const bucket = new s3.Bucket(this, &amp;#39;DataBucket&amp;#39;, { bucketName: `databrew-sample-${this.account}-${this.region}`, removalPolicy: cdk.RemovalPolicy.DESTROY }) const deployData = new s3deploy.BucketDeployment(this, &amp;#39;DeploySource&amp;#39;, { sources: [s3deploy.Source.asset(&amp;#39;./data&amp;#39;)], destinationBucket: bucket, destinationKeyPrefix: &amp;#34;src/&amp;#34; }) return {bucket, deployData} } createDataset(bucket: s3.IBucket) { return new databrew.CfnDataset(this, &amp;#39;Dataset&amp;#39;, { name: &amp;#34;databrew-sample-train-dataset&amp;#34;, input: { s3InputDefinition: { bucket: bucket.</description>
    </item>
    
    <item>
      <title>GoでAmazon Forecastに時系列データをimportしPredictorを作成して予測結果をS3にexportする</title>
      <link>https://www.sambaiz.net/article/380/</link>
      <pubDate>Mon, 20 Sep 2021 23:26:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/380/</guid>
      <description>以前コンソール上で実行したAmazon Forecastでの時系列データの学習、予測をGoで行う。全体のコードはGitHubにある。
Amazon Forecastで時系列データの予測を行う - sambaiz-net
Datasetの作成 以前と同じく電力消費量のデータセットを、予測対象の時系列データ(DatasetTypeTargetTimeSeries)として登録する。データの頻度は1時間でドメインはCustom。
func (f Forecast) CreateDataset(ctx context.Context, name string) (*string, error) { return f.skipIfAlreadyExists(&amp;#34;dataset&amp;#34;, name, func() (*string, error) { dataset, err := f.svc.CreateDataset(ctx, &amp;amp;forecast.CreateDatasetInput{ DatasetName: aws.String(name), DatasetType: types.DatasetTypeTargetTimeSeries, DataFrequency: aws.String(&amp;#34;H&amp;#34;), Domain: types.DomainCustom, Schema: &amp;amp;types.Schema{ Attributes: []types.SchemaAttribute{ { AttributeName: aws.String(&amp;#34;timestamp&amp;#34;), AttributeType: types.AttributeTypeTimestamp, }, { AttributeName: aws.String(&amp;#34;target_value&amp;#34;), AttributeType: types.AttributeTypeFloat, }, { AttributeName: aws.String(&amp;#34;item_id&amp;#34;), AttributeType: types.AttributeTypeString, }, }, }, }) if err != nil { return nil, err } return dataset.</description>
    </item>
    
    <item>
      <title>CDKでCloudWatch Dashboardsを作成しコンソール上からAWSアカウントを持たない外部ユーザーに公開する</title>
      <link>https://www.sambaiz.net/article/379/</link>
      <pubDate>Sat, 18 Sep 2021 14:38:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/379/</guid>
      <description>CloudWatch Dashboardsは、CloudWatchのメトリクスの値やグラフを配置できるダッシュボードで、 次のようにCDKで作成できる。metricName と namespace、dimensions はコンソール上で確認する。
new cloudwatch.Dashboard(this, `CloudWatchDashboard`, { dashboardName: test }) cloudwatch.TextWidget({ markdown: `# Test Dashboard by [sambaiz](https://www.sambaiz.net)`, width: 24 }) dashboard.addWidgets( new cloudwatch.GraphWidget({ title: &amp;#34;Request Count&amp;#34;, left: [new cloudwatch.Metric({ metricName: &amp;#34;RequestCount&amp;#34;, namespace: &amp;#34;AWS/ApplicationELB&amp;#34;, dimensions: { &amp;#34;LoadBalancer&amp;#34;: alb.loadBalancerFullName }, statistic: &amp;#34;sum&amp;#34; })], width: 16 }), ) ) 作ったダッシュボードはコンソール上からAWSアカウントを持たない外部のユーザーに公開することができて、Cognito UeerPoolによるパスワード認証をかけられる。
発行されたリンクにアクセスすると次のような画面が表示される。
参考 cdk-patterns/serverless</description>
    </item>
    
    <item>
      <title>CDKでECS(EC2)上にLocust masterとworkerのServiceをデプロイしCloud Mapで名前解決させる</title>
      <link>https://www.sambaiz.net/article/378/</link>
      <pubDate>Tue, 17 Aug 2021 02:34:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/378/</guid>
      <description>以前、EKS上にLocustをインストールしたが、使わないときにクラスタを落としていたりすると起動を待つ必要があったり、K8sのバージョンアップに追従する必要があったりと少し不便な所があった。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
そこで今回はECS上にLocust masterとworkerのServiceをデプロイする。
Dockerfile 次のようなDockerfileを用意する。
$ cat Dockerfile FROM locustio/locust COPY locustfile.py /mnt/locust/ Stack クラスタの作成については以前のものを使い回す。
CDKでALBとECS(EC2)クラスタを作成し、ecs-cliでDocker Composeの構成をデプロイする - sambaiz-net
locustfileを更新する度にデプロイすることになるので、ALBのヘルスチェックの間隔と回数および登録解除のタイムアウト時間を減らして高速化している。また、locustに認証の仕組みがないためIPアドレスでアクセス制限をしているが、ApplicationListenerのopenがデフォルトのtrueのままだとAllow from anyoneの設定が追加されフルオープンになってしまうのでfalseにしている。
import * as cdk from &amp;#39;@aws-cdk/core&amp;#39; import * as ecs from &amp;#39;@aws-cdk/aws-ecs&amp;#39; import * as ec2 from &amp;#39;@aws-cdk/aws-ec2&amp;#39; import * as iam from &amp;#39;@aws-cdk/aws-iam&amp;#39; import * as autoscaling from &amp;#39;@aws-cdk/aws-autoscaling&amp;#39; import * as elbv2 from &amp;#39;@aws-cdk/aws-elasticloadbalancingv2&amp;#39; import * as logs from &amp;#39;@aws-cdk/aws-logs&amp;#39; export class LocustECSStack extends cdk.</description>
    </item>
    
    <item>
      <title>CDKでALBとECS(EC2)クラスタを作成し、ecs-cliでDocker Composeの構成をデプロイする</title>
      <link>https://www.sambaiz.net/article/377/</link>
      <pubDate>Sun, 15 Aug 2021 16:05:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/377/</guid>
      <description>以前、CloudFormationでLBなしのECS(EC2)最小構成を構築したが、今回はALBのTargetGroupまでをCDKで作成し、Serviceのデプロイをecs-cliで行う。 EC2をECSクラスタに登録する際の echo ECS_CLUSTER=&amp;lt;cluster_name&amp;gt; &amp;gt;&amp;gt; /etc/ecs/ecs.config といった処理は クラスタのAsgCapacityProviderにASGを指定すると追加される。
ECS(EC2)のCloudFormation最小構成 - sambaiz-net
import * as cdk from &amp;#39;@aws-cdk/core&amp;#39; import * as ecr from &amp;#39;@aws-cdk/aws-ecr&amp;#39; import * as ecs from &amp;#39;@aws-cdk/aws-ecs&amp;#39; import * as ec2 from &amp;#39;@aws-cdk/aws-ec2&amp;#39; import * as iam from &amp;#39;@aws-cdk/aws-iam&amp;#39; import * as elbv2 from &amp;#39;@aws-cdk/aws-elasticloadbalancingv2&amp;#39; import * as autoscaling from &amp;#39;@aws-cdk/aws-autoscaling&amp;#39; import { Cluster } from &amp;#39;@aws-cdk/aws-ecs&amp;#39; export class ECSEC2Stack extends cdk.Stack { constructor(scope: cdk.Construct, id: string, props?: cdk.</description>
    </item>
    
    <item>
      <title>AWS X-rayでアプリケーションのリクエストをトレースし可視化する</title>
      <link>https://www.sambaiz.net/article/376/</link>
      <pubDate>Thu, 05 Aug 2021 02:32:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/376/</guid>
      <description>AWS X-rayはリクエストをトレースして、タイムラインやサービスマップを可視化、分析できるサービス。 サービスの数が増えると見えづらくなる、どこにどれくらいのトラフィックがあって、どこで問題が起きているのかといったことを一目で確認できる。
料金はトレースの記録と取得に対してかかり、SamplingRuleの設定によって抑えることができる。
今回はローカルにdocker-composeで立ち上げたWebサーバーからセグメントデータをxray-daemon経由で送り、コンソール上で取得できることを確認する。全体のコードはGitHubにある。
xray-daemonは次のようなフォーマットのデータを受け取りバッチで送るデーモン。
$ cat segment.txt {&amp;#34;format&amp;#34;: &amp;#34;json&amp;#34;, &amp;#34;version&amp;#34;: 1} {&amp;#34;trace_id&amp;#34;: &amp;#34;1-594aed87-ad72e26896b3f9d3a27054bb&amp;#34;, &amp;#34;id&amp;#34;: &amp;#34;6226467e3f845502&amp;#34;, &amp;#34;start_time&amp;#34;: 1498082657.37518, &amp;#34;end_time&amp;#34;: 1498082695.4042, &amp;#34;name&amp;#34;: &amp;#34;test.elasticbeanstalk.com&amp;#34;} $ cat segment.txt &amp;gt; /dev/udp/127.0.0.1/2000 ローカルで動かす場合は -o を付けてインスタンスメタデータを読みに行かないようにする必要がある。ドキュメントでは.awsを/rootにマウントしているが、そうすると送る際に NoCredentialProviders: no valid providers in chain. Deprecated. になってしまう。Dockerfileを見たところxrayユーザーで動かしていることが分かったので /home/xray にしている。
version: &amp;#34;3.9&amp;#34; services: xray-daemon: image: amazon/aws-xray-daemon:3.x ports: - &amp;#34;2000:2000/udp&amp;#34; command: - &amp;#34;-o&amp;#34; # Don&amp;#39;t check for EC2 instance metadata. volumes: - ~/.aws:/home/xray/.aws:ro environment: AWS_REGION: ap-northeast-1 app: build: . ports: - &amp;#34;8080:8080&amp;#34; volumes: - ~/.</description>
    </item>
    
    <item>
      <title>SageMaker Studioの使っていないKernelを自動でシャットダウンするsagemaker-studio-auto-shutdown-extension</title>
      <link>https://www.sambaiz.net/article/373/</link>
      <pubDate>Sun, 18 Jul 2021 23:09:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/373/</guid>
      <description>SageMaker Studioを使っているとインスタンスを明示的に立ち上げることがないので、シャットダウンするのを忘れて 無駄なインスタンスコストを発生させ続けてしまうことがある。
Kernelをシャットダウンすると不要になったインスタンスもシャットダウンされるので、使っていないKernelが自動で削除されるようにしたい。 かつてはこれを実現するための仕組みを自前で用意する必要があったが、現在はsagemaker-studio-auto-shutdown-extensionが公式から提供されている。
$ git clone https://github.com/aws-samples/sagemaker-studio-auto-shutdown-extension.git $ cd sagemaker-studio-auto-shutdown-extension $ ./install_tarball.sh $ jupyter serverextension list ... sagemaker_studio_autoshutdown enabled - Validating... sagemaker_studio_autoshutdown 0.1.0 OK last activityからの時間が設定値を超過するとJupyter Notebook Server APIを呼んでKernelをシャッドダウンするようになっている。
参考 Save costs by automatically shutting down idle resources within Amazon SageMaker Studio | AWS Machine Learning Blog</description>
    </item>
    
    <item>
      <title>GlueのカスタムコネクタでBigQueryに接続する</title>
      <link>https://www.sambaiz.net/article/372/</link>
      <pubDate>Tue, 13 Jul 2021 21:02:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/372/</guid>
      <description>GlueはConnectorによって様々なデータソースに対応していて、RDSやMongoDBなど標準で提供されているもの以外にも カスタムコネクタを用いることで接続できる。今回はMarketplaceで提供されているBigQueryのカスタムコネクタを用いてBigQueryのテーブルの内容をS3に出力するJobを作成する。
GlueのETL JobをGUIで構築したりモニタリングできるサービス、Glue StudioからMarketplaceに飛んで AWS Glue Connector for Google BigQueryをSubscribeする。
BigQuery ConnectorをactivateしConnectionを作成する。
Studioでない方のGlueのConnectionからも確認できる。
JobのRoleには 117940112483.dkr.ecr.us-east-1.amazonaws.com/818e4ebf-997f-4d87-beb3-e0196e500474/cg-1025003233/bigquery-spark-connector:2.12.0-latest をpullするための GetAuthorizationToken と GCPのcredentialをSecretsManagerから持ってくるための GetSecretValue が必要。 SecretsManagerにそのままcredentialのjsonを入れて実行したところ次のエラーが出た。正しくはcredentialsというキーにbase64エンコードしたjsonの値を入れる。
IllegalArgumentException: &amp;#39;A project ID is required for this service but could not be determined from the builder or the environment. Please set a project ID using the builder.&amp;#39; また、Apache Spark SQL connector for Google BigQueryが Storage Read APIを呼ぶので bigquery.readsessions.* の権限が必要。
JobのConnection Options に parentProject としてGCPのProjectID、table としてBQのテーブル名を入れると次のようなスクリプトが生成され、 実行するとS3にBQのデータが保存される。DataFrameに変換してクエリを実行することもできる。DynamicFrameは現状overwriteできないので次のスクリプトは冪等性を持たない。</description>
    </item>
    
    <item>
      <title>Athena(Presto)とGlue(Spark)で同じクエリを実行した際に異なる値が返る原因</title>
      <link>https://www.sambaiz.net/article/370/</link>
      <pubDate>Sat, 03 Jul 2021 23:13:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/370/</guid>
      <description>AWSではGlueのデータカタログでテーブルを共有して、 アドホックな集計は手軽にクエリを実行できるPrestoベースのAthena、 バッチ集計はリソースや時間の制約を回避できるSparkベースのGlueといったように併用することができる。
AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する - sambaiz-net
ANSI互換のSQLを実行するPrestoと デフォルトでHive互換のSpark SQLを実行するSparkで 使える文法に差があったりするものの、同じクエリが使い回せることもあって、そのような場合は同じ結果が返ってくることを期待してしまうが、 次のような挙動の違いによって大きく結果が異なってしまうことがある。
数値の型 Presto 0.198 以降ではデフォルトで小数リテラルをDECIMALとして扱うが、 Athena engine version 2 (Presto 0.217)では DOUBLEになる。engine version 1 (Presto 0.172)との互換のために parse-decimal-literals-as-double が渡されているのかもしれない。
SELECT typeof(1.2), /* double */ 1 / 3.0 * 10000000 /* 3333333.333333333 */ Spark 2.3 以降も小数リテラルをDECIMALとして扱い、Glue 2.0 (Spark 2.4.3) でもそうなっている。 DECIMALのscale(小数点以下の桁数)は最大6に制限される。
print(spark.sql(&amp;#34;&amp;#34;&amp;#34;SELECT 1.2&amp;#34;&amp;#34;&amp;#34;).dtypes) # [(&amp;#39;1.2&amp;#39;, &amp;#39;decimal(2,1)&amp;#39;)] print(spark.sql(&amp;#34;&amp;#34;&amp;#34;SELECT 1 / 3.0 * 10000000&amp;#34;&amp;#34;&amp;#34;).collect()) # [Row((CAST((CAST(CAST(1 AS DECIMAL(1,0)) AS DECIMAL(2,1)) / CAST(3.0 AS DECIMAL(2,1))) AS DECIMAL(14,6)) * CAST(CAST(10000000 AS DECIMAL(8,0)) AS DECIMAL(14,6)))=Decimal(&amp;#39;3333330.</description>
    </item>
    
    <item>
      <title>AWS SDK for Java 2.x のUnable to load an HTTP implementationとクライアント変更によるlambda実行高速化</title>
      <link>https://www.sambaiz.net/article/366/</link>
      <pubDate>Thu, 10 Jun 2021 22:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/366/</guid>
      <description>AWS SDK for Java 2.x では内部で使うHTTP Clientを変更できるようになっている。現在サポートされているのは次の4つ。
&amp;lt;!-- synchronous --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;software.amazon.awssdk&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;apache-client&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;software.amazon.awssdk&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;url-connection-client&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;!-- asynchronous --&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;software.amazon.awssdk&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;netty-nio-client&amp;lt;/artifactId&amp;gt; &amp;lt;/dependency&amp;gt; &amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;software.amazon.awssdk&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;aws-crt-client&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;2.16.79-PREVIEW&amp;lt;/version&amp;gt; &amp;lt;/dependency&amp;gt; system propertyでデフォルトクライアントを指定でき、クライアントごとに変えることもできる。
// JAVA_TOOL_OPTIONS: &amp;#34;-Dsoftware.amazon.awssdk.http.async.service.impl=software.amazon.awssdk.http.crt.AwsCrtSdkHttpService&amp;#34; System.setProperty(&amp;#34;software.amazon.awssdk.http.async.service.impl&amp;#34;, &amp;#34;software.amazon.awssdk.http.crt.AwsCrtSdkHttpService&amp;#34;); S3Client s3 = S3Client .builder() .region(Region.US_EAST_1) .httpClientBuilder(UrlConnectionHttpClient.builder()) .build(); S3AsyncClient s3Async = S3AsyncClient .builder() .region(Region.US_EAST_1) .build(); いずれの実装も依存に入っていないと次のエラーになる。
Unable to load an HTTP implementation from any provider in the chain. You must declare a dependency on an appropriate HTTP implementation or pass in an SdkHttpClient explicitly to the client builder Clientを変更してListObjectするlambdaを実行したところ、cold start時の初期化時間はapache-clientが最も短いが、 総実行時間はドキュメント通りaws-crt-clientが最速となった。 全体のコードはGitHubにある。</description>
    </item>
    
    <item>
      <title>AWS App Runnerの特徴と料金、CloudFormationのResource</title>
      <link>https://www.sambaiz.net/article/361/</link>
      <pubDate>Sun, 30 May 2021 03:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/361/</guid>
      <description>AWS App Runnerは最低限の設定でロードバランシングやオートスケール、ログやメトリクス、ドメインや証明書などを備えた、 リクエストベースのステートレスなコンテナアプリケーションを動かすことができるマネージドサービス。 Elastic BeanstalkのようにEC2やALB、AutoScaling Groupなどのリソースを作成するのではなく内部に持つ。
料金は、東京リージョンの場合
 レイテンシを抑えるために常にプロビジョニングされるコンテナインスタンスのメモリ量: 0.009 USD/GB リクエストを処理するアクティブなコンテナインスタンスのCPUとメモリ量: 0.081 USD/vCPU, 0.009 USD/GB  およびビルド時間で算出される。 Lambda(+ API Gateway)と異なりリクエストが来なくても最小インスタンスのメモリ分のコストは発生するのと、 Fargateが0.05056 USD/vCPU, 0.00553 USD/GBであることを考えると少し割高に見えるが、 Lambdaの制約を受けずに、リクエストが来ない時間帯はコストが抑えられるのは良さそうだ。
FargateでECSを使う - sambaiz-net
コンテナアプリケーション管理のCLIツールAWS CopilotもApp Runnerに対応していて、 Environmentを作成するとVPCが作られるが、現状App RunnerはVPCに対応していない。 private subnetのRDSにアクセスしたり、ドキュメントでも言及されているようにElastiCacheにキャッシュを持てるようになるので対応されると嬉しい。
 (追記 2022-02-09) VPC対応された。
 CloudFormationのResourceは現状AWS::AppRunner::Serviceのみで、 これと必要なRoleだけ作れば最低限動き、https://h2w86ea3gf.ap-northeast-1.awsapprunner.com/ のようなURLでアクセスできる。 まだリリースされたばかりでAutoScalingのResourceがなかったりするので、今後のアップデートに期待している。
Resources: AccessRole: Type: AWS::IAM::Role Properties: AssumeRolePolicyDocument: Version: &amp;#39;2008-10-17&amp;#39; Statement: - Effect: Allow Principal: Service: - build.apprunner.amazonaws.com Action: sts:AssumeRole ManagedPolicyArns: - arn:aws:iam::aws:policy/service-role/AWSAppRunnerServicePolicyForECRAccess TestService: Type: AWS::AppRunner::Service Properties: ServiceName: test-service SourceConfiguration: AuthenticationConfiguration: AccessRoleArn: !</description>
    </item>
    
    <item>
      <title>CDKでGlue Data CatalogのDatabase,Table,Partition,Crawlerを作成する</title>
      <link>https://www.sambaiz.net/article/357/</link>
      <pubDate>Sun, 09 May 2021 01:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/357/</guid>
      <description>CDKでGlue Data CatlogのDatabase,Table,Partition,Crawlerを作成する。 PartitionやCrawlerはまだL2 constructが存在しない。storageDescriptorの設定はTableの実装を参考にした。
TableやPartitionはCrawlerで自動生成することができるが、ファイル数が膨大だと時間がかかることもあり、Tableはマニュアルで、Partitionの作成はAPIを呼んで作ることがあって、CDKで管理すると今の状態に関わらずデプロイによって必要なものが存在することを保証できる。
AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する - sambaiz-net
import * as cdk from &amp;#39;@aws-cdk/core&amp;#39;; import { Database, Table, DataFormat, Schema, CfnPartition } from &amp;#39;@aws-cdk/aws-glue&amp;#39;; import { Bucket } from &amp;#39;@aws-cdk/aws-s3&amp;#39;; import { format, subDays } from &amp;#39;date-fns&amp;#39; export class CdkGlueDataCatalogSampleStack extends cdk.Stack { constructor(scope: cdk.Construct, id: string, props?: cdk.StackProps) { super(scope, id, props); const db = new Database(this, &amp;#39;TestDB&amp;#39;, { databaseName: &amp;#34;test-db&amp;#34;, }) const bucket = new Bucket(this, &amp;#39;DBBucket&amp;#39;, { bucketName: `test-db-bucket-${this.</description>
    </item>
    
    <item>
      <title>CDKでKinesis Data Analytics上にPyFlinkのコードをデプロイして動かす</title>
      <link>https://www.sambaiz.net/article/334/</link>
      <pubDate>Sat, 24 Apr 2021 23:48:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/334/</guid>
      <description>KDAがPyFlinkをサポートしたのでCDKで構築して動かしてみる。 全体のコードはGitHubにある。
Kinesis Data AnalyticsのSQL, Lambdaへの出力とCDKによるリソースの作成 - sambaiz-net
今回動かすのは次の、KDSから流れてきたデータにクエリを実行し、その結果をS3に書き込む簡単なコード。
from pyflink.table import EnvironmentSettings, StreamTableEnvironment from pyflink.table.window import Tumble import os import json APPLICATION_PROPERTIES_FILE_PATH = &amp;#34;/etc/flink/application_properties.json&amp;#34; # on kda def get_application_properties(): if os.path.isfile(APPLICATION_PROPERTIES_FILE_PATH): with open(APPLICATION_PROPERTIES_FILE_PATH, &amp;#34;r&amp;#34;) as file: contents = file.read() properties = json.loads(contents) return properties else: print(&amp;#39;A file at &amp;#34;{}&amp;#34; was not found&amp;#39;.format( APPLICATION_PROPERTIES_FILE_PATH)) def property_map(props, property_group_id): for prop in props: if prop[&amp;#34;PropertyGroupId&amp;#34;] == property_group_id: return prop[&amp;#34;PropertyMap&amp;#34;] def main(): table_env = StreamTableEnvironment.</description>
    </item>
    
    <item>
      <title>AWS GlueのJobのBookmarkを有効にして前回の続きから処理を行う</title>
      <link>https://www.sambaiz.net/article/333/</link>
      <pubDate>Fri, 16 Apr 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/333/</guid>
      <description>GlueのJobのBookmarkは どこまで処理したかを記録し、次回はその続きから実行できるようにする機能。 1.0以前は対応していなかったParquetやORCも今は対応している。
AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する - sambaiz-net
Job bookamarkをEnableにして、DynamicFrameのメソッドを呼ぶ際にtranscation_ctxを渡し、job.commit()するとBookmarkされる。
例えば、S3のjsonをソースとするテーブルをカウントして出力する次のjobは、Bookmarkを有効にすると既にカウントしたものが読まれなくなるため、 トータルの件数ではなく前回との増分が出力される。 S3の結果整合性による、参照できるようになるまでのラグも考慮されていて、 単純に保存時間のみによって対象を選ぶのではなく、リストを持って対象の時間の少し前から見るようになっている。
import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, [&amp;#39;JOB_NAME&amp;#39;]) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[&amp;#39;JOB_NAME&amp;#39;], args) source = glueContext.create_dynamic_frame.from_catalog( database=&amp;#34;test&amp;#34;, table_name=&amp;#34;test&amp;#34;, transformation_ctx=&amp;#34;source&amp;#34;) source.toDF().createOrReplaceTempView(&amp;#34;t&amp;#34;) df = spark.sql(&amp;#34;SELECT COUNT(1) as cnt FROM t&amp;#34;) df.write.mode(&amp;#34;append&amp;#34;).csv( &amp;#34;s3://*****/test-table-summary&amp;#34;, compression=&amp;#34;gzip&amp;#34;) job.</description>
    </item>
    
    <item>
      <title>CDKでStep Functionsによるワークフローを構築する</title>
      <link>https://www.sambaiz.net/article/332/</link>
      <pubDate>Sat, 10 Apr 2021 21:19:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/332/</guid>
      <description>Step FunctionsはLambdaやGlueのJobなどからなるワークフローを構築できるサービス。これをCDKで構築する。 全体のコードはGitHubにある。
taskをnext()で繋げたワークフローをStateMachineに渡す流れ。 複数のtaskを同時に実行するParallel()は配列をoutputする。 条件分岐を行うChoice()でotherwise()を指定しないとエラーになる。
Objectの中でinputの値を参照する場合、キーの名前を.$で終える必要があり、 fromJsonPathAt() を用いるとキーに.$が追加される。
import * as cdk from &amp;#39;@aws-cdk/core&amp;#39;; import * as sfn from &amp;#39;@aws-cdk/aws-stepfunctions&amp;#39;; import * as tasks from &amp;#39;@aws-cdk/aws-stepfunctions-tasks&amp;#39;; import { Function, Code, Runtime } from &amp;#39;@aws-cdk/aws-lambda&amp;#39;; import * as path from &amp;#39;path&amp;#39; export class CdkStepfunctionsSampleStack extends cdk.Stack { constructor(scope: cdk.Construct, id: string, props?: cdk.StackProps) { super(scope, id, props); const fetchScoreFunction = new Function(this, &amp;#39;FetchScoreFunction&amp;#39;, { runtime: Runtime.GO_1_X, code: Code.fromAsset(path.join(__dirname, &amp;#39;lambda&amp;#39;, &amp;#39;fetchScore&amp;#39;)), handler: &amp;#34;main&amp;#34; }) const invokeFetchScoreFunction1 = new tasks.</description>
    </item>
    
    <item>
      <title>Application Auto Scalingのcustom-resourceによるKinesis Data Streamsのオートスケール設定</title>
      <link>https://www.sambaiz.net/article/331/</link>
      <pubDate>Sun, 21 Mar 2021 23:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/331/</guid>
      <description>Application Auto Scalingは、Auto Scaling groupによるEC2のオートスケールのようなことを他のリソースでも行えるようにするサービスで、 DynamoDBのキャパシティやECSのServiceなどコンソール上から設定できるものだけではなく、自前のAPIでハンドリングすることで任意のリソースをこの仕組みに乗せることができる。これを用いてKinesis Data Streamsのオートスケールを行う手法を紹介しているのが次の記事。
Scale Amazon Kinesis Data Streams with AWS Application Auto Scaling | AWS Big Data Blog
サンプルテンプレートの設定項目を見ていく。
ScalableTarget ServiceNamespaceでdynamodbやecsといった名前空間を、ScalableDimensionでdynamodb:table:ReadCapacityUnitsやecs:service:DesiredCountといった増減する値を指定する。 ResourceIdにはScalableDimensionのResourceTypeに応じた対象を指定するわけだが、 custom-resourceの場合は状態を確認したり更新するためのAPIのURLを指定するようだ。 ドキュメントにはOutputValueの値と書いてあるが、特に関係ないように見える。
Roleにはcloudwatch:Describe/Put/DeleteAlarmsおよびexecute-api:Invoke*の権限を与える。
KinesisAutoScaling: Type: AWS::ApplicationAutoScaling::ScalableTarget DependsOn: LambdaScaler Properties: MaxCapacity: 8 MinCapacity: 1 ResourceId: !Sub https://${MyApi}.execute-api.${AWS::Region}.amazonaws.com/prod/scalableTargetDimensions/${MyKinesisStream} RoleARN: !Sub ${CustomApplicationAutoScalingServiceRole.Arn} ScalableDimension: &amp;#39;custom-resource:ResourceType:Property&amp;#39; ServiceNamespace: custom-resource ScalingPolicy AdjustmentTypeとScalingAdjsutmentの値によって増減値を設定する。 AdjustmentTypeにはChangeInCapacity | ExactCapacity | PercentChangeInCapacityのいずれかを指定する。
AutoScalingPolicyOut: Type : &amp;#34;AWS::ApplicationAutoScaling::ScalingPolicy&amp;#34; DependsOn: KinesisAutoScaling Properties: PolicyName: KinesisScaleOut PolicyType: StepScaling ResourceId: !Sub https://${MyApi}.</description>
    </item>
    
    <item>
      <title>Goのio packageのReader/Writer/Closer/Seeker interfaceとストリーム処理</title>
      <link>https://www.sambaiz.net/article/330/</link>
      <pubDate>Thu, 11 Mar 2021 20:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/330/</guid>
      <description>Goのio packageにはデータの読み書きに関わるインタフェース、Reader/Writer/Closer/Seeker およびこれらを組み合わせた ReadSeeker などが定義されている。
 io.Reader  最大でlen(p)バイトpに読み込んで、読み込んだバイト数を返す。 最後まで読んだらerrでio.EOFを返すが、これは最後のバイトと同時でもその後でも良いことになっている。
type Reader interface { Read(p []byte) (n int, err error) } var EOF = errors.New(&amp;#34;EOF&amp;#34;)  io.Writer  データを書き込み、そのバイト数を返す。全て書き込めなかった(len(p) != n)場合はエラーを返す必要がある。
type Writer interface { Write(p []byte) (n int, err error) }  io.Closer  2回以上呼んだときの挙動は規定されていない。
type Closer interface { Close() error }  io.Seeker  オフセットと起点を渡して読み書きする地点を変更し、Startからのオフセットを返す。
type Seeker interface { Seek(offset int64, whence int) (int64, error) } const ( SeekStart = 0 // seek relative to the origin of the file 	SeekCurrent = 1 // seek relative to the current offset 	SeekEnd = 2 // seek relative to the end ) ReaderとWriterを繋げてストリーム処理を行うことで、メモリ使用量を抑えることができる。 io.</description>
    </item>
    
    <item>
      <title>Athena(Presto)でWindow関数を用いた集計を行う</title>
      <link>https://www.sambaiz.net/article/328/</link>
      <pubDate>Wed, 24 Feb 2021 22:03:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/328/</guid>
      <description>Athena(Presto)でSUM()やAVG()といった集計関数にOVERを付けてWindow集計を行う。
Window Functions — Presto 0.247 Documentation
次のテストデータを使う。
$ cat test-data.csv {&amp;#34;date&amp;#34;:&amp;#34;2021-02-01&amp;#34;,&amp;#34;user&amp;#34;:1,&amp;#34;value&amp;#34;:10} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-01&amp;#34;,&amp;#34;user&amp;#34;:2,&amp;#34;value&amp;#34;:20} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-01&amp;#34;,&amp;#34;user&amp;#34;:3,&amp;#34;value&amp;#34;:30} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-01&amp;#34;,&amp;#34;user&amp;#34;:1,&amp;#34;value&amp;#34;:40} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-01&amp;#34;,&amp;#34;user&amp;#34;:2,&amp;#34;value&amp;#34;:50} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-01&amp;#34;,&amp;#34;user&amp;#34;:3,&amp;#34;value&amp;#34;:60} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-02&amp;#34;,&amp;#34;user&amp;#34;:1,&amp;#34;value&amp;#34;:100} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-02&amp;#34;,&amp;#34;user&amp;#34;:2,&amp;#34;value&amp;#34;:200} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-02&amp;#34;,&amp;#34;user&amp;#34;:3,&amp;#34;value&amp;#34;:300} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-02&amp;#34;,&amp;#34;user&amp;#34;:1,&amp;#34;value&amp;#34;:400} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-02&amp;#34;,&amp;#34;user&amp;#34;:2,&amp;#34;value&amp;#34;:500} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-02&amp;#34;,&amp;#34;user&amp;#34;:3,&amp;#34;value&amp;#34;:600} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-03&amp;#34;,&amp;#34;user&amp;#34;:1,&amp;#34;value&amp;#34;:1} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-03&amp;#34;,&amp;#34;user&amp;#34;:2,&amp;#34;value&amp;#34;:2} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-03&amp;#34;,&amp;#34;user&amp;#34;:3,&amp;#34;value&amp;#34;:3} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-03&amp;#34;,&amp;#34;user&amp;#34;:1,&amp;#34;value&amp;#34;:4} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-03&amp;#34;,&amp;#34;user&amp;#34;:2,&amp;#34;value&amp;#34;:5} {&amp;#34;date&amp;#34;:&amp;#34;2021-02-03&amp;#34;,&amp;#34;user&amp;#34;:3,&amp;#34;value&amp;#34;:6} PARTITION BY その値ごとに集計する。
SELECT date, user, value, SUM(value) OVER (PARTITION BY user) as sum_value FROM test ORDER BY date, user; GROUP BYしているわけではないので、行数は減らない。
&amp;#34;date&amp;#34;,&amp;#34;user&amp;#34;,&amp;#34;value&amp;#34;,&amp;#34;sum_value&amp;#34; &amp;#34;2021-02-01&amp;#34;,&amp;#34;1&amp;#34;,&amp;#34;40&amp;#34;,&amp;#34;555&amp;#34; &amp;#34;2021-02-01&amp;#34;,&amp;#34;1&amp;#34;,&amp;#34;10&amp;#34;,&amp;#34;555&amp;#34; &amp;#34;2021-02-01&amp;#34;,&amp;#34;2&amp;#34;,&amp;#34;20&amp;#34;,&amp;#34;777&amp;#34; &amp;#34;2021-02-01&amp;#34;,&amp;#34;2&amp;#34;,&amp;#34;50&amp;#34;,&amp;#34;777&amp;#34; &amp;#34;2021-02-01&amp;#34;,&amp;#34;3&amp;#34;,&amp;#34;60&amp;#34;,&amp;#34;999&amp;#34; &amp;#34;2021-02-01&amp;#34;,&amp;#34;3&amp;#34;,&amp;#34;30&amp;#34;,&amp;#34;999&amp;#34; &amp;#34;2021-02-02&amp;#34;,&amp;#34;1&amp;#34;,&amp;#34;100&amp;#34;,&amp;#34;555&amp;#34; &amp;#34;2021-02-02&amp;#34;,&amp;#34;1&amp;#34;,&amp;#34;400&amp;#34;,&amp;#34;555&amp;#34; &amp;#34;2021-02-02&amp;#34;,&amp;#34;2&amp;#34;,&amp;#34;500&amp;#34;,&amp;#34;777&amp;#34; &amp;#34;2021-02-02&amp;#34;,&amp;#34;2&amp;#34;,&amp;#34;200&amp;#34;,&amp;#34;777&amp;#34; &amp;#34;2021-02-02&amp;#34;,&amp;#34;3&amp;#34;,&amp;#34;600&amp;#34;,&amp;#34;999&amp;#34; &amp;#34;2021-02-02&amp;#34;,&amp;#34;3&amp;#34;,&amp;#34;300&amp;#34;,&amp;#34;999&amp;#34; &amp;#34;2021-02-03&amp;#34;,&amp;#34;1&amp;#34;,&amp;#34;1&amp;#34;,&amp;#34;555&amp;#34; &amp;#34;2021-02-03&amp;#34;,&amp;#34;1&amp;#34;,&amp;#34;4&amp;#34;,&amp;#34;555&amp;#34; &amp;#34;2021-02-03&amp;#34;,&amp;#34;2&amp;#34;,&amp;#34;5&amp;#34;,&amp;#34;777&amp;#34; &amp;#34;2021-02-03&amp;#34;,&amp;#34;2&amp;#34;,&amp;#34;2&amp;#34;,&amp;#34;777&amp;#34; &amp;#34;2021-02-03&amp;#34;,&amp;#34;3&amp;#34;,&amp;#34;6&amp;#34;,&amp;#34;999&amp;#34; &amp;#34;2021-02-03&amp;#34;,&amp;#34;3&amp;#34;,&amp;#34;3&amp;#34;,&amp;#34;999&amp;#34; ORDER BY (+ RANGE) ソートし範囲を絞って集計する。 デフォルトの集計範囲は RANGE UNBOUNDED PRECEDING なので、次の例はそれまでの値の行で集計されている。</description>
    </item>
    
    <item>
      <title>Amazon Forecastで時系列データの予測を行う</title>
      <link>https://www.sambaiz.net/article/327/</link>
      <pubDate>Sun, 21 Feb 2021 01:04:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/327/</guid>
      <description>Amazon Forecastは機械学習によって時系列データの予測を行うマネージドサービス。 ドメインやアルゴリズムを選んでデータを投入すればそれらしい出力が得られる。 まずこれで予測してみて、その結果をベースラインとしてSageMakerなどで自作したモデルを評価するといった使い方もできる。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
料金はデータストレージに$0.088/GB、学習に$0.24/hour、予測に$0.60/1000回かかる。
今回は開発者ガイドと同じく 電力消費量のデータセットを用いて動かしてみる。
$ head electricityusagedata.csv 2014-01-01 01:00:00,2.53807106598985,client_0 2014-01-01 01:00:00,23.648648648648624,client_1 2014-01-01 01:00:00,0.0,client_2 2014-01-01 01:00:00,144.81707317073176,client_3 ... データのインポート まずドメインを選んでDataset groupを作成する。Dataset groupには予測対象の時系列データに加えて、他の関連する時系列データやメタデータを含めることができる。
CSVをS3に置き、それを読めるRoleを渡してインポートする。データの間隔とカラムの順は元データと合致するように設定する。 タイムスタンプは yyyy-MM-dd か yyyy-MM-dd HH:mm:ss の形式である必要がある。
学習 インポートが終わるとTrain predictorできるようになる。 予測の間隔と期間を設定し、アルゴリズムを、AutoMLか、ARIMAやCNN-QRといったものの中からマニュアルで選んで学習を始める。
時系列データのMAモデルとARモデル、その定常性と反転可能性 - sambaiz-net
ハイパーパラメータの最適化や、国ごとの祝日や天気を考慮するオプションもある。
予測 学習が終わるとCreate a forecastして予測値を得られる。
Goでの取得はこんな感じ。
package main import ( &amp;#34;context&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;github.com/aws/aws-sdk-go/aws&amp;#34; &amp;#34;github.com/aws/aws-sdk-go/aws/session&amp;#34; &amp;#34;github.com/aws/aws-sdk-go/service/forecastqueryservice&amp;#34; ) func main() { mySession := session.Must(session.NewSession()) svc := forecastqueryservice.New(mySession) ctx := context.TODO() output, err := svc.</description>
    </item>
    
    <item>
      <title>Kinesis Data Analyticsによる集計遅延箇所の特定</title>
      <link>https://www.sambaiz.net/article/324/</link>
      <pubDate>Sun, 07 Feb 2021 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/324/</guid>
      <description>Kinesis Data Analytics (KDA)はマッピングしたカラムに加えて、Kinesis Data Streams (KDS)に入った時間 APPROXIMATE_ARRIVAL_TIME とKDAのin-app STREAMに入った時間 ROWTIME をソースに含める。これらとログのタイムスタンプを合わせることで集計遅延が起きた際にどこが原因になっているかを特定することができる。
Log aggregatorでのタイムスタンプの付加 集計対象のログを集約サーバーを中継して送っている場合、そこでのタイムスタンプを付加しておくとバッファリングによる滞りを検知できる。 fluentdのmonitor_agentの値をDatadogなどに送って監視することもできるが、 集計ウィンドウが短い場合、collection_intervalの関係で、正確な状態を把握しづらいことがある。
fluentdのmonitor_agentのデータをGoでGoogle Stackdriverに送って監視する - sambaiz-net
レコードへの付加にはfluentdのコアに含まれているrecord_transformerを使うこともできるが、性能が良いfluent-plugin-record-modifierという選択肢もある。
fluentdのrecord_transformerでログを加工する - sambaiz-net
&amp;lt;filter test.log&amp;gt; @type record_modifier &amp;lt;record&amp;gt; ts_aggr ${Time.now().strftime(&amp;#39;%Y-%m-%dT%H:%M:%S.%L%z&amp;#39;)} &amp;lt;/record&amp;gt; &amp;lt;/filter&amp;gt; また、ログのタイムスタンプをtime_keyとしている場合、keep_time_key trueにするか&amp;lt;inject&amp;gt;してKDSに送られるようにする。
fluentdのとでtime_formatを指定しなかった場合の挙動と内部処理 - sambaiz-net
TIMESTAMPのパース タイムスタンプ文字列をTIMESTAMP型のカラムにマッピングすることもできるが、2021-01-01T00:00:00.000+0900形式の文字列をマッピングしたところ、UTCで18時間後ろにずれた時間になってしまった。
そこで一度CHARにマッピングしてからフォーマットを明示してCHAR_TO_TIMESTAMP(&#39;&amp;lt;format_string&amp;gt;&#39;,&amp;ldquo;column_name&amp;rdquo;)することにした。 &amp;lt;format_string&amp;gt;はJavaのSimpleDateFormatで記述し、この例の場合はyyyy-MM-dd&#39;&#39;T&#39;&#39;HH:mm:ss.SSSZのようになる。
マッピングの変更後にSourcesの方に何も出てこなくなった場合は、クエリの方で問題が起きている可能性があるので、コンソール上でクエリを保存してみてエラーにならないか確認し、必要なら修正する。
クエリ Kinesis Data AnalyticsのSQL, Lambdaへの出力とCDKによるリソースの作成 - sambaiz-net
各タイムスタンプのMINを取って現在時刻との差を取れば、そのウインドウ集計対象のログのうち最も古いものの各箇所からのレイテンシが出せる。
CREATE OR REPLACE PUMP &amp;#34;TEST_PUMP&amp;#34; AS INSERT INTO &amp;#34;TEST_STREAM&amp;#34; (&amp;#34;min_ts&amp;#34;, &amp;#34;min_ts_kds&amp;#34;, &amp;#34;min_ts_kda&amp;#34;) SELECT STREAM MIN(CHAR_TO_TIMESTAMP(&amp;#39;yyyy-MM-dd&amp;#39;&amp;#39;T&amp;#39;&amp;#39;HH:mm:ss.SSSZ&amp;#39;, &amp;#34;ts&amp;#34;)) as min_ts, MIN(&amp;#34;APPROXIMATE_ARRIVAL_TIME&amp;#34;) as min_ts_kds, &amp;#34;ROWTIME&amp;#34; as min_ts_kda FROM &amp;#34;SOURCE_SQL_STREAM_001&amp;#34; GROUP BY STEP(&amp;#34;SOURCE_SQL_STREAM_001&amp;#34;.</description>
    </item>
    
    <item>
      <title>CDKでCognito UserPoolとClientを作成しトリガーやFederationを設定する</title>
      <link>https://www.sambaiz.net/article/318/</link>
      <pubDate>Sun, 06 Dec 2020 18:25:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/318/</guid>
      <description>CloudFormationでCognito UserPoolを作成すると、以前はドメインやFederationの設定などを手作業で行う必要があったが、 去年の10月に諸々のリソースが追加され、その必要がなくなった。
API GatewayでCognitoの認証をかけて必要ならログイン画面に飛ばす処理をGoで書く - sambaiz-net
今回はCDKの高レベルAPIを用いて、UserPoolとClientを作成し、トリガーやGoogleのFederationの設定を行って、特定のGoogleアカウントでのみ登録されるようにする。 全体のコードはGitHubにある。
Cognito UserPoolのPreSignUp時に呼ばれるLambdaで登録ユーザーを制限する - sambaiz-net
UserPool 事前にdomainPrefixを決めておき、OAuthのclient_idとsecretをSecretsManagerに置いておく。
standardAttributesと、ログイン時にusernameの代わりとなるsignInAliasesは後から変更できない。 変更してデプロイしてもreplaceではなくエラーになってしまう。 トリガーのruntimeはCDKと同じNodeにしても良いがバージョンのライフサイクルが早く追従するのが大変なのでGoにしている。
private createUserPool(userPoolName: string, domainPrefix: string, signUpAllowEmails: string[]): UserPool { const userPool = new UserPool(this, &amp;#39;UserPool&amp;#39;, { userPoolName, standardAttributes: { email: { required: true, mutable: true }, fullname: { required: true, mutable: true }, }, signInAliases: { username: true, email: true }, lambdaTriggers: { preSignUp: new Function(this, &amp;#39;PreSignUpFunction&amp;#39;, { runtime: Runtime.GO_1_X, code: Code.fromAsset(path.join(__dirname, &amp;#39;userPoolTrigger&amp;#39;)), handler: &amp;#34;preSignUp.</description>
    </item>
    
    <item>
      <title>EKSにKubeflowをインストールする</title>
      <link>https://www.sambaiz.net/article/316/</link>
      <pubDate>Sun, 29 Nov 2020 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/316/</guid>
      <description>Kubernetes上で機械学習を行うためのツールキットKubeflowを EKSにインストールする。 m5.large * 4のクラスタをCDKで作成した。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
まずKubeflowのCLIツールkfctlをインストールする。 内部でeksctlが呼ばれるがフラグでprofileを指定できないので環境変数に入れておく。
$ curl -L https://github.com/kubeflow/kfctl/releases/download/v1.2.0/kfctl_v1.2.0-0-gbc038f9_darwin.tar.gz &amp;gt; kfctl.tar.gz $ tar -xvf kfctl.tar.gz $ ./kfctl version kfctl v1.2.0-0-gbc038f9 $ eksctl version 0.32.0 $ export AWS_PROFILE=*** AWS用の設定ファイルを持ってきて、RegionやRole、Cognito UserPoolあるいはusernameとpasswordによる認証の設定をKfAwsPluginに書く。UserPoolの作成もCDKで行える。それとACMで証明書を発行しておく。
CDKでCognito UserPoolとClientを作成しトリガーやFederationを設定する - sambaiz-net
AWSのサービスにアクセスするのにServiceAccountに関連づけられたRoleを用いる場合はenablePodIamPolicy: trueにして、ワーカーノードのRoleを用いる場合はrolesにそのロール名を書く。 KfDefにはManifestごとのKustomizeに関する設定が並んでいるがそのままで問題ない。
kustomizeでkubernetesのmanifestを環境ごとに生成する - sambaiz-net
$ mkdir &amp;lt;cluster_name&amp;gt; &amp;amp;&amp;amp; cd &amp;lt;cluster_name&amp;gt; $ curl -L https://raw.githubusercontent.com/kubeflow/manifests/v1.1-branch/kfdef/kfctl_aws_cognito.v1.2.0.yaml &amp;gt; kfctl_aws.yaml $ cat kfctl_aws.yaml apiVersion: kfdef.apps.kubeflow.org/v1 kind: KfDef metadata: namespace: kubeflow spec: applications: - kustomizeConfig: repoRef: name: manifests path: namespaces/base name: namespaces .</description>
    </item>
    
    <item>
      <title>GoでAthenaのクエリを実行する</title>
      <link>https://www.sambaiz.net/article/309/</link>
      <pubDate>Sat, 14 Nov 2020 16:19:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/309/</guid>
      <description>segmentio/go-athenaを使う。database/sqlのドライバーとして提供されていて、 StartQueryExecution()と stateのポーリング、 値のキャストを行う。
package main import ( &amp;#34;database/sql&amp;#34; &amp;#34;errors&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;github.com/aws/aws-sdk-go/aws/session&amp;#34; &amp;#34;github.com/aws/aws-sdk-go/service/sts&amp;#34; _ &amp;#34;github.com/segmentio/go-athena&amp;#34; ) func outputLocation() (string, error) { svc := sts.New(session.Must(session.NewSession())) result, err := svc.GetCallerIdentity(&amp;amp;sts.GetCallerIdentityInput{}) if err != nil { return &amp;#34;&amp;#34;, err } if result.Account == nil || svc.Config.Region == nil { return &amp;#34;&amp;#34;, errors.New(&amp;#34;account or region is nil&amp;#34;) } return fmt.Sprintf(&amp;#34;s3://aws-athena-query-results-%s-%s&amp;#34;, *result.Account, *svc.Config.Region), nil } func execute(query string) (*sql.Rows, error) { loc, err := outputLocation() if err !</description>
    </item>
    
    <item>
      <title>VSCodeのdevcontainerにSAM CLIをインストールしlocal invokeする</title>
      <link>https://www.sambaiz.net/article/301/</link>
      <pubDate>Mon, 12 Oct 2020 09:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/301/</guid>
      <description>VSCodeのdevcontainerにAWS SAM CLIをインストールしてDockerを用いたlocal invokeもできるようにする。
VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する - sambaiz-net
HomebrewとSAM CLIのインストール 手順に従ってbrewでインストールする。
 Homebrewがsudoとgit、psを必要とするのでインストールする デフォルトのrootでHomebrewをインストールするとDon&amp;rsquo;t run this as root!になるので non-root userを作って そのユーザーで実行する brew install aws-sam-cliでsamはインストールできているのにexit 1して失敗するのを握り潰している 次にdockerが必要となるので入れている  FROMdebian:buster ARG USERNAME=vscode ARG USER_UID=1000 ARG USER_GID=$USER_UID # Create the user RUN groupadd --gid $USER_GID $USERNAME \  &amp;amp;&amp;amp; useradd --uid $USER_UID --gid $USER_GID -m $USERNAME \  # # [Optional] Add sudo support. Omit if you don&amp;#39;t need to install software after connecting.</description>
    </item>
    
    <item>
      <title>ElastiCacheでRedisクラスタを作成する際の設定</title>
      <link>https://www.sambaiz.net/article/303/</link>
      <pubDate>Fri, 09 Oct 2020 00:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/303/</guid>
      <description>CacheClusterとReplicationGroup コンソールでは意識することがないが、CloudFormationでElastiCacheクラスターを構築する場合は CacheClusterと ReplicationGroupという概念が存在する。 プライマリノードとリードレプリカノードは別のCacheClusterに作成され、それらをカプセル化したのがReplicationGroup。
Cluster Mode 有効にすると最大90までシャードを増やせるようになる。作成後にCluster Modeを有効にはできない。
シャードには1つのPrimary Nodeと0-5つのReplica Nodeが含まれる。 Replica NodeはPrimary NodeのデータをレプリケーションするRead Onlyなノード。手動でPrimary Nodeに昇格させたり、NodeやAZの障害時に自動で昇格したりする。
CloudFormationではシャード数はReplicationGroupのNumNodeGroupsで、レプリカノードの数はReplicasPerNodeGroupで設定できる。
リクエストはキーから計算されるハッシュスロットによって各シャードに分散され、 ClUSTER SLOTSコマンドでハッシュスロットとノードのマッピングが取れる。 go-redisのようなCluster対応クライアントは内部でこのマッピングと計算したハッシュスロットからリクエストを送るノードを決定している。
Node Size 少なくとも総アイテムサイズをシャード数で割った分がメモリに乗るようにする。 これに加えてスナップショットを作成する際にフォークしたプロセスで実行されるBGSAVEや、フェイルオーバーで使うメモリが必要。 reserved-memory-percentパラメータ(以前のバージョンではreserved-memoryだった)でmaxmemoryの25%を予約することが推奨されており、デフォルトは25になっている。
Subnet Group クラスタに設定するVPCのサブネットの集合。クラスタ作成後には変更できない。</description>
    </item>
    
    <item>
      <title>Kinesis Data AnalyticsのSQL, Lambdaへの出力とCDKによるリソースの作成</title>
      <link>https://www.sambaiz.net/article/302/</link>
      <pubDate>Sat, 03 Oct 2020 19:44:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/302/</guid>
      <description>Kinesis Data AnalyticsでStreaming SQLを実行し、 Lambdaに送る。ほかの接続先としてData StreamやFirehoseがあり、フォーマットはJSONとCSVから選べる。
Kinesis Streams/Firehose/Analyticsを試す - sambaiz-net
STREAMとPUMP (in-application) STREAMを作成し、PUMPで他のSTREAMをSELECTした結果をINSERTすることでデータを流していく。出力先を設定する際にSTREAMを選ぶ。 STREAMはRDBのテーブルのようにカラムを持ち、JOINもできる。
CREATE OR REPLACE STREAM &amp;#34;TEMPSTREAM&amp;#34; ( &amp;#34;column1&amp;#34; BIGINT NOT NULL, &amp;#34;column2&amp;#34; INTEGER, &amp;#34;column3&amp;#34; VARCHAR(64)); CREATE OR REPLACE PUMP &amp;#34;SAMPLEPUMP&amp;#34; AS INSERT INTO &amp;#34;TEMPSTREAM&amp;#34; (&amp;#34;column1&amp;#34;, &amp;#34;column2&amp;#34;, &amp;#34;column3&amp;#34;) SELECT STREAM inputcolumn1, inputcolumn2, inputcolumn3 FROM &amp;#34;INPUTSTREAM&amp;#34;; Windowed Queries  Tumbling Windows  固定の重複しない区間で集計するクエリ。次のように書くと60秒区切りで集計できる。
GROUP BY STEP(&amp;#34;SOURCE_SQL_STREAM_001&amp;#34;.ROWTIME BY INTERVAL &amp;#39;60&amp;#39; SECOND) ROWTIMEは 最初のSTREAMにデータが入った時のタイムスタンプが格納される特別なカラム。SELECTしなくても自動で渡される。
 Stagger Windows  固定の区間ではなく最初に対象パーティションのデータが届くとそこからINTERVALの区間のWindowが始まる。 Tumbling Windowsではデータに含まれるタイムスタンプで集計する場合、遅れて届くことで同じ区間のレコードが分かれてしまう問題があるがそれを緩和できる。</description>
    </item>
    
    <item>
      <title>EKS上のLocustから負荷をかける際のリソースの割り当てやインスタンスタイプの調整</title>
      <link>https://www.sambaiz.net/article/299/</link>
      <pubDate>Sun, 20 Sep 2020 16:39:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/299/</guid>
      <description>EKS上にLocustをインストールしたのだが、ユーザーを増やしてもRPSが大して伸びない。リソースを調整してなるべく効率的に負荷をかけられるようにする。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
前提 実行するシナリオは次のGETリクエストを送るだけのもの。Chartの都合で0.x系のlocustfileになっている。
from locust import HttpLocust, TaskSet, task class MyTaskSet(TaskSet): @task def index(self): self.client.get(&amp;#34;/&amp;#34;) class MyUser(HttpLocust): task_set = MyTaskSet min_wait = 5 max_wait = 15 ちなみに負荷をかける対象はECS+Fargateに立ち上げたAPIサーバーで、こちらが問題にならないよう余裕を持って動かしている。 スケールするからといってAPI Gatewayなどに向けるとリクエスト数による多額の課金が発生し得るので注意だ。
ECSでアプリケーションを動かすBoilerplateを作った - sambaiz-net
なお、ファイルディスクリプタの数は元から十分大きかったため特に変更していない。
ファイルディスクリプタの上限を増やす - sambaiz-net
$ kubectl exec tryeksstackclusterchartlocustchart1abdd876-worker-d5c7b85cbq6sh -- /bin/sh -c &amp;#34;ulimit -n&amp;#34; 1048576 ワーカー数とリソースの割り当て m5.large (2vCPU, メモリ10GiB)の2ノードに5ワーカーを立ち上げ負荷をかけたところ230RPSあたりで頭打ちになってしまった。
Container Insightsのメトリクスを見るとワーカーPodのCPUの使用率が100%に張り付いていることが分かる。
CloudWatch Container InsightsでEKSのメトリクスを取得する - sambaiz-net
ノードのCPUは40%ほどしかリクエストされておらず同量のlimitsがかかっているので、ワーカーのリクエストCPUを増やすか数を増やせば簡単にRPSを増やせそうだ。
まずはリクエストCPUを100mから500mに増やした。2.5倍ではないのは40%の中にはkube-systemやContainer InsightsのDaemonSetが含まれているため。リクエスト率は90%ほどになった。
$ kubectl describe nodes ... Non-terminated Pods: (9 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- amazon-cloudwatch cloudwatch-agent-sfz5l 200m (10%) 200m (10%) 200Mi (6%) 200Mi (6%) 5m49s amazon-cloudwatch fluentd-cloudwatch-5vnhg 100m (5%) 0 (0%) 200Mi (6%) 400Mi (13%) 5m49s default tryeksstackclusterchartlocustchart1abdd876-master-595c44cczwmcm 100m (5%) 100m (5%) 128Mi (4%) 128Mi (4%) 4m54s default tryeksstackclusterchartlocustchart1abdd876-worker-fddd54db4d7xr 500m (25%) 500m (25%) 128Mi (4%) 128Mi (4%) 4m54s default tryeksstackclusterchartlocustchart1abdd876-worker-fddd54dbz2kpj 500m (25%) 500m (25%) 128Mi (4%) 128Mi (4%) 4m54s kube-system aws-node-875rb 10m (0%) 0 (0%) 0 (0%) 0 (0%) 6m39s kube-system coredns-75b44cb5b4-7qpfl 100m (5%) 0 (0%) 70Mi (2%) 170Mi (5%) 4m54s kube-system coredns-75b44cb5b4-gc6mv 100m (5%) 0 (0%) 70Mi (2%) 170Mi (5%) 4m54s kube-system kube-proxy-wj8fk 100m (5%) 0 (0%) 0 (0%) 0 (0%) 6m39s worker: { replicaCount: 5, config: { configmapName: configmap.</description>
    </item>
    
    <item>
      <title>CloudWatch Container InsightsでEKSのメトリクスを取得する</title>
      <link>https://www.sambaiz.net/article/300/</link>
      <pubDate>Fri, 18 Sep 2020 19:58:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/300/</guid>
      <description>CloudWatch Container Insightsは EKS/EC2上で動くK8sクラスタおよびECSのメトリクスを取得する機能。今回はEKSで使う。
CDKでクラスタを作成する場合、ECSではcontainerInsightsをtrueにすることでセットアップされるが、EKSにはまだ存在しないため手動で行う。PRは上がっている。
CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う - sambaiz-net
まずCloudWatchにログとメトリクスを送れるようにするため、ワーカーノードのIAMロールか、 PodのServiceAccountに関連づけられたIAMロールに CloudWatchAgentServerPolicyをアタッチする。今回はCDKで先にクラスタやロールを作るため前者の方法を取る。
cluster.defaultNodegroup?.role.addManagedPolicy(ManagedPolicy.fromAwsManagedPolicyName(&amp;#39;CloudWatchAgentServerPolicy&amp;#39;)) セットアップは次のコマンドの実行で完了し、amazon-cloudwatchネームスペースにCloudWatchメトリクスを送信するエージェントとFluentdのDaemonSetや、 各リソースを取得するClusterRoleやServiceAccountなどが作成される。cluster-nameとregion-nameの部分は書き換える。
$ curl https://raw.githubusercontent.com/aws-samples/amazon-cloudwatch-container-insights/latest/k8s-deployment-manifest-templates/deployment-mode/daemonset/container-insights-monitoring/quickstart/cwagent-fluentd-quickstart.yaml | sed &amp;#34;s/{{cluster_name}}/cluster-name/;s/{{region_name}}/cluster-region/&amp;#34; | kubectl apply -f - $ kubectl get daemonset --namespace amazon-cloudwatch NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE cloudwatch-agent 2 2 2 2 2 &amp;lt;none&amp;gt; 60s fluentd-cloudwatch 2 2 2 2 2 &amp;lt;none&amp;gt; 57s メトリクスはCloudWatchのContainerInsightsネームスペースに送られる。 Podに割り当てられたCPUとメモリの使用率を出してみたところ、負荷をかけた際にCPUが100%に張り付いたので正しく送られていそうだ。</description>
    </item>
    
    <item>
      <title>CDKでEKSクラスタの作成からHelm ChartでのLocustのインストールまでを一気に行う</title>
      <link>https://www.sambaiz.net/article/297/</link>
      <pubDate>Wed, 16 Sep 2020 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/297/</guid>
      <description>以前、Helmでlocustをインストールしたが、今回はEKSにCDKでインストールする。CDKだとクラスタの作成からできるのでcdk deployで一気に環境が整う。
KubernetesにHelmでLocustによる分散負荷試験環境を立てる - sambaiz-net
$ npm run cdk -- --version 1.62.0 (build 8c2d7fc) まずVPCとClusterを作成する。mastersRoleをユーザーもassumeできるようPrincipalにAWSアカウントも入れている。
AWSのAssumeRole - sambaiz-net
その後、実行するタスクを記述したスクリプトlocustfileのConfigMapを作成し、 これをChartのworker.config.configmapNameで参照する。キー名を間違えがち。 ChartのリポジトリはHelm Hubのもの。
 追記 (2020-12-21): 以前はHelm Hubの https://kubernetes-charts.storage.googleapis.com/ を参照していたが、helm/charts リポジトリがdeprecated になり削除されてしまったので、archiveを参照するようにした。
 import * as cdk from &amp;#39;@aws-cdk/core&amp;#39;; import { Cluster, KubernetesVersion, DefaultCapacityType } from &amp;#39;@aws-cdk/aws-eks&amp;#39; import { Vpc, SubnetType, InstanceType } from &amp;#39;@aws-cdk/aws-ec2&amp;#39; import { Role, ManagedPolicy, ServicePrincipal, AccountPrincipal, CompositePrincipal } from &amp;#39;@aws-cdk/aws-iam&amp;#39; import * as fs from &amp;#39;fs&amp;#39;; export class TryEksStack extends cdk.</description>
    </item>
    
    <item>
      <title>AWS Organizaionsで複数のアカウントを一元管理する</title>
      <link>https://www.sambaiz.net/article/298/</link>
      <pubDate>Tue, 15 Sep 2020 23:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/298/</guid>
      <description>AWS Organizationsは複数のAWSアカウントを一元管理する機能。 既存のアカウントを一元管理できるだけではなく新規アカウントを簡単に作成することができるので、 セキュリティの向上やコストの分離、クォータなどアカウント全体に及ぶ影響の局所化などのためにサービスや環境ごとにアカウントを分けるハードルが下がる。 引き続きアカウントごとのコストも確認できるが、請求は一括で行われるので支払いの管理が楽になるし、ボリュームディスカウントの使用量が合算されRIも共有できるのでコストの上でも不利にならない。
また、アカウントやそれをグルーピングしたOrganizational Unit(OU)およびOrganizaion全体に対して、 サービスやアカウントを制御するService control policies (SCPs)などのポリシーを設定することもできる。
Organizationを作成したアカウントがマスターアカウントとなる。変更するにはOrganizationを削除する必要があるため管理専用のアカウントを作成するのがおすすめらしい。 通常、新規アカウントを作成するには住所や支払い情報などを入れる必要があるが、 Organizationだとその辺りが省略されアカウント名とルートアカウントのメールアドレスだけで済む。 管理に用いられるIAMロール名を入れる項目もあるが何も入れなければデフォルトのOrganizationAccountAccessRoleになる。
新アカウントのルートユーザーのパスワードは発行されないので必要なら再発行することになるが、 マスターアカウントのIAMユーザーにアカウント作成時に入力したロールのsts:AssumeRole権限を与えれば 新アカウントの方にユーザーを作らなくてもコンソール上部のメニューからスイッチできる。
AWSのAssumeRole - sambaiz-net
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Action&amp;#34;: &amp;#34;sts:AssumeRole&amp;#34;, &amp;#34;Resource&amp;#34;: &amp;#34;arn:aws:iam::&amp;lt;new_account_id&amp;gt;:role/&amp;lt;role_name&amp;gt;&amp;#34; } ] } 基本的にマスターアカウントのリソースの権限は必要ないと思われるが、MFAの権限はないと設定できないので追加する。
初回はアカウントIDとロール名を入力することになる。一度入力すると履歴に残るのでそこから選べる。
参考 20180214 AWS Black Belt Online Seminar AWS Organizations</description>
    </item>
    
    <item>
      <title>VSCodeのDocker開発コンテナでJupyter Notebookを開いてAthenaのクエリを実行し可視化する</title>
      <link>https://www.sambaiz.net/article/294/</link>
      <pubDate>Fri, 04 Sep 2020 19:34:04 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/294/</guid>
      <description>ローカルでJupyter Notebookを動かすために以前はjupyter/datascience-notebookのイメージを立ち上げていた。 Notebookはエディタとしての機能に乏しいため通常のコードを書くのが大変だったのだが、 VSCodeのPython extensionにはJupyter notebookサポートが入っていてそのまま開けて実行できるのを知ったので移行することにした。 今回はVSCodeのDocker開発コンテナからNotebookを開いてAthenaのクエリを実行し可視化する。
VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する - sambaiz-net
環境構築 ~/.awsをマウントする。Dockerfileは上記の記事のと同じ。
{ &amp;#34;name&amp;#34;: &amp;#34;Python&amp;#34;, &amp;#34;build&amp;#34;: { &amp;#34;dockerfile&amp;#34;: &amp;#34;Dockerfile&amp;#34; }, &amp;#34;settings&amp;#34;: { &amp;#34;terminal.integrated.shell.linux&amp;#34;: &amp;#34;/bin/bash&amp;#34;, &amp;#34;python.pythonPath&amp;#34;: &amp;#34;/usr/local/bin/python&amp;#34;, &amp;#34;python.linting.enabled&amp;#34;: true, &amp;#34;python.linting.pylintEnabled&amp;#34;: false, &amp;#34;python.linting.flake8Enabled&amp;#34;: true, &amp;#34;python.linting.flake8Path&amp;#34;: &amp;#34;/usr/local/bin/flake8&amp;#34;, &amp;#34;editor.formatOnSave&amp;#34;: true, &amp;#34;python.formatting.provider&amp;#34;: &amp;#34;yapf&amp;#34;, &amp;#34;python.formatting.yapfPath&amp;#34;: &amp;#34;/usr/local/bin/yapf&amp;#34;, &amp;#34;python.linting.mypyEnabled&amp;#34;: true, &amp;#34;python.linting.mypyPath&amp;#34;: &amp;#34;/usr/local/bin/mypy&amp;#34;, }, &amp;#34;extensions&amp;#34;: [ &amp;#34;ms-python.python&amp;#34; ], &amp;#34;mounts&amp;#34;: [ &amp;#34;source=${localEnv:HOME}/.aws,target=/root/.aws,type=bind,readonly&amp;#34; ] } アプリケーションで使うパッケージはPoetryでインストールすることにしている。
PoetryでPythonの依存パッケージを管理する - sambaiz-net
KernalをvenvのPythonに切り替えるためにipykernelをインストールする必要がある。
[tool.poetry.dependencies] python = &amp;#34;^3.8&amp;#34; PyAthena = &amp;#34;^1.11.1&amp;#34; pandas = &amp;#34;^1.1.1&amp;#34; ipykernel = &amp;#34;^5.</description>
    </item>
    
    <item>
      <title>SageMakerでTensorFlowのモデルを学習させる</title>
      <link>https://www.sambaiz.net/article/293/</link>
      <pubDate>Mon, 10 Aug 2020 13:39:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/293/</guid>
      <description>以前PyTorchのモデルを学習させたが、そのTensorFlow版。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
コード 全体のコードはGitHubにある。
モデル モデルはTitanicのを使う。
TensorFlow2のKeras APIでTitanicのモデルを作る - sambaiz-net
make_csv_dataset()はbatch_sizeが必須になっているが、 これをそのままfilter()しようとすると、ValueError: predicate return type must be convertible to a scalar boolean tensor.になってしまうのでunbatch()している。 SageMakerのServing containerを用いる場合はsave_format=&amp;quot;tf&amp;quot;にしてSavedModel形式で保存する必要がある。
$ cat model.py import tensorflow as tf import logging class Model: def __init__(self, logger: logging.Logger, dropout: float): self.logger = logger self.model = tf.keras.Sequential([ tf.keras.layers.DenseFeatures(self._feature_columns()), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dropout(dropout), tf.keras.layers.Dense(128, activation=tf.nn.relu), tf.keras.layers.Dense(2, activation=tf.nn.sigmoid), ]) self.model.compile(optimizer=&amp;#39;adam&amp;#39;, loss=&amp;#39;binary_crossentropy&amp;#39;, metrics=[&amp;#39;accuracy&amp;#39;]) def _feature_columns(self): return [ tf.feature_column.numeric_column(&amp;#39;age&amp;#39;), tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_identity(&amp;#39;sex&amp;#39;, 2)), tf.feature_column.numeric_column(&amp;#39;fare&amp;#39;) ] def _fill(self, feature: str, value): def __fill(x): if x[feature] == -1.</description>
    </item>
    
    <item>
      <title>SageMakerで学習したPyTorchのモデルをElastic Inferenceを有効にしてデプロイする</title>
      <link>https://www.sambaiz.net/article/290/</link>
      <pubDate>Sun, 26 Jul 2020 02:43:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/290/</guid>
      <description>学習させたモデルをSageMakerのホスティングサービスにデプロイする。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
推論時に呼ばれる関数 推論時には次の関数が呼ばれる。
 model_fn(model_dir): モデルをロードする input_fn(request_body, request_content_type): リクエストボディのデシリアライズ predict_fn(input_data, model): モデルで推論する output_fn(prediction, content_type): Content-Typeに応じたシリアライズ  input_fn() と output_fn() はJSON, CSV, NPYに対応した実装が、predict_fn() はモデルを呼び出す実装がデフォルトとして用意されていて、 model_fn() も後述するElastic Inferenceを使う場合model.ptというファイルをロードするデフォルト実装が使われる。 ただしその場合モデルがtorch.jit.save()でTorchScriptとして保存してある必要がある。
今回は predict_fn() のみ実装した。
$ cat inference.py import torch def predict_fn(input_data, model): device = torch.device(&amp;#39;cuda&amp;#39; if torch.cuda.is_available() else &amp;#39;cpu&amp;#39;) model = model.to(device) input_data = input_data.to(device) model.eval() with torch.jit.optimized_execution(True, {&amp;#34;target_device&amp;#34;: &amp;#34;eia:0&amp;#34;}): output = model(input_data) return output.max(1)[1] デプロイ Training jobがモデルを保存したS3のパスを取ってきてPyTorchModelを作る。
from sagemaker.pytorch.model import PyTorchModel training_job_name = &amp;#39;pytorch-training-2020-07-25-08-41-45-674&amp;#39; training_job = sess.</description>
    </item>
    
    <item>
      <title>SageMakerでPyTorchのモデルを学習させる</title>
      <link>https://www.sambaiz.net/article/287/</link>
      <pubDate>Fri, 24 Jul 2020 22:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/287/</guid>
      <description>AWSの機械学習サービスSageMakerでPyTorchのモデルを学習させる。
コード まず学習させるモデルとそれを呼び出すエントリーポイントになるコードを書く。全体のコードはGitHubにある。 実際の環境と同じSageMakerのコンテナをローカルで動かしてVSCodeのRemote Developmentで接続して開発すると入っていないパッケージは警告が出たりして良い。
VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する - sambaiz-net
モデル 以前作ったMNISTのモデルを使う。
PyTorchでMNISTする - sambaiz-net
$ cat model.py import torch from torch import nn, cuda import torch.nn.functional as F import torch.distributed as dist import torch.optim as optim class Model(nn.Module): def __init__(self, dropout): super(Model, self).__init__() self.conv1 = nn.Conv2d(1, 64, 5) # -&amp;gt; 24x24 self.pool1 = nn.MaxPool2d(2) # -&amp;gt; 12x12 self.conv2 = nn.Conv2d(64, 128, 5) # -&amp;gt; 8x8 self.dropout = nn.Dropout(p=dropout) self.dense = nn.</description>
    </item>
    
    <item>
      <title>VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する</title>
      <link>https://www.sambaiz.net/article/289/</link>
      <pubDate>Sun, 19 Jul 2020 19:34:04 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/289/</guid>
      <description>SageMakerで学習させるモデルを開発するにあたって、Notebooks上ではコードを書きづらいのでVS Codeで書いているのだが、ローカルに依存パッケージをインストールして実行しているため エディタ上では警告が出ていなくても、実際の環境にはパッケージがなかったりすることがある。
そんな場合に便利なのがVS CodeのRemote Development。 これはローカルのVS CodeからリモートのVS Code Serverに接続してその環境で開発することができるエクステンションで、 Dockerコンテナのほか、SSHでリモートマシンやVMに接続したり、WindowsならWSLにも接続して開発環境を揃えることができる。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
設定 .devcontainer/に次のファイルを置く。
PyTorch  Dockerfile  aws/deep-learning-containersの Deep Learning Containers Imagesから選び、ECRからpullするため認証情報を登録しておく。
$ aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin https://763104351884.dkr.ecr.us-east-1.amazonaws.com これをベースに、開発用ツールを入れてVSCodeのDevelopment Container Scriptsを実行する。
$ cat .devcontainer/Dockerfile FROM763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.1-cpu-py36-ubuntu16.04 RUN conda install -y yapf flake8 mypy # VS Code Development Container Scripts # https://github.com/microsoft/vscode-dev-containers/tree/v0.128.0/script-library ARG INSTALL_ZSH=&amp;#34;true&amp;#34; ARG USERNAME=&amp;#34;vscode&amp;#34; ARG USER_UID=&amp;#34;1000&amp;#34; ARG USER_GID=&amp;#34;${USER_UID}&amp;#34; ARG UPGRADE_PACKAGES=&amp;#34;true&amp;#34; ARG COMMON_SCRIPT_SOURCE=&amp;#34;https://raw.</description>
    </item>
    
    <item>
      <title>PoetryでPythonの依存パッケージを管理する</title>
      <link>https://www.sambaiz.net/article/288/</link>
      <pubDate>Sat, 18 Jul 2020 21:23:04 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/288/</guid>
      <description>Poetryは比較的新しいPythonの依存管理ツールで、 pipenvの依存解決に失敗することがある問題を解消したり、ライブラリを開発しやすくしたものらしい。 まだスターはpipenvの半分ほどだがバージョンもv1.0.0に到達したしpipenvよりも速くて安定しているという話もあるので使ってみる。
インストール pipでインストールした。ドキュメントによると依存が衝突する可能性があるとのことだったが、自分の環境では特に問題なかった。
$ pip install --user poetry $ poetry --version Poetry version 1.0.9 使い方 PEP 518で定義されている設定ファイルpyproject.tomlを置く。
$ cat pyproject.toml [tool.poetry] name = &amp;#34;poetry-demo&amp;#34; version = &amp;#34;0.1.0&amp;#34; description = &amp;#34;&amp;#34; authors = [&amp;#34;sambaiz &amp;lt;godgourd@gmail.com&amp;gt;&amp;#34;] [tool.poetry.dependencies] python = &amp;#34;^3.7&amp;#34; [build-system] requires = [&amp;#34;poetry&amp;gt;=0.12&amp;#34;] build-backend = &amp;#34;poetry.masonry.api&amp;#34; poetry addするとvenvがなければ作成してその中にパッケージをインストールしpyproject.tomlにも追加する。
$ poetry add pendulum Creating virtualenv poetry-demo-t1vYebNd-py3.7 in /Users/*****/Library/Caches/pypoetry/virtualenvs ... $ cat pyproject.toml ... [tool.poetry.dependencies] python = &amp;#34;^3.7&amp;#34; pendulum = &amp;#34;^2.1.1&amp;#34; poetry install でpyproject.</description>
    </item>
    
    <item>
      <title>VPCエンドポイント</title>
      <link>https://www.sambaiz.net/article/274/</link>
      <pubDate>Sat, 23 May 2020 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/274/</guid>
      <description>VPCエンドポイントは PrivateLink対応のサービスおよび、S3やDynamoとAWSネットワーク内で接続するためのエンドポイント。 インターネットに出ない分セキュアでゲートウェイへの負荷も抑えられる。 料金は時間あたりとトラフィック量による。
VPCエンドポイントを使うためにアプリケーション側に手を入れる必要はなく、 S3とDynamoがサポートしているGatewayのエンドポイントではルートテーブルによって、 その他多くのサービスがサポートしているInterfaceのエンドポイントでは名前解決の時点で向き先が変わるようになっている。
まずVPCのDNS ResolutionとDNS HostnamesをtrueにしてPrivate DNSで名前解決されるようにしておく必要がある。 エンドポイントを作成する際の設定項目は対象サービスと、VPCとSubnet、SGとサービスによってはPolicy。 サービスと1:1対応しているわけではなく、例えばECSの場合は次の3つのエンドポイントが必要。
com.amazonaws.region.ecs-agent com.amazonaws.region.ecs-telemetry com.amazonaws.region.ecs 作成するとSubnet内にエンドポイントとそれに紐づくENIが立ち、インターネットゲートウェイなしでもAPIが叩けるようになった。</description>
    </item>
    
    <item>
      <title>DAX (DynamoDB Accelerator)の特性と挙動確認</title>
      <link>https://www.sambaiz.net/article/260/</link>
      <pubDate>Wed, 26 Feb 2020 23:21:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/260/</guid>
      <description>DAXとは DAXはDynamoDBの前段に置かれるマネージドなインメモリキャッシュで、 Read速度の向上(数ms-&amp;gt;数百μs)とテーブルのRead Capacityの節約に効果がある。
DynamoDBとSDKのAPIの互換性があるため置き換えるだけで使えるようになっている。 クライアントの実装としてはHTTPではない独自のプロトコルで通信している点が異なる。
クラスタ作成時に指定するのはノード数とインスタンスタイプで、 ノード数はスループットに、インスタンスタイプはスループットとメモリ量(キャッシュヒット率)に影響する。 複数のノードがある場合、一つがWriteするプライマリーノードになり、他はリードレプリカになる。 なのでノード数を増やしてもWriteのスループットは上がらない。プライマリーノードに問題が発生したら自動でフェイルオーバーする。 ノードは最大10個まで増減できるが、インスタンスタイプは変更できない。最大10個というのは足りるのかと思ったが、数百万RPS捌けるようなので十分そうだ。
インスタンスに対して時間課金が発生し、可用性のために3ノード以上にすることが推奨されている。 そのため、リクエストがそれほどなかったり、キャッシュミスばかりだとインスタンス代の方が高くつくこともあるが、 そこそこReadするなら目に見えてコスト削減されるはずだ。ただしどれくらい次の整合性を許容できるかによる。
キャッシュの整合性 DAXはDynamoDBとは異なり、結果整合性のある読み込み(Eventually Consistent Reads)のみをサポートしているので、プライマリノードにキャッシュされ、全てのノードにレプリケーションが完了するまでの間は異なる結果を返す可能性がある。また、DAXからDynamoDBへのリクエストも結果整合性のある読み込みで行われる。
リクエストの結果は、Itemがなかった場合のネガティブキャッシュも含めて、 Item Cache(GetItem,BatchGetItem)とQuery Cache(Query,Scan)にキャッシュされ、 それぞれパラメータグループで設定されたTTLが過ぎるか、LRUアルゴリズムによって破棄される。 TTLのデフォルトは5分。
書き込みリクエストが来るとまずDynamoDBに書き込んで成功したことを確認してからキャッシュしてレスポンスを返す。 この際Item Cacheは更新されるが、Query Cacheは更新されずTTL/LRUによって破棄されるまで同じ値を返し続けてしまう。 もし問題がある場合は、TTLを短くするかDynamoDBを直接見に行くことになるが、そうするとDAXの効果が薄れてしまう。 また、大量のデータを書き込むと、その分レイテンシが増加したり、それらがすべてキャッシュに乗ることで既存のものがLRUで追い出されてキャッシュヒット率が悪くなることがあり、それらを回避するため直接書き込むという選択肢もあるが、Item Cacheが更新されないことを許容する必要がある。
取れるメトリクス CPU使用率や、キャッシュヒット数、各リクエスト数や接続数が取れる。
ノードごとのCPU使用率も取れる。Writeやキャッシュミスによるプライマリーノードの負荷の高まりに注意。 レイテンシが大きくなり接続数が増える悪循環に陥る。
プライマリーノードのCPU使用率が80%程度でテーブルのキャパシティに余裕があってもリクエストがスロットリングされることがあり、一つ上のインスタンスタイプでクラスタを作り直したところ解消した。 この際、いきなり全リクエストが新クラスタに送られることで膨大なキャッシュミスが発生し、 テーブルのキャパシティを超過したりプライマリーノードに負荷が集中しないように、 Route53で割合で解決されるようにしたが、これはうまく流れてくれた。
挙動確認 DAX/DynamoへリクエストするだけだったらLambdaでも良いが、負荷をかけるのでECS上にアプリケーションをデプロイすることにした。 以前作ったBoilerplateベースで、コードはここ。
ECSでアプリケーションを動かすBoilerplateを作った - sambaiz-net
TableとDAXのClusterを作成。
createDynamoTable() { return new dynamodb.Table(this, &amp;#39;DynamoTable&amp;#39;, { tableName: &amp;#34;dax-test-table&amp;#34;, partitionKey: { name: &amp;#39;id&amp;#39;, type: dynamodb.AttributeType.STRING }, readCapacity: 50, writeCapacity: 50, }); } createDaxCluster(subnetIds: string[]) { const subnetGroup = new dax.</description>
    </item>
    
    <item>
      <title>ECSでアプリケーションを動かすBoilerplateを作った</title>
      <link>https://www.sambaiz.net/article/259/</link>
      <pubDate>Mon, 24 Feb 2020 16:17:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/259/</guid>
      <description>https://github.com/sambaiz/ecs-boilerplate
ECS上でアプリケーションを動かすBoilerplateを作った。CDKでデプロイする。以前Digdagを動かしたときのを汎用的にしたもの。
CDKでECS+Fargate上にDigdagを立ててCognito認証を挟む - sambaiz-net
new ECSStack(app, &amp;#39;ECSBoilerplateSampleStack&amp;#39;, { /* // If vpcAttributes is not specified, new VPC is created. vpcAttributes: { vpcId: &amp;#39;&amp;#39;, availabilityZones: [], publicSubnetIds: [], privateSubnetIds: [], }, // DNS record. Even if this is not specified, you can access with ELB domain (***.elb.amazonaws.com) route53: { zoneId: &amp;#39;&amp;#39;, zoneName: &amp;#39;example.com&amp;#39;, recordName: &amp;#39;foo&amp;#39;, }, // Certificate Manager ARN. Required if accessing with HTTPS acmArn: &amp;#39;arn:aws:acm:****&amp;#39; // default values containerPort: 8080, cpu: 256, memoryLimitMiB: 512, minCapacity: 1, maxCapacity: 5, scaleCPUPercent: 80 */ }); CDKがECRへのpushまでやってくれるのでcdk deployすれば動き始め、削除するときもStackを消せばよい。</description>
    </item>
    
    <item>
      <title>LA,ディズニーランドからre:Inventに参加しグランドキャニオンへドライブしてきた</title>
      <link>https://www.sambaiz.net/article/250/</link>
      <pubDate>Sun, 22 Dec 2019 23:45:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/250/</guid>
      <description>一昨年、去年と参加したGoogle I/Oのチケットが今年は当たらなかったので、前から行ってみたかったAWSのre:Inventに参加することにした。 会場はラスベガスで、会期は12/2-6。前後の土日ともう2日つなげて現地時間10日間の旅程にした。 去年、カナダのバンクーバーから西海岸のシアトル、ポートランド、サンフランシスコは巡ったので、今回はまだ訪れていないロサンゼルスからスタートすることにした。会期後はグランドキャニオンまで足を伸ばす。
準備 例年通りExpediaで航空券と宿を取り、加えてラスベガスからグランドキャニオンへ行くためにレンタカーを予約した。 I/O会期中のシリコンバレー近辺とは異なり、ラスベガスのホテルは本当に安くて、OYO(元フーターズ)が一泊2千円で取れた。 re:Inventは65000人規模のイベントなんだが現地のUberドライバー曰く、最大20万人規模のイベントもやったりするらしいのでキャパシティは十分そうだ。 多くの人が申し込むJTBのツアーはホテルのグレードを考えてもかなり割高に感じた。
LAでは北のハリウッド、南のアナハイム、ディズニーランドまで行くことを考えてダウンタウンに宿を取った。 価格だけで選ぶとスキッド・ロウといった危ないエリアの近くになりかねないので注意が必要だ。 ディズニーのチケットも事前に購入した。2パークいけるパークホッパーチケットに、色々な特典を含むMaxPassを付けて$214。 高い日のPeak料金ではあるんだが、家族連れで来たら大変じゃないかと思う。
ラスベガスといえばカジノとショーの街だというし、シルク・ドゥ・ソレイユ Oのチケットを買った。 チケットを印刷するのを忘れていたので現地のFedex Officeで印刷した。ファイルを添付してメールを送るとコードが送られてくるので、それを印刷機に入力するだけで簡単。
アメリカでの運転は初めてで往復できるか不安だったので、グランドキャニオンの南、フラッグスタッフという町の空港で乗り捨てられるAlamoで予約した。 日本でも5年は走っていないペーパードライバーなので、2時間出張教習してもらい、後はタイムズのカーシェアで練習した。 それと免許センターで国際免許を発行した。特に試験とかはなくて手数料だけ払えばもらえる。警察署でも発行してくれるようだが即日発行ではないようだ。
今回のSIMはこれ。
Amazon.co.jp： 【AT&amp;amp;T】ハワイ・アメリカ本土 プリペイドSIM 30日 データ容量8GB 大容量通話付き: 家電・カメラ
回線はAT&amp;amp;Tで、30日、8GB、テザリング可で電話もできる(香港の番号だが国際通話ができる)と、申し分ないスペックに対して安すぎるのが若干不安だったが、 ドキュメント通り現地でSIMを挿してAPNを作成したらすぐにつながり、その後も全く問題なかった。すごい。
LA/ディズニーランド 行きの航空券が関空乗り継ぎだった。国内線スタートの良いところは搭乗時間の締め切りが国際線よりはるかに緩いことで、実際それに救われた。
10時間ほどのフライトでLAXに到着。Lax-itというライドシェア用の乗り場を目指す。 国際線ターミナルからは逆のところにあるので、ひっきりなしに走っているシャトルバスに乗る。 Uberをこの辺りに呼ぶとLax-it内のポート番号が表示される仕様だ。
ホテルにチェックイン後、Cole&amp;rsquo;sという店のDip sandwitchを食べに行く。サンドイッチを肉汁のスープに浸して食べる。 カフェみたいなのをイメージしていったら酒場で緊張した。
地下鉄でハリウッドに移動。運賃はTapというカードにチャージする仕組み。 持ってなかったら$2くらい余分に払うと自販機から出てくる。距離に関係なく同一運賃。 このカードでMetroのバスにも乗れるが、バスではチャージできないそうなので乗る場合は少し余分に入れておく。 ただ、結局バスは乗らなかった。ディズニーランド行きのバスもあっておそらく最安なんだが、 Localなのでとても時間がかかるし、サウス・ロサンゼルスというこれまた治安悪いエリアを突っ切るのが怖かったからだ。
ハリウッドではWalk of Fameの有名人の名前プレートを見ながら散歩していた。途中ハリウッドサインが見えたが想像していたより遠い。
せっかくなのでビバリーヒルズのロデオドライブにも行ってきた。表参道みたいな感じであまり用がなかった。
次の日はディズニーランドに行く前にGrand Central MarketのEggslutで朝食を取った。8時開店で8時半には着いたんだが、既に20人くらいの行列ができている人気店だ。 よく分からなかったのでslutというのを注文した。食べるまで気づかなかったんだが星野珈琲のモーニングのあれだ。美味しい。
Uberでディズニーランドに向かう。そういえばアプリ入れてないなと思ってPlayストアで調べたがひっかからない。もしやと思って調べてみると
は？？しょうがないのでアカウントの地域をアメリカに切り替えて入れた。1年間変更できず、日本向けのアプリをダウンロードできなくなった。納得いかない。 ともあれ、これでアプリからファストパスが取れるようになった。ファストパスは30分に1回取れるが、既に持ってるものを消費しなくてはいけない。 人気のアトラクションはかなり先の時間になってしまうか取れなくなってしまうので、 東京と同様のアトラクションが結構あるディズニーランドパークよりカリフォルニアアドベンチャーの方を先に回った方が良さそう。 ただ、パークの方でもスターウォーズのエリアはかなり作り込んであって新鮮だし、写真を撮ってくれる人もいるので暗くなる前に行くべきかもしれない。 撮ってもらった写真はMaxPassならアプリからダウンロードできる。一人って言ったら、&amp;ldquo;Oh, Solo&amp;quot;って言われたのにうまく反応できなくて悔しい。
カリフォルニアアドベンチャーの方はカーズのアトラクションが一番人気で、ファストパスを取るならそれが最有力だと思う。 スピード感がありながらフワッとする感じはなくて楽しかった。 撮影ポイントがあって出たところのモニターにアプリに入力するコードが表示されているんだが、 切り替わるのが早すぎるので、カメラで撮るなりして一旦コードだけ控えておいたほうが良い。 あとミッキーの顔が書かれた観覧車にも乗った。Swingするのに乗ったら、それこそバイキングかってぐらい揺らしてきて 泣きそうになった。実際泣き出す子もいると思う。
パークの城が東京で見るのと違うなと思ったら、こっちの城はシンデレラ城じゃなくて眠れる森の美女の城らしい。
行きのUberは$40くらいだったのに対して帰りは$60くらいかかった。需要と供給だ。
ラスベガス/re:Invent re:Invent前日にLAXからLASへ。 もう降りたところからスロットマシンが置いてあってさすがカジノの街だ。ここのUber乗り場はパーキングにある。</description>
    </item>
    
    <item>
      <title>ECS(EC2)のCloudFormation最小構成</title>
      <link>https://www.sambaiz.net/article/247/</link>
      <pubDate>Fri, 15 Nov 2019 20:12:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/247/</guid>
      <description>EC2でECSのServiceを動かすCFnテンプレートを書く。以前Fargateで動かしたものを一部再利用する。
ECS FargateでSidecarのFluentdでログをS3に送る構成をCloudFormationで構築する - sambaiz-net
EC2で動かす場合、自分でリソースが不足しないようにインスタンスのスケールを気遣うことになるが、VPC外での実行やprivilegedをtrueにするなどEC2でしかできないことがある。あと同リソースで比較すると安い。
まずはEC2インスタンス以外のリソースを書く。LaunchType以外はFargateのときとほぼ同じ。 LBなしでバッチのようなものを動かすことを想定した最小構成。
ECSCluster: Type: AWS::ECS::Cluster Properties: ClusterName: &amp;#39;test-cluster&amp;#39; LogGroup: Type: AWS::Logs::LogGroup Properties: LogGroupName: &amp;#39;test-task-log-group&amp;#39; RetentionInDays: 1 TaskDefinition: Type: AWS::ECS::TaskDefinition Properties: RequiresCompatibilities: - EC2 Cpu: &amp;#39;256&amp;#39; Memory: &amp;#39;512&amp;#39; ContainerDefinitions: - Name: &amp;#39;app&amp;#39; Image: &amp;#39;busybox&amp;#39; EntryPoint: - &amp;#39;sh&amp;#39; - &amp;#39;-c&amp;#39; Command: - &amp;#39;while true; do echo &amp;#34;{\&amp;#34;foo\&amp;#34;:1000,\&amp;#34;time\&amp;#34;:\&amp;#34;2019-05-09T20:00:00+09:00\&amp;#34;}&amp;#34;; sleep 10; done&amp;#39; Essential: &amp;#39;true&amp;#39; LogConfiguration: LogDriver: &amp;#39;awslogs&amp;#39; Options: awslogs-group: !Ref LogGroup awslogs-region: &amp;#39;ap-northeast-1&amp;#39; awslogs-stream-prefix: &amp;#39;app&amp;#39; Environment: - Name: &amp;#39;TZ&amp;#39; Value: &amp;#39;Asia/Tokyo&amp;#39; Volumes: - Name: &amp;#39;varlog&amp;#39; ECSService: Type: AWS::ECS::Service Properties: Cluster: !</description>
    </item>
    
    <item>
      <title>Lambda環境でできない処理をECSで実行する</title>
      <link>https://www.sambaiz.net/article/245/</link>
      <pubDate>Mon, 28 Oct 2019 22:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/245/</guid>
      <description>以前cdkbotというツールを出した。これはGitHubのPRからCDKのデプロイなどを実行できるツールでlambda上で動いていた。
PR上でCDKのレビューやデプロイを行うツールcdkbotを作った - sambaiz-net
npmやgitといった外部コマンドを実行するため、layerにバイナリを詰めて上げていた。
Lambda上でnpm installできるLayerを作った - sambaiz-net
CDKにはローカルのDockerfileをbuildしてECRに上げてくれる ecs.ContainerImage.fromAsset()という関数があって、これに対応させるためlayerにdockerを追加してみたのだがrootが取れず動かない。 rootなしで動くudockerでdind(docker in docker)のイメージを動かしたりもしてみたがbuildはできなかった。
この他にもCloudFrontなどリソースによっては作成に時間がかかり、Lambdaのタイムアウト上限に到達する問題もあったので、lambda+API Gatewayでwebhookのリクエストだけ受け取り、ECSのTaskを立ち上げて処理を行わせることにした。
templateにECSまわりのものを追加したところ、Serverless Application Repository非対応ということで上げられなくなってしまった。
AWS SAMでLambdaの関数をデプロイしServerless Application Repositoryに公開する - sambaiz-net
Taskへのパラメータの受け渡し runTaskのcontainerOverridesでコマンドの引数としてlambdaに来たリクエストをそのまま渡そうとしたところ、8192文字文字の上限に当たってしまったので SQSで受け渡すことにした。
sess := session.New() sqsSvc := sqs.New(sess) if _, err := sqsSvc.SendMessage(&amp;amp;sqs.SendMessageInput{ MessageBody: aws.String(string(payload)), QueueUrl: aws.String(os.Getenv(&amp;#34;OPERATION_QUEUE_URL&amp;#34;)), MessageGroupId: aws.String(&amp;#34;group&amp;#34;), }); err != nil { fmt.Println(err.Error()) return response{ StatusCode: http.StatusInternalServerError, }, err } 同時実行数を制限する アプリケーションの特性上、同時実行されないようにしたい。 そこで普段はTask0のServiceを作成し、実行するときは要求Taskを1にして、Queueが空になるまで実行させ、最後に0に戻すようにした。Taskがない場合は立ち上がりにやや時間がかかるが、元々Lambdaで動いていたこともあって常に料金が発生する常駐リソースをなるべく使いたくなかった。
sess := session.New() sqsSvc := sqs.New(sess) for { res, err := sqsSvc.</description>
    </item>
    
    <item>
      <title>PR上でCDKのレビューやデプロイを行うツールcdkbotを作った</title>
      <link>https://www.sambaiz.net/article/235/</link>
      <pubDate>Thu, 29 Aug 2019 22:53:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/235/</guid>
      <description>sambaiz/cdkbot
PRのコメントで/diffや/deployと打つとcdk diffやcdk deployが走る。 diffを見てレビューし、良ければ/deployでデプロイし完了するとmergeされる。
以前CircleCIでmerge時にdeployされる仕組みを作った。
CDK/CircleCI/GitHubでAWSリソース管理リポジトリを作る - sambaiz-net
ただ、この仕組みだと CFnの実行時エラーのためにデプロイできない状態のものがmasterブランチにmergeされてしまい、その修正のために何回も試行錯誤のPRを出すことになったり、 Stack間の依存がある場合リソースを削除するとcdk deployによって依存解決された順序だと失敗してしまうという問題があった。 cdkbotでは必要ならデプロイするStackを選べて、完了してからmergeすることでこれらの問題を解決した。 また、AWS外のCIにとても強い権限を与えていたがそれも必要なくなった。
単純にブランチの状態でデプロイしてしまうと古い状態に巻き戻ってしまう可能性があるので、内部でbaseブランチをmergeしていたり、 ラベルによってそのPRがデプロイ可能かどうかを制御していたりする。 最低限デプロイできるようになってから、この辺りの仕組みを整えるまでに存外に時間がかかった。
Serverless Application Repositoryに公開してあるので簡単にインストールできる。
AWS SAMでLambdaの関数をデプロイしServerless Application Repositoryに公開する - sambaiz-net
 追記 (2019-10-26): ap-northeast-1に対応していないのと、ECSのリソースを作成できないため、Serverless Application Repositoryに公開するのはやめた。makeでインストールできる。 Lambda環境でできない処理をECSで実行する - sambaiz-net
 外部コマンド gitやnpmといった外部コマンドを実行する必要があるが、標準では入っていないのでLambda Layerで入れている。
Lambda上でnpm installできるLayerを作った - sambaiz-net
Go moduleのキャッシュ Dockerコンテナ内でテストを実行しているが、毎回go moduleの解決が走ることで時間はかかるし、テザリングの容量に大打撃を受けたので、 ローカルのキャッシュをコピーするようにした。
test: docker build -t cdkbot-npmbin ./npm-lambda-layer docker build -t cdkbot-test -f ./test/Dockerfile . docker rm -f cdkbot-test || true docker run -itd --name cdkbot-test cdkbot-test /bin/sh docker cp .</description>
    </item>
    
    <item>
      <title>CDKでECS&#43;Fargate上にDigdagを立ててCognito認証を挟む</title>
      <link>https://www.sambaiz.net/article/234/</link>
      <pubDate>Wed, 31 Jul 2019 03:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/234/</guid>
      <description>AWSでワークフローエンジンDigdagを立てるにあたりスケールを見越してECS+Fargateで動かす。 全体のコードはGitHubにある。
FargateでECSを使う - sambaiz-net
リソースはCDKで作る。最近GAになったので高レベルのクラスを積極的に使っている。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
$ npm run cdk -- --version 1.2.0 (build 6b763b7) VPC FargateなのでVPCが必要。 テンプレートを書くとSubnetやRouteTable、NATGatewayなど記述量が多くなるところだが、CDKだとこれだけで済む。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
const vpc = new ec2.Vpc(this, &amp;#39;VPC&amp;#39;, { cidr: props.vpcCidr, natGateways: 1, maxAzs: 2, subnetConfiguration: [ { name: &amp;#39;digdag-public&amp;#39;, subnetType: ec2.SubnetType.PUBLIC, }, { name: &amp;#39;digdag-private&amp;#39;, subnetType: ec2.SubnetType.PRIVATE, }, { name: &amp;#39;digdag-db&amp;#39;, subnetType: ec2.SubnetType.ISOLATED, } ] }) DB DigdagはPostgreSQLを使う。
const db = new rds.DatabaseCluster(this, &amp;#39;DBCluster&amp;#39;, { engine: rds.</description>
    </item>
    
    <item>
      <title>Lambda上でnpm installできるLayerを作った</title>
      <link>https://www.sambaiz.net/article/233/</link>
      <pubDate>Tue, 23 Jul 2019 23:44:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/233/</guid>
      <description>Lambda上でnpm installするためにnpmとnode, npmrc入りのLambda Layerを作った。 GitHubにある。
Lambda Layerでバイナリやライブラリを切り出す - sambaiz-net
まずは/usr/bin/npmをそのまま入れて実行してみた。
FROMlambci/lambda-base:build WORKDIR/opt RUN curl -sL https://rpm.nodesource.com/setup_12.x | bash - &amp;amp;&amp;amp; \  yum install -y nodejs &amp;amp;&amp;amp; \  mkdir bin &amp;amp;&amp;amp; \  cp /usr/bin/node bin/node &amp;amp;&amp;amp; \  cp /usr/bin/npm bin/ &amp;amp;&amp;amp; \  zip -yr /tmp/npm-layer.zip ./* $ docker build -t npmbin . $ docker run npmbin cat /tmp/npm-layer.zip &amp;gt; npm-layer.zip &amp;amp;&amp;amp; unzip npm-layer.zip -d layer 相対パスでの参照に失敗したようだが対象のパスが見当たらない。</description>
    </item>
    
    <item>
      <title>Lambda Layerでバイナリやライブラリを切り出す</title>
      <link>https://www.sambaiz.net/article/232/</link>
      <pubDate>Mon, 22 Jul 2019 21:09:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/232/</guid>
      <description>Lambdaで実行したい外部コマンドがある場合、通常バイナリをパッケージに含めることになりデプロイに時間がかかってしまう。
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;os/exec&amp;#34; &amp;#34;github.com/aws/aws-lambda-go/events&amp;#34; &amp;#34;github.com/aws/aws-lambda-go/lambda&amp;#34; ) func handler(request events.APIGatewayProxyRequest) (events.APIGatewayProxyResponse, error) { cmd := exec.Command(&amp;#34;git&amp;#34;, &amp;#34;clone&amp;#34;, &amp;#34;https://github.com/sambaiz/foobar.git&amp;#34;, &amp;#34;/tmp/repo&amp;#34;) output, err := cmd.CombinedOutput() if err != nil { return events.APIGatewayProxyResponse{ Body: fmt.Sprintf(&amp;#34;%s %s&amp;#34;, string(output), err.Error()), StatusCode: 500, }, nil } return events.APIGatewayProxyResponse{ Body: string(output), StatusCode: 200, }, nil } func main() { lambda.Start(handler) } exec: &amp;#34;git&amp;#34;: executable file not found in $PATH Lambda Layerを使うと ライブラリやバイナリを切り出すことができ、複数Functionで共有することもできる。 ディレクトリをzipにしてLayerに指定すると中身が/optに展開され、/opt/binにはPATHが、/opt/libにはLD_LIBRARY_PATHが通るほか、 言語ごとのパッケージ置き場がある。</description>
    </item>
    
    <item>
      <title>AWS SAMとGoでPRのコメントに対して返事を返すGitHub Appを作る</title>
      <link>https://www.sambaiz.net/article/231/</link>
      <pubDate>Fri, 19 Jul 2019 21:21:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/231/</guid>
      <description>GitHub Appはリポジトリにインストールできるアプリケーションで、 Access TokenやOAuth Appと異なり ユーザーとは独立した権限を与えて実行することができる。
今回はPRの特定のコメントに反応して返事を返すAppを作る。
AWS SAMでデプロイする。全体のコードはGitHubにある。
AWS SAMでLambdaの関数をデプロイしServerless Application Repositoryに公開する - sambaiz-net
GitHub Appの作成 Settings &amp;gt; Developer settings から作成できる。いろいろ項目はあるが、NameとHomepage URL、Webhook URLを入れればひとまず作成はできる。 Webhook URLはあとで決まるので適当な値を入れておく。必須にはなっていないがリクエストを検証するため適当なWebhook secretも入れる。 PermissionsはPull requestsではなくIssueのRead &amp;amp; Writeが必要で、 さらにそうすると表示されるようになるSubscribe to eventsのIssue commentにチェックを入れる。
作成すると秘密鍵がダウンロードできるのでSecretsmanagerに上げておき、LambdaのRoleにもこれを取得できるRoleを付ける。
AWS Systems Manager (SSM)のParameter Storeに認証情報を置き参照する - sambaiz-net
$ aws secretsmanager create-secret --name GitHubCdkAppSecretKey --secret-string $(cat private-key.pem) Policies: - PolicyName: read-cdk-github-app-secret-key PolicyDocument: Version: &amp;#34;2012-10-17&amp;#34; Statement: - Effect: Allow Action: secretsmanager:GetSecretValue Resource: &amp;lt;secret-key arn&amp;gt; Webhooksのリクエスト内容 設定でチェックを入れたeventが起きると次のようなリクエストが送られてくる。 Headerの X-GitHub-Event によってbodyの中身が決まり、 X-Hub-Signature と、bodyと設定したWebhook secretから生成したHMACのMAC値を比較することで リクエストを検証することができる。</description>
    </item>
    
    <item>
      <title>Cognito UserPoolのPreSignUp時に呼ばれるLambdaで登録ユーザーを制限する</title>
      <link>https://www.sambaiz.net/article/228/</link>
      <pubDate>Sun, 07 Jul 2019 17:10:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/228/</guid>
      <description>サードパーティのIdPからCognitoにSignUpできるようにする場合、特定のドメインのメールアドレスといったような制限をかけたいことがある。 PreSignUp時のLambdaでこれを弾いてやることでUserPoolに入らないようにすることができる。
Lambda CognitoEventUserPoolsPreSignupを受け取って返す。
package main import ( &amp;#34;errors&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;github.com/aws/aws-lambda-go/events&amp;#34; &amp;#34;github.com/aws/aws-lambda-go/lambda&amp;#34; ) func handler(event events.CognitoEventUserPoolsPreSignup) (events.CognitoEventUserPoolsPreSignup, error) { fmt.Printf(&amp;#34;PreSignup of user: %s\n&amp;#34;, event.UserName) if event.Request.UserAttributes[&amp;#34;email&amp;#34;] != &amp;#34;godgourd@gmail.com&amp;#34; { return event, errors.New(&amp;#34;Forbidden&amp;#34;) } return event, nil } func main() { lambda.Start(handler) } リソース UserPoolのLambdaConfigでトリガーを設定できる。 CognitoからLambdaを呼べるPermissionが必要。
UserPool: Type: AWS::Cognito::UserPool Properties: ... LambdaConfig: PreSignUp: !GetAtt PresignupLambdaFunction.Arn UserPoolLambdaInvokePermission: Type: AWS::Lambda::Permission Properties: Action: lambda:invokeFunction Principal: cognito-idp.amazonaws.com FunctionName: !GetAtt PresignupLambdaFunction.Arn SourceArn: arn:aws:cognito-idp:&amp;lt;region&amp;gt;:&amp;lt;account_id&amp;gt;:userpool/* なおALBのActionでCognito認証を入れると登録失敗時に500エラーになってしまう。これを回避するには自前でやるしかないのかもしれない。
LambdaとALBでCognito認証をかけて失敗したらログイン画面に飛ばす - sambaiz-net</description>
    </item>
    
    <item>
      <title>LambdaとALBでCognito認証をかけて失敗したらログイン画面に飛ばす</title>
      <link>https://www.sambaiz.net/article/227/</link>
      <pubDate>Wed, 03 Jul 2019 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/227/</guid>
      <description>ALBのTargetとしてLambdaが選択できるようになり、 若干の時間課金が発生する代わりに柔軟にルーティングできるAPI Gatewayのように使えるようになった。 ActionとしてCognito認証を入れて認証に失敗したらログイン画面を表示させる。
API GatewayでCognitoの認証をかけて必要ならログイン画面に飛ばす処理をGoで書く - sambaiz-net
ACMで証明書を発行する HTTPSでListenするため証明書が必要。 AWS Certificate Manager (ACM)でAWSで使える証明書を無料で発行でき参照できる。 外部で取ったドメインでもよい。 検証方法はDNSとメールとで選ぶことができて、DNSで行う場合Route53ならワンクリックで検証用のCNAMEレコードを作成できる。 検証までやや時間がかかるのでちゃんと通ってるかnslookupで確認しといた方がよい。
Application Load Balancer (ALB) 次の要素から構成されるL7のロードバランサー。
 Listener: 指定したプロトコルとポートでリクエストを受ける。 ListenerRule: パスやHeaderなどの値を条件にどのTargetGroupにルーティングするかのルール。 TargetGroup: ルーティングする1つ以上のTarget。Instance, IP, Lambdaが選べる。  Ruleの作成 Serverless FrameworkではALBのenentを付けるだけでLambdaに向くRuleが作成されるが、そこにはCognitoを追加できなさそうなので使っていない。OnUnauthenticatedRequestで認証失敗時の挙動を選択できる。UserPoolとSecretありのClientはあらかじめ作っておく。コールバックURLにはhttps://&amp;lt;domain&amp;gt;/oauth2/idpresponseを追加する。
API Gatewayだとタイムアウトの上限が30秒なのに対してALBはLambdaの上限まで待てる。
$ cat serverless.yml service: alb-cognito-auth-example frameworkVersion: &amp;#34;&amp;gt;=1.28.0 &amp;lt;2.0.0&amp;#34; provider: name: aws runtime: go1.x region: ap-northeast-1 package: exclude: - ./** include: - ./bin/** functions: privateapi: handler: bin/privateapi timeout: 30 resources: Resources: ALBTargetGroup: DependsOn: InvokeLambdaPermissionForALB Type: AWS::ElasticLoadBalancingV2::TargetGroup Properties: Name: &amp;#34;private-lambda-target-group&amp;#34; TargetType: &amp;#34;lambda&amp;#34; Targets: - Id: !</description>
    </item>
    
    <item>
      <title>API GatewayでCognitoの認証をかけて必要ならログイン画面に飛ばす処理をGoで書く</title>
      <link>https://www.sambaiz.net/article/226/</link>
      <pubDate>Wed, 03 Jul 2019 20:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/226/</guid>
      <description>ブラウザから直接API GatewayのエンドポイントにアクセスしたときにCognitoのTokenで認証し、失敗したらログイン画面を表示させる。 API GatewayでCognitoの認証をかける場合、AuthorizerでUserPoolを指定するのが最も簡単なパターンだが、 これだとHeaderにTokenを付けてアクセスする必要があり認証に失敗するとUnauthorizedが返る。
Cognito UserPoolとAPI Gatewayで認証付きAPIを立てる - sambaiz-net
なおAPI GatewayではなくALBをLambdaの前段に挟めば今回やることが簡単に実現できる。
LambdaとALBでCognito認証をかけて失敗したらログイン画面に飛ばす - sambaiz-net
準備 UserPoolとClientを作成する。 CloudFormationで作成する場合SchemaのMutableのデフォルトがfalseで、変えると作り直されてしまうことに注意。
Resources: Userpool: Type: AWS::Cognito::UserPool Properties: AdminCreateUserConfig: AllowAdminCreateUserOnly: false Schema: - Mutable: true Name: email Required: true - Mutable: true Name: name Required: true UsernameAttributes: - email UserPoolName: testpool UserpoolClient: Type: AWS::Cognito::UserPoolClient Properties: UserPoolId: Ref: Userpool ClientName: testclient GenerateSecret: true その後、GoogleのOAuth Client IDを作成し、フェデレーションの設定を行ってGoogleアカウントでもログインできるようにした。 これはID Poolのフェデレーティッドアイデンティティとは異なる機能。 UserPoolのドメインや、外部IdPとのAttributes Mapping、Clientの設定はCloudFormationではできないので手で行う。
 追記 (2020-12-06): 今はCloudFormationで行えるようになっている。
CDKでCognito UserPoolとClientを作成しトリガーやFederationを設定する - sambaiz-net</description>
    </item>
    
    <item>
      <title>AWS DeepRacerを始める</title>
      <link>https://www.sambaiz.net/article/224/</link>
      <pubDate>Mon, 10 Jun 2019 23:06:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/224/</guid>
      <description>AWS DeepRacerは自走する1/18スケールのレーシングカーで、 SageMakerやRoboMakerなどを使って強化学習し、実機を走らせたりバーチャルのDeepRacerリーグで競うことができる。 カメラの画像の処理や、強化学習のアルゴリズムの実装の必要はなく、報酬関数だけで動いてくれるので敷居が低い。
強化学習とDQN(Deep Q-network) - sambaiz-net
設定項目 Action space 取りうるアクションである速度とステアリングの組み合わせのリスト。次の項目から生成される。
 Maximum steering angle (1 - 30) Steering angle granularity (3, 5, 7) Maximum speed (0.8 - 8) Speed granularity (1, 2, 3) Loss type (Mean square error, Huber) Number of experience episodes between each policy-updating iteration (5 - 100)  Reward function 強化学習の報酬関数。次の入力パラメータを用いて実装する。
{ &amp;#34;all_wheels_on_track&amp;#34;: Boolean, # flag to indicate if the vehicle is on the track &amp;#34;x&amp;#34;: float, # vehicle&amp;#39;s x-coordinate in meters &amp;#34;y&amp;#34;: float, # vehicle&amp;#39;s y-coordinate in meters &amp;#34;distance_from_center&amp;#34;: float, # distance in meters from the track center  &amp;#34;is_left_of_center&amp;#34;: Boolean, # Flag to indicate if the vehicle is on the left side to the track center or not.</description>
    </item>
    
    <item>
      <title>CDK/CircleCI/GitHubでAWSリソース管理リポジトリを作る</title>
      <link>https://www.sambaiz.net/article/223/</link>
      <pubDate>Mon, 20 May 2019 09:23:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/223/</guid>
      <description>AWS CDKでリソースを記述し、PullRequestに対して自動でcdk diffで変更があるものを表示して、mergeしたときにcdk deployする。 全体のコードはGitHubにある。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
 追記 (2019-08-29): このフローで起こったいくつかの問題を解決するため新しいツールを作った。 PR上でCDKのレビューやデプロイを行うツールcdkbotを作った - sambaiz-net
 CI Userの作成 まずcdkコマンドを実行するためのCI Userを作成する。これはCDK管理外のスタックで、AWSコンソール上から手動で上げる。
AWSのAssumeRole - sambaiz-net
AssumeRoleしかできないCIUserからCIAssumeRoleをassumeすることにした。
AWSTemplateFormatVersion: &amp;#39;2010-09-09&amp;#39; Resources: CIAssumeRole: Type: &amp;#39;AWS::IAM::Role&amp;#39; Properties: RoleName: &amp;#39;CIAssumeRole&amp;#39; ManagedPolicyArns: - &amp;#39;arn:aws:iam::aws:policy/AdministratorAccess&amp;#39; AssumeRolePolicyDocument: Version: &amp;#39;2012-10-17&amp;#39; Statement: - Effect: &amp;#39;Allow&amp;#39; Principal: AWS: - !Ref AWS::AccountId Action: - &amp;#39;sts:AssumeRole&amp;#39; CIGroup: Type: &amp;#39;AWS::IAM::Group&amp;#39; Properties: GroupName: &amp;#39;CI&amp;#39; CIPolicies: Type: &amp;#39;AWS::IAM::Policy&amp;#39; Properties: PolicyName: &amp;#39;CI&amp;#39; PolicyDocument: Statement: - Effect: Allow Action: &amp;#39;sts:AssumeRole&amp;#39; Resource: !</description>
    </item>
    
    <item>
      <title>AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする</title>
      <link>https://www.sambaiz.net/article/222/</link>
      <pubDate>Sun, 19 May 2019 01:43:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/222/</guid>
      <description>AWS CDK(Cloud Development Kit)はTypeScriptやJavaなどのコードから CloudFormationのテンプレートを生成して差分を確認しデプロイできる公式のツール。まだdeveloper preview。
$ npm i -g aws-cdk $ cdk --version 0.33.0 (build 50d71bf) $ mkdir cdk-vpc $ cd cdk-vpc $ cdk init app --language=typescript CloudFormationのリソースと対応するCfnFooや、それを内部で作成する高レベル(L2)のResource ClassFooが実装されている。 ただし、現状CfnFooに対応するResource Classが存在しないものや、複数のリソースを内部で作成するResource Classが存在する。 例えば、ec2.VpcはCfnVPCだけではなく、Public/Private Subnet、NATGatewayまでまとめて一般的な構成で作る。Resource Classはまだ変更が多い。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
型があり補完が効くので通常のテンプレートと比べて書きやすいし、ループしたりすることもできる。
import * as cdk from &amp;#39;@aws-cdk/cdk&amp;#39; import * as ec2 from &amp;#39;@aws-cdk/aws-ec2&amp;#39; interface Export { vpc: ec2.Vpc } export class VPCStack extends cdk.Stack { protected deployEnv: string export: Export constructor(scope: cdk.</description>
    </item>
    
    <item>
      <title>ECS FargateでSidecarのFluentdでログをS3に送る構成をCloudFormationで構築する</title>
      <link>https://www.sambaiz.net/article/221/</link>
      <pubDate>Thu, 09 May 2019 23:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/221/</guid>
      <description>DAEMONを動かすことはできず、 fluentd logdriverもサポートされていないFargateで、 サイドカーとしてFluentdのコンテナを動かしてアプリケーションのログをS3に送る。 全体のコードはGitHubにある。
FargateでECSを使う - sambaiz-net
Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る - sambaiz-net
Fluentd 必要なプラグインと設定ファイルを入れたイメージを作る。
FROMfluent/fluentd:v1.4-1 USERroot COPY ./fluent.conf /fluentd/etc/ # install plugin RUN apk add --update-cache --virtual .build-deps sudo build-base ruby-dev \  &amp;amp;&amp;amp; gem install fluent-plugin-s3 -v 1.0.0 --no-document \  &amp;amp;&amp;amp; gem install uuidtools \  &amp;amp;&amp;amp; gem sources --clear-all \  &amp;amp;&amp;amp; apk del .build-deps \  &amp;amp;&amp;amp; rm -rf /var/cache/apk/* \  /home/fluent/.gem/ruby/*/cache/*.gem # set timezone (Alpine) RUN apk --update-cache add tzdata &amp;amp;&amp;amp; \  cp /usr/share/zoneinfo/Asia/Tokyo /etc/localtime &amp;amp;&amp;amp; \  apk del tzdata &amp;amp;&amp;amp; \  rm -rf /var/cache/apk/* fluent.</description>
    </item>
    
    <item>
      <title>DatadogのAWS integrationとAlertの設定をTerraformで行う</title>
      <link>https://www.sambaiz.net/article/219/</link>
      <pubDate>Sat, 04 May 2019 19:25:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/219/</guid>
      <description>DatadogのAWS integrationとAlertの設定をTerraformで行い、バージョン管理やレビューできるようにする。 全体のコードはGitHubに置いてある。
AWS Integration まずdatadog_integration_awsでAWS integrationの設定を作成してExternalIDを取得し、Policy/Roleを作成する。必要な権限はドキュメントを参照。
resource &amp;#34;datadog_integration_aws&amp;#34; &amp;#34;test&amp;#34; { account_id = &amp;#34;${var.aws_account_id}&amp;#34; role_name = &amp;#34;${var.aws_integration_role_name}&amp;#34; filter_tags = [&amp;#34;datadog:1&amp;#34;] } data &amp;#34;aws_iam_policy_document&amp;#34; &amp;#34;datadog_aws_integration_assume_role&amp;#34; { statement { actions = [&amp;#34;sts:AssumeRole&amp;#34;] principals { type = &amp;#34;AWS&amp;#34; identifiers = [&amp;#34;arn:aws:iam::464622532012:root&amp;#34;] } condition { test = &amp;#34;StringEquals&amp;#34; variable = &amp;#34;sts:ExternalId&amp;#34; values = [ &amp;#34;${datadog_integration_aws.test.external_id}&amp;#34;, ] } } } Datadog providerにはないSlackなどその他のintegrationは手動で設定する必要がある。 また、ログを集める場合Serverless Application Repositoryから公式のDatadog-Log-Forwarderを入れて AWS IntegrationのところにLambdaのARNを入れるのも手動。
 追記 (2020-12-07): 今はDatadog Forwaderとなり、Serverless Application Repositoryからではなく、直接CloudFormationのスタックを上げるようになっているのでaws_cloudformation_stackでデプロイできる。AWS IntegrationのRoleに必要なPolicyを与えてCollect LogsタブのLambda Cloudwatch Logsにチェックを入れると、Optionally limit resource collectionを設定しているならそのtagを持つ、全てのFunctionのStreamに自動でForwarderへのSubscription Filterが作成され転送が始まる。チェックを外すと削除されるが、この際もtagを見ているようで、条件を変更するとSubscription Filterが一部残ることがあった。ログだけではなくlambdaからのカスタムメトリクスの中継や、estimated_costなどの拡張メトリクスの送信も行う。</description>
    </item>
    
    <item>
      <title>SageMaker NotebookでGitリポジトリにSSHでpush/pullできるようにする</title>
      <link>https://www.sambaiz.net/article/211/</link>
      <pubDate>Mon, 04 Mar 2019 22:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/211/</guid>
      <description>Sagemaker NotebookはAWSの機械学習のワークフローを提供するSageMakerの一部である マネージドなJupyter Notebooksで、可視化などはもちろん、ここから複数インタンスでの学習ジョブを実行したりすることができる。
Git統合 によってノートブック作成時にGitHubなどのリポジトリを指定すると前もって持ってきてくれるようになったが、 今のところHTTPSエンドポイントにしか対応していないようで、ユーザー名・パスワードまたはトークンといった個人に紐づく認証情報が必要になる。 今回はこの機能を使わずに、ライフサイクル設定でssh鍵を置き、これでpush/pullできるようにする。
パスフレーズなしの鍵を作って公開鍵を対象リポジトリのDeployKeyに登録してread/writeできるようにする。
$ mkdir sagemaker-sshkey $ cd sagemaker-sshkey $ ssh-keygen -t rsa -b 4096 -f id_rsa -N &amp;#34;&amp;#34; $ pbcopy &amp;lt; id_rsa.pub 秘密鍵はSSMのParameter Storeに登録する。
AWS Systems Manager (SSM)のParameter Storeに認証情報を置き参照する - sambaiz-net
$ aws ssm put-parameter --name &amp;#34;sagemaker-sshkey&amp;#34; --value &amp;#34;`cat id_rsa`&amp;#34; --type String --overwrite $ aws ssm get-parameters --names &amp;#34;sagemaker-sshkey&amp;#34; ライフサイクル設定でノートブック開始時に次のスクリプトが実行されるようにする。 このスクリプトはrootで実行される。Parameter Storeが読める権限をNotebookのIAMに付けておく。
#!/bin/bash  set -e su - ec2-user &amp;lt;&amp;lt;EOF cd /home/ec2-user aws ssm get-parameters --names &amp;#34;sagemaker-sshkey&amp;#34; | jq -r &amp;#34;.</description>
    </item>
    
    <item>
      <title>AWS SAMでLambdaの関数をデプロイしServerless Application Repositoryに公開する</title>
      <link>https://www.sambaiz.net/article/207/</link>
      <pubDate>Sun, 10 Feb 2019 16:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/207/</guid>
      <description>AWS SAM (Serverless Application Model)はAWS公式の サーバーレスアプリケーションのビルドツール。 CloudFormationのテンプレートを設定ファイルに書くことでLambda関数と共にイベントトリガーや他のリソースも含めてデプロイでき、 その点でServerless Frameworkと立ち位置が近いが、向こうがLambda以外のサーバーレス環境にも対応していたり、 プラグインによって機能拡張できるようになっている一方、こちらは比較的薄いツールになっている。 ただ、Serverless Application Repositoryで公開するにはSAMの形式にする必要があり、 Serverless FrameworkにもSAMのテンプレートを出力するプラグインがある。
Serverless FrameworkでLambdaをデプロイする - sambaiz-net
SAM CLIのインストール $ brew tap aws/tap $ brew install aws-sam-cli $ sam --version SAM CLI, version 0.11.0 init initすると次の構成のディレクトリが作られる。
$ sam init --runtime go1.x -n test-sam $ cd test-sam/ $ ls Makefile	README.md	hello-world	template.yaml template.yamlの中に関数の設定やCloudFormationのテンプレートを書く。
$ cat template.yaml AWSTemplateFormatVersion: &amp;#39;2010-09-09&amp;#39; Transform: AWS::Serverless-2016-10-31 Description: &amp;gt;test-sam Sample SAM Template for test-sam # More info about Globals: https://github.</description>
    </item>
    
    <item>
      <title>CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う</title>
      <link>https://www.sambaiz.net/article/206/</link>
      <pubDate>Sun, 03 Feb 2019 17:31:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/206/</guid>
      <description>Aurora ServerlessはオートスケールするAuroraで、 使ったAurora Capacity Unit (ACU)によって料金が発生するため、 使用頻度が少なかったり変動するアプリケーションにおいて安くRDBを使うことができる。 インスタンスを立てると最低でも月3000円くらいかかるが、Serverlessだとほとんどストレージ分から運用することができて趣味でも使いやすい。 ただしLambdaと同様に常に同等のリソースを使っている状態だとインスタンスと比べて割高になる。
今回はLambdaで使う。 Serverlessと名前には付いているが用途としてはLambdaに限らず、 むしろコンテナの数が容易に増え得るLambdaは同時接続数が問題になるRDBと一般に相性が良くない。 現在Betaの、コネクションを張らずにHTTPSでクエリを投げられるData APIはこの問題を解消すると思われるが、トランザクションが張れなかったり、レスポンスサイズに制限があるようだ。今回はコンソール上から初期クエリを流すためにData APIを有効にしている。
他の選択肢として、DynamoDBは現状最有力で最近トランザクションもサポートされたがSQLのように複雑なクエリは投げられない。 Athenaはクエリは投げられるがそこそこ時間がかかるし、INSERT/UPDATEはできずクエリごとに料金が発生する。
Serverless Frameworkを使ってリソースを作成しデプロイする。リポジトリはここ。
Serverless FrameworkでLambdaをデプロイする - sambaiz-net
VPCの作成 Aurora Serverlessの制限の一つとしてVPC内からしか接続しかできないというものがある。ということでVPCから作成していく。以前Terraformで作ったのと同じリソースをCloudFormationで作る。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
LambdaをVPC内で動かすとコンテナ起動時にENIも作成するため立ち上がりの際時間がかかる。必要なら定期的に呼び出して削除されないようにする。 また、今回はテストのため/24でVPCを切っているが、小さいとENIのIPアドレスが枯渇する可能性がある。
 VPC  TestVPC: Type: AWS::EC2::VPC Properties: CidrBlock: 172.32.0.0/24 Tags: - Key: Name Value: test-vpc  Subnet  Aurora Serverlessのために少なくとも2つのサブネットが必要。
TestPublicSubnet: Type: AWS::EC2::Subnet Properties: VpcId: !Ref TestVPC CidrBlock: 172.32.0.0/25 AvailabilityZone: us-east-1d Tags: - Key: Name Value: test-public-subnet1 TestPrivateSubnet1: Type: AWS::EC2::Subnet Properties: VpcId: !</description>
    </item>
    
    <item>
      <title>AWS Systems Manager (SSM)のParameter Storeに認証情報を置き参照する</title>
      <link>https://www.sambaiz.net/article/204/</link>
      <pubDate>Mon, 07 Jan 2019 23:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/204/</guid>
      <description>DBのパスワードやAPIトークンといった認証情報をバージョン管理するコードや設定ファイル上に書くとOSS化など公開範囲を広げるときにやや困るし漏れるリスクが高まるのでなるべく避けたい。 そこでSSMのParameter Storeに値を置き、実行時やデプロイ時に参照する。
SSMのParameter StoreとSecrets Manager Systems Manager (SSM)はAWSのリソースを可視化したり操作を自動化したりするサービス群で、 設定を持つParameter Storeはその一つ。値は暗号化して持つこともできる。 料金はかからない。
SSMのParameter Storeと似たような別のAWSのサービスに Secrets Managerというのがあって、RDSなどと連携してLambdaによって定期的に新しい値を生成しローテーションさせることができる。 ただし料金がシークレットの件数($0.4/月)とAPIコール($0.05/10000回)でかかる。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
今はParamter StoreとSecrets Managerが統合されていて、Parameter StoreのAPIでどちらも参照できるようだ。 今回はローテーションしないので単純に料金がかからないParameter Storeの方に書き込むことにする。 ただし、Parameter Storeは現状一度に大量のリクエストが飛ぶような使い方をするとRate exceededになってしまう問題がある。
実行時の値取得 実行時に値を取得するのはこんな感じ。
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;github.com/aws/aws-sdk-go/aws&amp;#34; &amp;#34;github.com/aws/aws-sdk-go/aws/awserr&amp;#34; &amp;#34;github.com/aws/aws-sdk-go/aws/session&amp;#34; &amp;#34;github.com/aws/aws-sdk-go/service/ssm&amp;#34; ) type Parameter struct { ssm *ssm.SSM } func newParameter(sess *session.Session) *Parameter { return &amp;amp;Parameter{ ssm: ssm.New(sess), } } func (s *Parameter) Get(name string, decrypt bool) (string, error) { param, err := s.</description>
    </item>
    
    <item>
      <title>AWS GlueでCSVを加工しParquetに変換してパーティションを切りAthenaで参照する</title>
      <link>https://www.sambaiz.net/article/203/</link>
      <pubDate>Tue, 01 Jan 2019 17:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/203/</guid>
      <description>AWS GlueはマネージドなETL(Extract/Transform/Load)サービスで、Sparkを使ってS3などにあるデータを読み込み加工して変換出力したり、AthenaやRedshift Spectrumで参照できるデータカタログを提供する。 今回はS3のCSVを読み込んで加工し、列指向フォーマットParquetに変換しパーティションを切って出力、その後クローラを回してデータカタログにテーブルを作成してAthenaで参照できることを確認する。
料金はジョブがDPU(4vCPU/16GBメモリ)時間あたり$0.44(最低2DPU/10分)かかる。 また、クローラも同様にDPUで課金される。
なお、AthenaのCTASでもParquetを出力することができる。 出力先にファイルがないようにする必要があったり重いクエリは失敗することがあるが手軽で良い。
import * as athena from &amp;#39;athena-client&amp;#39; const clientConfig: athena.AthenaClientConfig = { bucketUri: &amp;#39;s3://*****/*****&amp;#39; skipFetchResult: true, }; const awsConfig: athena.AwsConfig = { region: &amp;#39;us-east-1&amp;#39;, }; const client = athena.createClient(clientConfig, awsConfig); (async () =&amp;gt; { await client.execute(` CREATE TABLE ***** WITH ( format = &amp;#39;PARQUET&amp;#39;, external_location = &amp;#39;s3://*****&amp;#39; ) AS ( SELECT ~~ ) })(); 開発用エンドポイント ジョブの立ち上がりにやや時間がかかるため開発用エンドポイントを立ち上げておくとDPUが確保されて効率よく開発できる。 立ち上げている間のDPUの料金がかかる。つまりずっとジョブを実行し続けているようなもので結構高くつくので終わったら閉じるようにしたい。
ローカルやEC2から自分で開発用エンドポイントにsshしてREPLで実行したりNotebookを立てることもできるが、 コンソールから立ち上げたNotebookは最初からつながっていて鍵の登録も必要なくて楽。
$ ssh -i private-key-file-path 9007:169.</description>
    </item>
    
    <item>
      <title>FargateでECSを使う</title>
      <link>https://www.sambaiz.net/article/196/</link>
      <pubDate>Fri, 09 Nov 2018 00:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/196/</guid>
      <description>ECSはAWSのコンテナオーケストレーションサービス。 クラスタはEC2上に立てることもできるが、その場合Auto Scalingグループの設定やスケールイン時のdrainなどを考慮する必要がある。 Fargateで起動するとサーバーレスで実行でき、バックエンドの管理が必要がなくなる。 料金は割り当てたvCPUとメモリによって、最低1分の1秒単位で課金される。 Lambdaと同じくリソースあたりでいうとオンデマンドのEC2と比較して高くなっているが、柔軟にリソースが指定できる分いくらか差は縮まるかもしれない。
特にバッチ処理のように常にリソースが必要ないTaskは都度インスタンスを立ち上げるのも面倒なので良いと思う。 Lambdaと比較すると、実行環境を自由に作れるのと実行時間に制限がないというところが良いが、 Taskを作るトリガーは現状cronだけなのでそれ以外のイベントで実行したい場合はLambdaと組み合わせる必要がある。
AWSにはKubernetesクラスタを立てられるEKSもあるが、こちらはまだFargateに対応していない。 もしかしたら今月末のre:Inventで何か発表されるかもしれない。
 (追記: 2021-05-30): re:Invent 2019でEKSのFargate対応が発表された。
LA,ディズニーランドからre:Inventに参加しグランドキャニオンへドライブしてきた - sambaiz-net
 Clusterの作成 まずはClusterを作成する。
$ aws ecs create-cluster --cluster-name test Taskの登録 イメージやポートマッピング、ヘルスチェックや割り当てるリソースといったContainer definition を含むTask definitionを書く。 Fargateの場合networkModeはawsvpc固定になる。
{ &amp;#34;family&amp;#34;: &amp;#34;test-task&amp;#34;, &amp;#34;networkMode&amp;#34;: &amp;#34;awsvpc&amp;#34;, &amp;#34;containerDefinitions&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;nginx&amp;#34;, &amp;#34;image&amp;#34;: &amp;#34;nginx:1.15&amp;#34;, &amp;#34;portMappings&amp;#34;: [ { &amp;#34;containerPort&amp;#34;: 80, &amp;#34;hostPort&amp;#34;: 80, &amp;#34;protocol&amp;#34;: &amp;#34;tcp&amp;#34; } ], &amp;#34;essential&amp;#34;: true } ], &amp;#34;requiresCompatibilities&amp;#34;: [ &amp;#34;FARGATE&amp;#34; ], &amp;#34;cpu&amp;#34;: &amp;#34;256&amp;#34;, &amp;#34;memory&amp;#34;: &amp;#34;512&amp;#34; } Taskを登録する。</description>
    </item>
    
    <item>
      <title>Macでの開発環境構築メモ</title>
      <link>https://www.sambaiz.net/article/163/</link>
      <pubDate>Sat, 14 Apr 2018 14:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/163/</guid>
      <description>新しいMBPを買ったので開発環境の構築でやったことを残しておく
設定  アクセシビリティから3本指スクロールを有効にする ホットコーナーの左上にLaunchPad、右上にデスクトップを割り当てている 画面をなるべく広く使うためにDockは左に置いて自動的に隠す  bash_profile パッケージマネージャ以外で持ってきたバイナリは${HOME}/binに置くことにする。
touch ~/.bash_profile mkdir ${HOME}/bin echo &amp;#34;export PATH=\$PATH:${HOME}/bin&amp;#34; &amp;gt;&amp;gt; ~/.bash_profile HomeBrew &amp;amp; Cask /usr/bin/ruby -e &amp;#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&amp;#34; brew tap caskroom/cask 一般的なアプリケーション/コマンドのインストール XcodeとUnityとLINEは手動で入れる。
brew cask install google-chrome kap visual-studio-code slack kindle brew install jq gibo mysql wget Git git config --global user.name sambaiz git config --global user.email godgourd@gmail.com Docker &amp;amp; K8s brew cask install docker virtualbox minikube brew install docker kubernetes-helm fish bash前提で書かれたスクリプトも多いので、デフォルトシェルにはしない。</description>
    </item>
    
    <item>
      <title>Cognito UserPoolとAPI Gatewayで認証付きAPIを立てる</title>
      <link>https://www.sambaiz.net/article/157/</link>
      <pubDate>Sun, 25 Feb 2018 23:46:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/157/</guid>
      <description>UserPoolを作成。デフォルト設定はこんな感じ。 必須項目や、確認メールの文面などを自由にカスタマイズでき、 登録時などのタイミングでLambdaを発火させることもできる。
作成したUserPoolにアプリクライアントを追加する。 ブラウザで使うのでクライアントシークレットはなし。
クライアント側 amazon-cognito-identity-jsを使う。
依存するjsを持ってくる。
$ wget https://raw.githubusercontent.com/aws/amazon-cognito-identity-js/master/dist/amazon-cognito-identity.min.js $ wget https://raw.githubusercontent.com/aws/amazon-cognito-identity-js/master/dist/aws-cognito-sdk.min.js Sign UpからAPIを呼ぶところまでのボタンを並べた。 SignInするとOIDC標準のトークンがそのページのドメインのLocal Storageに書かれる。
OpenID ConnectのIDトークンの内容と検証 - sambaiz-net
CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.idToken CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.accessToken CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.refreshToken CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.&amp;lt;name&amp;gt;.clockDrift CognitoIdentityServiceProvider.&amp;lt;clientId&amp;gt;.LastAuthUser APIを呼ぶときはidTokenをAuthorization Headerに乗せる。
&amp;lt;button id=&amp;#34;signUp&amp;#34;&amp;gt;Sign Up&amp;lt;/button&amp;gt; &amp;lt;p&amp;gt;&amp;lt;label&amp;gt;Code:&amp;lt;input type=&amp;#34;text&amp;#34; id=&amp;#34;code&amp;#34;&amp;gt;&amp;lt;/label&amp;gt;&amp;lt;/p&amp;gt; &amp;lt;button id=&amp;#34;confirm&amp;#34;&amp;gt;Confirm&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;#34;signIn&amp;#34;&amp;gt;Sign In&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;#34;whoAmI&amp;#34;&amp;gt;Who am I?&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;#34;requestAPI&amp;#34;&amp;gt;Request API with token&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;#34;signOut&amp;#34;&amp;gt;Sign Out&amp;lt;/button&amp;gt; &amp;lt;script src=&amp;#34;aws-cognito-sdk.min.js&amp;#34;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script src=&amp;#34;amazon-cognito-identity.min.js&amp;#34;&amp;gt;&amp;lt;/script&amp;gt; &amp;lt;script&amp;gt; const USER_NAME = &amp;#34;*****&amp;#34;; const USER_PASSWORD = &amp;#34;*****&amp;#34;; const USER_EMAIL = &amp;#34;*****&amp;#34;; class CognitoUserPoolAuth { constructor(UserPoolId, clientId, apiEndpoint) { const poolData = { UserPoolId : UserPoolId, ClientId : clientId }; this.</description>
    </item>
    
    <item>
      <title>Serverless FrameworkでLambdaをデプロイする</title>
      <link>https://www.sambaiz.net/article/155/</link>
      <pubDate>Sun, 11 Feb 2018 23:20:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/155/</guid>
      <description>Serverless FrameworkでLambda Functionをデプロイする。 Apexが基本的にFunction自体のデプロイしかしないのに対して、こちらはeventの設定なども諸々やってくれて強い。
ApexでLambdaをデプロイする - sambaiz-net
$ npm install -g serverless $ serverless version 1.26.0 ApexではFunctionごとにディレクトリが作られたが、ServerlessではServiceごとに作られ、 一つのService内で複数のFunctionを定義できる。handlerは同じでも異なっていてもよい。
Apexの形式の場合、共通の処理をWebpackなどで各Functionに持って来たり、 同じような処理の複数のFunctionを立てる際はコピーする必要があったが、 こちらは必要最小限の変更でそれらを行うことができる。
templateからServiceをcreateする。
$ serverless create --template aws-nodejs --path testservice $ ls testservice/ handler.js	serverless.yml 設定ファイルserverless.yml にはLambdaの基本的なもののほかに、VPCやevent、IAM Roleなども書けて、これらはdeploy時に作られるCloudFormationのstackによって管理される。必要なら生のCloudFormationの設定も書ける。
ApexでもTerraformによって管理することができるが、書くのはこちらの方がはるかに楽。
ApexでデプロイしたLambdaのトリガーをTerraformで管理する - sambaiz-net
$ cat sesrverless.yml service: testservice provider: name: aws profile: foobar region: ap-northeast-1 runtime: nodejs6.10 memorySize: 512 timeout: 10 functions: hello: handler: handler.hello events: - http: path: hello/world method: get cors: true deployすると{service}-{stage}-{function}のFunctionが作られる。 今回の場合はtestservice-prd-test。stageをymlでも指定しなかった場合はデフォルト値のdevになる。</description>
    </item>
    
    <item>
      <title>DatadogのLambda Integrationで気象データを送ってアラートを飛ばす</title>
      <link>https://www.sambaiz.net/article/152/</link>
      <pubDate>Mon, 05 Feb 2018 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/152/</guid>
      <description>最近朝が寒くて布団から出るのがつらい。雨や雪が降っていたら尚更のこと、それならそれで現状を把握する必要がある。 そこで、無料から使える気象API OpenWeatherMapのデータをdatadogに送って、特に寒い日や雨雪が降るような朝にはアラートを飛ばすことにした。
インスタンスが立っていたらDataDog AgentのDogStatsD経由で送ることができ、 そうでなければ通常はAPIを呼ぶことになるんだが、Lambdaでは、AWS Integrationを設定すると有効になるLambda Integrationによって MONITORING|unix_epoch_timestamp|value|metric_type|my.metric.name|#tag1:value,tag2のフォーマットでconsole.logするだけでメトリクスが送られるようになっている。
 追記 (2020-12-07): 今はDatadog Forwaderを通して送ることができる。
 const axios = require(&amp;#39;axios&amp;#39;); const CITY = &amp;#39;Shibuya&amp;#39;; const API_KEY = &amp;#39;*****&amp;#39;; const WEATHER_API = `http://api.openweathermap.org/data/2.5/weather?q=${CITY}&amp;amp;units=metric&amp;amp;appid=${API_KEY}`; const METRIC_COUNTER = &amp;#39;counter&amp;#39;; const METRIC_GAUGE = &amp;#39;gauge&amp;#39;; const monitor = (metricName, metricType, value, tags) =&amp;gt; { const unixEpochTimestamp = Math.floor(new Date().getTime()); console.log(`MONITORING|${unixEpochTimestamp}|${value}|${metricType}|${metricName}|#${tags.join(&amp;#39;,&amp;#39;)}`); }; exports.handler = async (event, context, callback) =&amp;gt; { const data = (await axios.get(WEATHER_API)).data const namePrefix = &amp;#39;livinginfo.</description>
    </item>
    
    <item>
      <title>Athenaのmigrationやpartitionするathena-adminを作った</title>
      <link>https://www.sambaiz.net/article/145/</link>
      <pubDate>Sun, 24 Dec 2017 23:31:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/145/</guid>
      <description>https://github.com/sambaiz/athena-admin
AthenaはS3をデータソースとするマネージドなデータ分析基盤。Prestoベースで標準SQLを実行できる。
料金はスキャンしたデータ量にかかり、$5/TB。1MB切り上げで、10MB以下のクエリは10MBになる。 データ量に対してかなり安く使えるものの、フルスキャンしてしまうとBigQueryと同様にお金が溶けてしまうので、大抵はパーティションを切ることになるのだが都度locationを指定してADD PARTITIONを実行するのは大変。さらにスキーマを変更するのにもALTER TABLE ADD COLUMNSなどはないのでテーブルを作り直すことになるが、当然パーティションも全部作り直すことになる。
ではどうしようもないかというとMSCK REPAIR TABLEというのがあって、 これはS3のObjectのdt=YYYY-MM-DDのようなkey=valueのprefixを認識してパーティションを作るもの。作り直す際もこれ1クエリで終わる。それなら最初からそういう風に置けばよいのではというところだが、勝手にYYYY/MM/DD/HHのprefixを付けてしまうFirehoseのようなのもある。
今回作ったathena-adminは以下のような定義ファイルから、 パーティションのkey=valueのprefixが付くように置き換えたり、変更があったらmigrationする。 このファイルを書き換えるだけで基本的にどうにかなるし、バージョン管理すればテーブル定義の変更を追うことができる。
{ &amp;#34;general&amp;#34;: { &amp;#34;athenaRegion&amp;#34;: &amp;#34;ap-northeast-1&amp;#34;, &amp;#34;databaseName&amp;#34;: &amp;#34;aaaa&amp;#34;, &amp;#34;saveDefinitionLocation&amp;#34;: &amp;#34;s3://saveDefinitionBucket/aaaa.json&amp;#34; }, &amp;#34;tables&amp;#34;: { &amp;#34;sample_data&amp;#34;: { &amp;#34;columns&amp;#34;: { &amp;#34;user_id&amp;#34;: &amp;#34;int&amp;#34;, &amp;#34;value&amp;#34;: { &amp;#34;score&amp;#34;: &amp;#34;int&amp;#34;, &amp;#34;category&amp;#34;: &amp;#34;string&amp;#34; } /* &amp;#34;struct&amp;lt;score:int,category:string&amp;gt;&amp;#34; のように書くこともできる */ }, &amp;#34;srcLocation&amp;#34;: &amp;#34;s3://src/location/&amp;#34;, &amp;#34;partition&amp;#34;: { &amp;#34;prePartitionLocation&amp;#34;: &amp;#34;s3://pre/partition/&amp;#34;, /* optional */ &amp;#34;regexp&amp;#34;: &amp;#34;(\\d{4})/(\\d{2})/(\\d{2})/&amp;#34;, /* optional */ &amp;#34;keys&amp;#34;: [ { &amp;#34;name&amp;#34;: &amp;#34;dt&amp;#34;, &amp;#34;type&amp;#34;: &amp;#34;string&amp;#34;, &amp;#34;format&amp;#34;: &amp;#34;{1}-{2}-{3}&amp;#34;, /* optional */ } ] } } } } 使い方はこんな感じ。使い方によってはmigrate()だけ呼ぶこともあると思う。 replaceObjects()にはmatchedHandlerというのを渡すこともできて、 UTCからJSTに変換するといったこともできる。</description>
    </item>
    
    <item>
      <title>ApexでデプロイしたLambdaのトリガーをTerraformで管理する</title>
      <link>https://www.sambaiz.net/article/144/</link>
      <pubDate>Sun, 12 Nov 2017 22:23:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/144/</guid>
      <description>Apexでfunctionをデプロイするとトリガーが登録されないのであとで追加することになる。 これを手作業で行うこともできるのだが、せっかくなのでアプリケーションと一緒に管理したい。 そんなときのためにterraformコマンドをラップしたapex infraが用意されている。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
functionsと同列にinfrastructureディレクトリを作成してtfファイルを置く。 その下に環境ごとのディレクトリを作成することもできて、その場合は--envで指定した環境のものが使われる。
- functions - infrastructure main.tf variables.tf - modules - cloudwatch_schedule main.tf variables.tf project.json functionをデプロイするとそのARNが変数で取れるようになる。
$ apex list --tfvars apex_function_hello=&amp;#34;arn:aws:lambda:ap-northeast-1:*****:function:usetf_hello&amp;#34; 今回設定するトリガーはCloudwatch Eventのスケジューリング。作成するリソースは以下の通り。
 aws_cloudwatch_event_ruleでイベントルール(今回はschedule)を作成 aws_cloudwatch_event_targetでルールにターゲット(今回はLambda)を設定 aws_lambda_permissionでルールに対象Lambdaをinvokeする権限を付ける  $ cat infrastructure/modules/cloudwatch_schefule/variables.tf variable &amp;#34;lambda_function_name&amp;#34; {} variable &amp;#34;lambda_function_arn&amp;#34; {} variable &amp;#34;schedule_expression&amp;#34; { description = &amp;#34;cloudwatch schedule expression e.g. \&amp;#34;cron(0/5 * * * ? *)\&amp;#34;&amp;#34; } $ cat infrastructure/modules/cloudwatch_schefule/main.tf resource &amp;#34;aws_cloudwatch_event_rule&amp;#34; &amp;#34;lambda&amp;#34; { name = &amp;#34;lambda_rule_${var.lambda_function_name}&amp;#34; description = &amp;#34;invoke lambda ${var.</description>
    </item>
    
    <item>
      <title>Redashでデータを可視化する</title>
      <link>https://www.sambaiz.net/article/141/</link>
      <pubDate>Mon, 23 Oct 2017 23:59:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/141/</guid>
      <description>RedashはOSSのデータ可視化ツール。 BIツールのようにパラメータを変えながら指標を探っていくというよりは、分かっている指標を見るのに使うイメージ。 比較的機能が少ない分処理がわかりやすく、 クエリが自動生成されないため時間がかかるものを実行する前にある程度気づけるのが良いと思う。
docker-composeで立ち上げることもできるが、 AWSの各リージョンにAMIが用意されているのでそれで立ち上げる。
sshで入って以下のようなのを必要に応じて設定する。 メールを送る場合はSESでメールアドレスをVerifyしてやるのが簡単。 GSuiteを使っている場合、OAuthのClientID、Secretを発行しドメインを登録するとそれで認証できる。
$ ssh ubuntu@***** $ sudo vi /opt/redash/.env export REDASH_MAIL_SERVER=&amp;#34;email-smtp.us-east-1.amazonaws.com&amp;#34; export REDASH_MAIL_USE_TLS=&amp;#34;true&amp;#34; export REDASH_MAIL_USERNAME=&amp;#34;*****&amp;#34; export REDASH_MAIL_PASSWORD=&amp;#34;*****&amp;#34; export REDASH_MAIL_DEFAULT_SENDER=&amp;#34;*****&amp;#34; # Email address to send from export REDASH_GOOGLE_CLIENT_ID=&amp;#34;&amp;#34; export REDASH_GOOGLE_CLIENT_SECRET=&amp;#34;&amp;#34; $ cd /opt/redash/current $ sudo -u redash bin/run ./manage.py org set_google_apps_domains {{domains}} $ sudo supervisorctl restart all HTTPS対応するのに/etc/nginx/sites-available/redashを編集する。crtとkeyの場所は変える。
upstream rd_servers { server 127.0.0.1:5000; } server { server_tokens off; listen 80 default; access_log /var/log/nginx/rd.access.log; gzip on; gzip_types *; gzip_proxied any; location /ping { proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_pass http://rd_servers; } location / { return 301 https://$host$request_uri; } } server { listen 443 ssl; # Make sure to set paths to your certificate .</description>
    </item>
    
    <item>
      <title>ApexでLambdaをデプロイする</title>
      <link>https://www.sambaiz.net/article/140/</link>
      <pubDate>Sun, 22 Oct 2017 16:06:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/140/</guid>
      <description>ApexでLambdaをデプロイする。 とても簡単に使えるし、変なこともしないので良い感じ。
 Serverless Frameworkだとeventの設定までカバーできてより便利。
Serverless FrameworkでLambdaをデプロイする - sambaiz-net
 インストール。ダウンロードして実行できるようにしている。
$ curl https://raw.githubusercontent.com/apex/apex/master/install.sh | sh  IAMFullAccess AWSLambdaFullAccess  を付けたIAMのプロファイルを登録しておく。
$ aws configure --profile apex $ aws configure list --profile apex Name Value Type Location ---- ----- ---- -------- profile apex manual --profile access_key ****************OVGQ shared-credentials-file secret_key ****************oi5t shared-credentials-file region ap-northeast-1 config-file ~/.aws/config apex initしてnameとdescriptionを入れるとIAMが登録され、 ディレクトリ構造が作られる。
$ apex init --profile apex Project name: try-apex Project description: test [+] creating IAM try-apex_lambda_function role [+] creating IAM try-apex_lambda_logs policy [+] attaching policy to lambda_function role.</description>
    </item>
    
    <item>
      <title>Lambda上でPuppeteer/Headless Chromeを動かすStarter Kitを作った</title>
      <link>https://www.sambaiz.net/article/132/</link>
      <pubDate>Sun, 10 Sep 2017 23:45:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/132/</guid>
      <description>PuppeteerでHeadless Chromeを動かすコードを Lambda上で動かすStarter Kitを作った。
puppeteer-lambda-starter-kit
Chromeの準備 Puppeteerのインストール時に落としてくるChromeをLambda上で動かそうとしても Lambdaにないshared libraryに依存しているため失敗する。
error while loading shared libraries: libpangocairo-1.0.so.0: cannot open shared object file: No such file or directory Lambda上でHeadless Chromeを動かす例がないか調べたらserverless-chromeというのがあって、 Headless用の設定でChromeをビルドしていた。 ほかにはchromelessというのもあるが これはserverless-chromeに 依存している。 最小構成でPuppeteerを使いたかったので、今回はこれらを使わず一から作ることにした。
serverless-chromeにもビルドしたものが置いてあるが、少しバージョンが古いようだったので最新版でビルドした。 基本的には書いてある 通りやればうまくいく。他のプロセスとのshared memoryとして/dev/shmを使っているのを、/tmpに置き換える ようにしないと、実行時のpage.goto()でFailed Provisional Load: ***, error_code: -12になる。
ビルドしたheadless_shellには問題になった依存は含まれていないようだ。
$ ldd headless_shell linux-vdso.so.1 =&amp;gt; (0x00007ffcb6fed000) libpthread.so.0 =&amp;gt; /lib64/libpthread.so.0 (0x00007f5f17dbe000) libdl.so.2 =&amp;gt; /lib64/libdl.so.2 (0x00007f5f17bba000) librt.so.1 =&amp;gt; /lib64/librt.so.1 (0x00007f5f179b1000) libnss3.so =&amp;gt; /usr/lib64/libnss3.so (0x00007f5f17692000) libnssutil3.so =&amp;gt; /usr/lib64/libnssutil3.so (0x00007f5f17466000) libsmime3.so =&amp;gt; /usr/lib64/libsmime3.</description>
    </item>
    
    <item>
      <title>TerraformでVPCを管理するmoduleを作る</title>
      <link>https://www.sambaiz.net/article/121/</link>
      <pubDate>Sun, 23 Jul 2017 02:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/121/</guid>
      <description>Terraform
$ brew install terraform $ terraform -v Terraform v0.9.11 Terraformの設定要素 provider IaaS(e.g. AWS)、PaaS(e.g. Heroku)、SaaS(e.g. CloudFlare)など。
AWS Providerはこんな感じ。 ここに直接access_keyやsecret_keyを書くこともできるが、誤って公開されてしまわないように環境変数か variableで渡す。
provider &amp;#34;aws&amp;#34; {# access_key = &amp;#34;${var.access_key}&amp;#34; # secret_key = &amp;#34;${var.secret_key}&amp;#34;  region = &amp;#34;us-east-1&amp;#34; } $ export AWS_ACCESS_KEY_ID=&amp;#34;anaccesskey&amp;#34; $ export AWS_SECRET_ACCESS_KEY=&amp;#34;asecretkey&amp;#34; varibale CLIでオーバーライドできるパラメーター。typeにはstringのほかにmapやlistを渡すことができ、 何も渡さないとdefault値のものが、それもなければstringになる。
variable &amp;#34;key&amp;#34; { type = &amp;#34;string&amp;#34; default = &amp;#34;value&amp;#34; description = &amp;#34;description&amp;#34; } 値を渡す方法はTF_VAR_をprefixとする環境変数、-var、-var-fileがある。 また、moduleのinputとして渡されることもある。
$ export TF_VAR_somelist=&amp;#39;[&amp;#34;ami-abc123&amp;#34;, &amp;#34;ami-bcd234&amp;#34;]&amp;#39; $ terraform apply -var foo=bar -var foo=baz $ terraform apply -var-file=foo.</description>
    </item>
    
    <item>
      <title>fluentdのAggregatorをELBで負荷分散し、Blue/Green Deploymentする</title>
      <link>https://www.sambaiz.net/article/113/</link>
      <pubDate>Sun, 25 Jun 2017 00:35:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/113/</guid>
      <description>デプロイやスループットの調整を簡単にするため、BeanstalkでAggregatorを立ち上げた。
負荷分散 TCPの24224(設定による)が通るようにEC2,ELBのSGとリスナーの設定をする必要があって、 ELBのSGのアウトバウンドの設定が見落とされがち。ELBのクロスゾーン分散は有効になっている。
まず、ELBに3台、それぞれ別のAZ(1b, 1c, 1d)に配置されている状態でログを送り始めるとそれぞれ均等にログが届いた。 その状態から4台(1b * 2, 1c, 1d)にすると、2つのインスタンス(1b, 1c)のみに均等にログが届くようになった。 4台になると(1b, 1c)と(1b, 1d)に分けられてELBのノードがそれらの組に紐づいたということだと思う。 各ノードにはDNSラウンドロビンするようになっている。実際restartすると今度は別の組の方に送られた。
では、なぜ一度送り始めると同じ方にしか飛ばないかというと、forwardプラグインのexpire_dns_cacheがデフォルトでnilになっていて、 heartbeatが届いている間は無期限にDNSキャッシュするようになっているため。これに0(キャッシュしない)か秒数を指定すると、 その間隔で他の組のインスタンスにもログが届くようになった。 expire_dns_cacheしなくても複数のインスタンスからラウンドロビンされるため全体でいえば分散される。
heartbeat ELB配下のEC2を全て落としてもheartbeatに失敗しないため、standyに移行せずELBのバックエンド接続エラーになってログがロストしてしまうようだ。 ログにも出ず、以下のようにactive-standbyの設定をしてもstandbyに移行しない。 全てのインスタンスが同時に落ちるというのは滅多に起きないだろうが、少なくとも検知できるようにはしておく。
&amp;lt;server&amp;gt; name td1 host autoscale-td1.us-east-1.elasticbeanstalk.com port 24224 &amp;lt;/server&amp;gt; &amp;lt;server&amp;gt; name td2 host autoscale-td2.us-east-1.elasticbeanstalk.com port 24224 standby &amp;lt;/server&amp;gt; Blue/Green Deployment Blue-Green Deploymentというのは、2つの系を用意し、activeでない方にデプロイし、 スワップして反映させるもの。ダウンタイムなしで問題が起きた際にもすぐに切り戻すことができる。 スワップして向き先を変えるにはexpire_dns_cacheを設定する必要がある。
Auto Scaling 増えるのはいいとして減るときに、 送り先で一時的に障害が起きていたりするとバッファをflushできずにログがロストする可能性がある。 それでいうとログの送り元でも同じことが起こりうるんだが、通常Aggregatorにしか送らないので比較的問題になりにくい。
これを避けたい場合、Auto Scalingグループの設定で スケールインから保護を有効にして これから立ち上がるインスタンスはスケールインしなくすることができる。 それまでに立ち上がっていたインスタンスには適用されないので注意。
スケールインしないということは最大の台数で止まってしまうので、 ピークを過ぎたらスワップしてバッファが全て掃けたことを確認してからTerminateすることになる。 これを日常的にやるのは面倒なので、実際は予期しない流量の増加に備えて一応設定しておき、 普段はしきい値にひっかからないような最低台数で待ち構えておくことにするかな。
あとはヘルスチェックによって潰される可能性はなくもないが、それはもうやむなし・・・。
参考 AWS ELBの社内向け構成ガイドを公開してみる 負荷分散編 – Cross-Zone Routingを踏まえて ｜ Developers.</description>
    </item>
    
    <item>
      <title>fluentdでKinesis streamsに送るときの性能確認</title>
      <link>https://www.sambaiz.net/article/108/</link>
      <pubDate>Mon, 05 Jun 2017 23:48:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/108/</guid>
      <description>localでのstreamsとproducerのbenchmark aws-fluent-plugin-kinesisの make benchmarkはlocalにDummyServerを立ち上げて送っている。
空でもいいのでroleをつけておく必要がある。
$ git clone https://github.com/awslabs/aws-fluent-plugin-kinesis.git $ cd aws-fluent-plugin-kinesis $ yum install -y ruby-devel gcc $ echo &amp;#39;gem &amp;#34;io-console&amp;#34;&amp;#39; &amp;gt;&amp;gt; Gemfile $ make $ make benchmark RATEを指定しなければデフォルトで秒間1000レコードが送られる設定。 fluentdを起動してから10秒後にプロセスをkillし、そのレコード数などを出力している。
t2.microでデフォルト(RATE=1000)で実行した結果がこれ。 固める分producerの方はややパフォーマンスが落ちる。
bundle exec rake benchmark TYPE=streams Results: requets: 20, raw_records: 9400, records: 9400 bundle exec rake benchmark TYPE=producer Results: requets: 14, raw_records: 1005, records: 8900 RATE=3000のとき。producerではraw_recordsが1/100、リクエスト数は1/5。 streamsだとシャードを増やしていく必要があるが、producerの方は当分大丈夫そうだ。
bundle exec rake benchmark TYPE=streams Results: requets: 57, raw_records: 27600, records: 27600 bundle exec rake benchmark TYPE=producer Results: requets: 12, raw_records: 241, records: 25200 RATE=10000のとき。raw_records, requestの圧縮率はさらに上がり、 パフォーマンスの差が大きくなってきている。</description>
    </item>
    
    <item>
      <title>BeanstalkでのパッケージのバージョンがAMIでのバージョンと異なる原因</title>
      <link>https://www.sambaiz.net/article/106/</link>
      <pubDate>Sun, 04 Jun 2017 23:40:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/106/</guid>
      <description>User-Dataとは EC2インスタンス起動時に、シェルスクリプトを走らせたりcloud-initディレクティブを適用できる機能。 コンソールではインスタンスの詳細の設定の、高度な詳細のところから設定できる。
BeanstalkでのUser-Data 実はBeanstalkでも使われていて、CloudFormationで設定されている。
&amp;#34; /bin/bash /tmp/ebbootstrap.sh &amp;#34;, ... &amp;#34;Fn::FindInMap&amp;#34;: [ &amp;#34;AWSEBOptions&amp;#34;, &amp;#34;options&amp;#34;, &amp;#34;UserDataScript&amp;#34; ] &amp;#34; &amp;gt; /tmp/ebbootstrap.sh &amp;#34;, ... &amp;#34;AWSEBOptions&amp;#34;: { &amp;#34;options&amp;#34;: { &amp;#34;UserDataScript&amp;#34;: &amp;#34;https://s3-ap-northeast-1.amazonaws.com/elasticbeanstalk-env-resources-ap-northeast-1/stalks/eb_node_js_4.0.1.90.2/lib/UserDataScript.sh&amp;#34;, &amp;#34;guid&amp;#34;: &amp;#34;f08557fc43ac&amp;#34;, } } このshellの中では、時計を同期させたり、awsebユーザーを作成したりするほかに、 非Beanstalk AMI(is_baked=false)ではyum updateが走るようになっている。 そのため、AMIでのバージョンとBeanstalkで立ち上がったときのバージョンが異なることがあるようだ。
GUID=$7 function update_yum_packages { if is_baked update_yum_packages_$GUID; then log yum update has already been done. else log Updating yum packages. yum --exclude=aws-cfn-bootstrap update -y || echo Warning: cannot update yum packages. Continue... mark_installed update_yum_packages_$GUID # Update system-release RPM package will reset the .</description>
    </item>
    
    <item>
      <title>Node.jsでの文字コードの変換</title>
      <link>https://www.sambaiz.net/article/89/</link>
      <pubDate>Tue, 28 Mar 2017 21:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/89/</guid>
      <description>node-iconvを使う。
$ npm install iconv SHIFT_JISからUTF-8への変換はこんな感じ。
const Iconv = require(&amp;#39;iconv&amp;#39;).Iconv; const before = new Buffer([ 0x8b, 0x8d, 0x8e, 0x4d, 0x26, 0x82, 0xb2, 0x94, 0xd1 ]); const iconv = new Iconv(&amp;#39;SHIFT_JIS&amp;#39;, &amp;#39;UTF-8&amp;#39;); console.log(`before: ${before.toString(&amp;#39;hex&amp;#39;)}${before.toString()}`) const after = iconv.convert(before); console.log(`after: ${after.toString(&amp;#39;hex&amp;#39;)}${after.toString()}`); before: 8b8d8e4d2682b294d1 ���M&amp;amp;���� after: e7899be79abf26e38194e9a3af 牛皿&amp;amp;ご飯 文字コードによっては変換後に表せないことがある。 例えば、UTF-8からSHIFT_JISへの変換でサロゲートペア🍚を渡すと変換できず、エラーになる。
throw errnoException(&amp;#39;EILSEQ&amp;#39;, &amp;#39;Illegal character sequence.&amp;#39;); //IGNOREを付けることで そのような文字があった場合でもエラーにしないようにできる。
const Iconv = require(&amp;#39;iconv&amp;#39;).Iconv; const before = &amp;#34;牛皿&amp;amp;🍚&amp;#34;; const iconv = new Iconv(&amp;#39;UTF-8&amp;#39;, &amp;#39;SHIFT_JIS//IGNORE&amp;#39;); console.log(`before: ${new Buffer(before).</description>
    </item>
    
    <item>
      <title>FluentdとKPL(Kinesis Producer Library)でログをまとめてスループットを稼ぐ</title>
      <link>https://www.sambaiz.net/article/84/</link>
      <pubDate>Wed, 15 Mar 2017 23:00:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/84/</guid>
      <description>KPL(Kinesis Producer Library)とは Developing Amazon Kinesis Streams Producers Using the Amazon Kinesis Producer Library - Amazon Kinesis Streams
Kinesisに送るとき、自動リトライしてくれたり、レコードをまとめてスループットを向上してくれたりするアプリケーション。Protobufを使っている。 普通に送るとどんなに小さくてもシャード*1000レコード/秒しか最大でPUTできないのを、KPLを使ってまとめることで増やすことができる。
fluentdで送る aws-fluent-plugin-kinesisでkinesis_producerを指定するとKPLを使って送信する。
&amp;lt;kinesis_producer&amp;gt;の中にKPLの設定を書くことができる。
&amp;lt;kinesis_producer&amp;gt; record_max_buffered_time 10 &amp;lt;/kinesis_producer&amp;gt; record_max_bufferd_time はバッファされたレコードが送られるまでの最大時間(ms)。デフォルトは100ms。この時間が経つか、他のリミットに当たったらレコードは送られる。
 AggregationMaxCount: 一つのレコードにまとめる最大レコード数 AggregationMaxSize: まとめたレコードの最大バイト数 CollectionMaxCount: PutRecordsで送る最大アイテム数 CollectionMaxSize: PutRecordsで送るデータ量  CloudWatchに送るmetrics_levelはデフォルトでdetailedになっていて、 コンソールのメトリクスからstream名で検索すると KinesisProducerLibraryにUserRecordsPerKinesisRecordや、UserRecordsDataPut、BufferingTime、RequestTimeなどいろいろ表示される。
とりあえず試しにこんな設定で送ってみる。
&amp;lt;match hoge.log&amp;gt; @type kinesis_producer region ap-northeast-1 stream_name teststream include_time_key true flush_interval 1 buffer_chunk_limit 1m try_flush_interval 0.1 queued_chunk_flush_interval 0.01 num_threads 15 &amp;lt;/match&amp;gt; Lambdaで読む まとめられたレコードをkinesis-aggregationで分解して読む。 今回はNode.jsでやる。
$ npm install --save aws-kinesis-agg 注意する必要があるのはドキュメントの情報が古くて、 関数の引数が足りないこと。第二引数のcomputeChecksumsが抜けているので気付かないと一つずつずれていくことになる。</description>
    </item>
    
    <item>
      <title>fluentdでKinesis Streamsに送ってLambdaで読んでS3に保存する</title>
      <link>https://www.sambaiz.net/article/73/</link>
      <pubDate>Sun, 26 Feb 2017 18:56:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/73/</guid>
      <description>aws-fluent-plugin-kinesisでKinesis Streamsに送り、Lambdaで読んでS3に保存する。 要するにFirehoseのようなことをやりたいのだけれどTokyoリージョンにまだ来ないので自分でやる。
fluentdで送る $ td-agent-gem install fluent-plugin-kinesis try_flush_intervalとqueued_chunk_flush_intervalはドキュメントには載っていないが、 以下のページによるとそれぞれqueueに次のchunkがないときとあるときのflushする間隔。 いずれもデフォルトは1だが、これを減らすことでもっと頻繁に吐き出されるようになるらしい。
Fluentd の out_forward と BufferedOutput
あとシャードに振り分けるためのpartition_key を指定できる。デフォルトはランダム。
&amp;lt;source&amp;gt; @type tail path /var/log/td-agent/hoge.log pos_file /etc/td-agent/log.pos tag hoge.log format json time_key timestamp # 2017-01-01T01:01:01+0900 time_format %Y-%m-%dT%H:%M:%S%z &amp;lt;/source&amp;gt; &amp;lt;match hoge.log&amp;gt; @type kinesis_streams region ap-northeast-1 stream_name teststream include_time_key true flush_interval 1 buffer_chunk_limit 1m try_flush_interval 0.1 queued_chunk_flush_interval 0.01 num_threads 15 &amp;lt;/match&amp;gt; いくつか送ってみる。
for i in `seq 1 1000` do echo &amp;#39;{&amp;#34;hoge&amp;#34;: &amp;#34;fuga&amp;#34;, &amp;#34;timestamp&amp;#34;: &amp;#34;2017-01-01T01:01:01+0900&amp;#34;}&amp;#39; &amp;gt;&amp;gt; /var/log/td-agent/hoge.</description>
    </item>
    
    <item>
      <title>AWSのAssumeRole</title>
      <link>https://www.sambaiz.net/article/72/</link>
      <pubDate>Sat, 25 Feb 2017 20:40:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/72/</guid>
      <description>AWS Security Token Serviceによる、 RoleArn(arn:aws:iam::&amp;lt;account id&amp;gt;:role/&amp;lt;role name&amp;gt;)から一時的なCredentialを取得する仕組み。 前もって発行したAPIキーとは違い、有効期限が存在するため続けて呼ぶ場合は失効する前に再発行する必要がある。
ではRoleArnを知っていたら誰でも取得できるかというと、もちろんそうではなく、 ロールの信頼関係、&amp;quot;Action&amp;quot;: &amp;quot;sts:AssumeRole&amp;quot;のPrincipalのところで信頼する対象を設定する。 例えば、Serviceでec2.amazonaws.comを指定してEC2がAssumeRoleするのを許可したり、 AWSで(他の)アカウントやユーザーを指定してそのAPIキーでこのRoleのCredentialを取得できるようにしたりといった感じ。
{ &amp;#34;Version&amp;#34;: &amp;#34;2012-10-17&amp;#34;, &amp;#34;Statement&amp;#34;: [ { &amp;#34;Effect&amp;#34;: &amp;#34;Allow&amp;#34;, &amp;#34;Principal&amp;#34;: { &amp;#34;Service&amp;#34;: &amp;#34;ec2.amazonaws.com&amp;#34; }, &amp;#34;Action&amp;#34;: &amp;#34;sts:AssumeRole&amp;#34; } ] } EC2にロールを設定すると、実はそのロールについてAssumeRoleして自動でCredentialを取得している。 EC2にロールを設定するにはロールとは別に インスタンスプロファイルを作成 する必要があるが、コンソールでEC2のサービスロールを作ると同名のインスタンスプロファイルが自動で作成される。 さらに、AssumeRoleのServiceとしてec2.amazonaws.comが追加されている。
$ curl http://169.254.169.254/latest/meta-data/iam/info { &amp;#34;Code&amp;#34; : &amp;#34;Success&amp;#34;, &amp;#34;LastUpdated&amp;#34; : &amp;#34;2017-02-25T10:56:33Z&amp;#34;, &amp;#34;InstanceProfileArn&amp;#34; : &amp;#34;arn:aws:iam::*****:instance-profile/assume_role_test&amp;#34;, &amp;#34;InstanceProfileId&amp;#34; : &amp;#34;*****&amp;#34; } $ curl http://169.254.169.254/latest/meta-data/iam/security-credentials/assume_role_test { &amp;#34;Code&amp;#34; : &amp;#34;Success&amp;#34;, &amp;#34;LastUpdated&amp;#34; : &amp;#34;2017-02-25T10:56:23Z&amp;#34;, &amp;#34;Type&amp;#34; : &amp;#34;AWS-HMAC&amp;#34;, &amp;#34;AccessKeyId&amp;#34; : &amp;#34;*****&amp;#34;, &amp;#34;SecretAccessKey&amp;#34; : &amp;#34;*****&amp;#34;, &amp;#34;Token&amp;#34; : &amp;#34;*****&amp;#34;, &amp;#34;Expiration&amp;#34; : &amp;#34;2017-02-25T17:26:07Z&amp;#34; } 参考 IAMロール徹底理解 〜 AssumeRoleの正体 ｜ Developers.</description>
    </item>
    
    <item>
      <title>ELBのスケーリングとsurge queue</title>
      <link>https://www.sambaiz.net/article/68/</link>
      <pubDate>Tue, 21 Feb 2017 19:48:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/68/</guid>
      <description>バックエンドだけではなくELB自体もスケーリングし、内部node数はdigで調べることができる。 このnode数は自分ではコントロールできず、基本的に意識することはない。
$ dig ****.ap-northeast-1.elb.amazonaws.com ;; ANSWER SECTION: *****.elb.amazonaws.com. 60 IN A xxx.xxx.xxx.xxx *****.elb.amazonaws.com. 60 IN A yyy.yyy.yyy.yyy nodeが増えるのにはある程度時間がかかるので、 アクセスが急増(5分間で50%以上のトラフィック増加が目安) したら捌ききれず、503を返すことがある。 前もって多量のアクセスが来ることが分かっていて、 AWSサポートがBusiness以上なら pre-warming申請することでnodeが増えた状態で待ち構えられる。
バックエンドのアプリケーションがリクエストを処理できない場合、ELBのsurge queueに溜まっていく。 この数はCloudWatchのSurgeQueueLength(キュー長の急増)メトリクスで確認できる。 また、SurgeQueueLengthの最大値1024を超えるとリクエストは拒否され、その数はSpoiloverCount(過剰数)メトリクスに出る。
参考 ELBの挙動とCloudWatchメトリクスの読み方を徹底的に理解する ｜ Developers.IO
Elastic Load Balancing でのレイテンシーのトラブルシューティング</description>
    </item>
    
    <item>
      <title>Kinesis Streams/Firehose/Analyticsを試す</title>
      <link>https://www.sambaiz.net/article/67/</link>
      <pubDate>Mon, 20 Feb 2017 21:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/67/</guid>
      <description>https://aws.amazon.com/jp/kinesis/
リアルタイムのストリーミングデータを扱うサービス群。 いまのところTokyoリージョンではKinesis Streamsしか使えない。
Kinesis Firehose AWSのデータストアに送るストリーム。自分でデータを読む処理を書かなくてよく、スケーリングも勝手にやってくれるので簡単に使える。
https://aws.amazon.com/jp/kinesis/firehose/faqs/
Q: 送信先とは何ですか? 送信先はデータが配信されるデータストアです。Amazon Kinesis Firehose では、 現在送信先として Amazon S3、Amazon Redshift、Amazon Elasticsearch Service がサポートされています。 料金は取り込まれたデータ量による。 一見そんなに高くならないように見えるが、5KB単位で切り上げられるのでレコードのサイズが小さくて数が多い場合に注意が必要。
今回はS3に送ってみる。
圧縮方法を設定したり、Lambdaを噛ませたりすることができる。
StatusがActiveになったらKinesis Agentで送ってみる。 CloudWatchとFirehoseにPutする権限が必要。Firehoseはkinesis:ではなくfirehose:なので注意。
$ sudo yum install –y aws-kinesis-agent /etc/aws-kinesis/agent.jsonを編集する。リージョンごとのエンドポイントは ここ にある。
{ &amp;#34;awsAccessKeyId&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;awsSecretAccessKey&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;firehose.endpoint&amp;#34;: &amp;#34;https://firehose.us-east-1.amazonaws.com&amp;#34;, &amp;#34;flows&amp;#34;: [ { &amp;#34;filePattern&amp;#34;: &amp;#34;/tmp/hoge.log&amp;#34;, &amp;#34;deliveryStream&amp;#34;: &amp;#34;hogefugastream&amp;#34; } ] } $ sudo service aws-kinesis-agent start $ sudo chkconfig aws-kinesis-agent on $ echo &amp;#34;aaa&amp;#34; &amp;gt;&amp;gt; /tmp/hoge.log $ tail /var/log/aws-kinesis-agent/aws-kinesis-agent.</description>
    </item>
    
    <item>
      <title>GoでDynamoDBを使う</title>
      <link>https://www.sambaiz.net/article/63/</link>
      <pubDate>Sun, 12 Feb 2017 23:15:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/63/</guid>
      <description>テーブルを作成する プライマリキー テーブルの操作のガイドライン - Amazon DynamoDB
プライマリキーとしてパーティションキー(ハッシュキー)とオプションのソートキー(レンジキー)を設定する。 DynamoDBはこのパーティションキーに基づいて、複数のパーティションに分散して保存する。 テーブルにプロビジョニングされたスループット性能はパーティション間で均等に使われるので、 ソートキーを設定する場合にこれを最大限に活用するためには、 あるパーティションにリクエストが集中しないよう、パーティションキーに特定の値ばかり集中しないようなフィールドを 選ぶ必要がある。
セカンダリインデックス パーティションキーのグローバルセカンダリインデックス(GSI)と ソートキーのローカルセカンダリインデックス(LSI)がある。 射影されるフィールドを選択でき、ここに含まれないフィールドは返さない。 ただし、すべてをインデックスに書き込むのはコストが高いのでなるべく絞る。
キャパシティユニット  1読み込みキャパシティユニット: 4kbを超えないデータを1秒に1~2回(整合性による)読み込める 1書き込みキャパシティユニット: 1kbを超えないデータを1秒に1回書き込める  ユニットに応じて1時間あたりで課金される。
未使用のキャパシティがある場合、最大5分保持してバーストに備えてくれる。
読み書きする aws-sdk-goを直接使ってもいいが、簡単に扱えるラッパー guregu/dynamo を使うことにした。
type Data struct { ID int64 `dynamo:&amp;#34;id&amp;#34;` Name string Age int } db := dynamo.New(session.New(), &amp;amp;aws.Config{Region: aws.String(&amp;#34;ap-northeast-1&amp;#34;)}) table := db.Table(&amp;#34;testtable&amp;#34;) Create &amp;amp; Update d := Data{ID: 1, Name: &amp;#34;hogefuga&amp;#34;, Age: 123} if err := table.Put(d).Run(); err != nil { return err } if err := table.</description>
    </item>
    
    <item>
      <title>EC2のインスタンスストア</title>
      <link>https://www.sambaiz.net/article/58/</link>
      <pubDate>Mon, 06 Feb 2017 21:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/58/</guid>
      <description>http://docs.aws.amazon.com/ja_jp/AWSEC2/latest/UserGuide/InstanceStorage.html
EC2ではインスタンスタイプによってはEBSに加えてインスタンスストアが使える。しかも追加料金なし。 対象はストレージが&amp;quot;EBSのみ&amp;quot;でないもの。
https://aws.amazon.com/jp/ec2/instance-types/
インスタンスストアはインスタンスが停止したり、障害が起きると消える一時ストレージ。再起動では消えない。 ホストに物理的にアタッチされているので、バッファやキャッシュなどの頻繁に読み書きされ、消えてもいいデータに最適。 他のインスタンスにアタッチすることはできない。容量や性能もインスタンスタイプに依存する。
インスタンスストアボリュームの追加は インスタンスの起動時に、新しいボリュームを追加し、ボリュームタイプをインスタンスストアにすることで行うことができる。
今回はSSDストレージ1 x 4のm3.mediumで試す。これは4gbのボリュームが一つ追加できるという意味。
まずはインスタンスストアを追加してないインスタンス。 lsblkというのはlist block devicesの略。
$ df -h ファイルシス サイズ 使用 残り 使用% マウント位置 /dev/xvda1 7.8G 1.2G 6.6G 15% / ... $ dd if=/dev/zero of=hoge bs=1M count=1000 $ ls -sh 合計 1001M 1001M hoge $ df -h ファイルシス サイズ 使用 残り 使用% マウント位置 /dev/xvda1 7.8G 2.2G 5.6G 28% / ... $ lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT xvda 202:0 0 8G 0 disk └─xvda1 202:1 0 8G 0 part / それに対してインスタンスストア(/dev/xvdb)を追加したインスタンス。</description>
    </item>
    
    <item>
      <title>複数EC2インスタンスを立ち上げてvegetaで負荷試験する</title>
      <link>https://www.sambaiz.net/article/43/</link>
      <pubDate>Sun, 18 Dec 2016 20:52:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/43/</guid>
      <description>vegetaで負荷をかける。
インスタンスを立ち上げるスクリプト コードはここ。 sambaiz/loadtest
まずキーペアを作成し、EC2インスタンスを立ち上げて、全てのインスタンスが使えるようになるまで待つ。
aws ec2 create-key-pair --key-name LoadTestKeyPare --query &amp;#39;KeyMaterial&amp;#39; --output text &amp;gt; LoadTestKeyPare.pem chmod 400 LoadTestKeyPare.pem aws ec2 run-instances --image-id $AMI_ID --count $INSTANCE_NUM --instance-type t2.micro --key-name LoadTestKeyPare --security-group-ids $SECURITY_GROUP_IDS --subnet-id $SUBNET_ID ... aws ec2 wait instance-status-ok --instance-ids $INSTANCE_IDS このAMIは事前にPackerでつくったもの。vegetaをインストールしてファイルディスクリプタの上限を増やしている。
{ &amp;#34;variables&amp;#34;: { &amp;#34;aws_access_key&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;aws_secret_key&amp;#34;: &amp;#34;&amp;#34; }, &amp;#34;builders&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;amazon-ebs&amp;#34;, &amp;#34;access_key&amp;#34;: &amp;#34;{{user `aws_access_key`}}&amp;#34;, &amp;#34;secret_key&amp;#34;: &amp;#34;{{user `aws_secret_key`}}&amp;#34;, &amp;#34;region&amp;#34;: &amp;#34;ap-northeast-1&amp;#34;, &amp;#34;source_ami&amp;#34;: &amp;#34;ami-0c11b26d&amp;#34;, &amp;#34;instance_type&amp;#34;: &amp;#34;t2.micro&amp;#34;, &amp;#34;ssh_username&amp;#34;: &amp;#34;ec2-user&amp;#34;, &amp;#34;ami_name&amp;#34;: &amp;#34;loadtest {{timestamp}}&amp;#34; }], &amp;#34;provisioners&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;shell&amp;#34;, &amp;#34;inline&amp;#34;: [ &amp;#34;wget https://github.</description>
    </item>
    
    <item>
      <title>PackerでAMIを作る</title>
      <link>https://www.sambaiz.net/article/24/</link>
      <pubDate>Tue, 18 Oct 2016 22:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/24/</guid>
      <description>https://www.packer.io/
いろんなプラットフォームのイメージを作ることができるツール。 これでfluentdのログサーバーのAMIを作る。
$ brew install packer # mac $ packer -v 0.10.1 設定ファイルはこんな感じ。variablesの値は{{user ... }}のところで使われる。 buildersに作るイメージの情報を書いて、provisionersで環境を作る。
provisionersにはchefやansibleなども指定できるが、 継ぎ足し継ぎ足しで秘伝のタレ化したAMIも最初は、コマンドいくつか実行するだけなのでとりあえず手作業で作った、後でなんとかするなんてものもあったりして、 そういうものは無理にchefなどで始めず、手軽にshellでpacker buildするといいと思う。 手作業よりも楽だしソースが別にあるので使われていないAMIを消すのも簡単。
fileではpermissionがないところに置くことができないので、一旦置いてshellで移動する。
{ &amp;#34;variables&amp;#34;: { &amp;#34;aws_access_key&amp;#34;: &amp;#34;&amp;#34;, &amp;#34;aws_secret_key&amp;#34;: &amp;#34;&amp;#34; }, &amp;#34;builders&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;amazon-ebs&amp;#34;, &amp;#34;access_key&amp;#34;: &amp;#34;{{user `aws_access_key`}}&amp;#34;, &amp;#34;secret_key&amp;#34;: &amp;#34;{{user `aws_secret_key`}}&amp;#34;, &amp;#34;region&amp;#34;: &amp;#34;ap-northeast-1&amp;#34;, &amp;#34;source_ami&amp;#34;: &amp;#34;ami-1a15c77b&amp;#34;, &amp;#34;instance_type&amp;#34;: &amp;#34;t2.small&amp;#34;, &amp;#34;ssh_username&amp;#34;: &amp;#34;ec2-user&amp;#34;, &amp;#34;ami_name&amp;#34;: &amp;#34;fluentd-logserver {{timestamp}}&amp;#34; }], &amp;#34;provisioners&amp;#34;: [{ &amp;#34;type&amp;#34;: &amp;#34;file&amp;#34;, &amp;#34;source&amp;#34;: &amp;#34;td-agent.conf&amp;#34;, &amp;#34;destination&amp;#34;: &amp;#34;/home/ec2-user/td-agent.conf&amp;#34; }, { &amp;#34;type&amp;#34;: &amp;#34;shell&amp;#34;, &amp;#34;inline&amp;#34;: [ &amp;#34;curl -L https://toolbelt.treasuredata.com/sh/install-redhat-td-agent2.sh | sh&amp;#34;, &amp;#34;sudo mv /home/ec2-user/td-agent.</description>
    </item>
    
  </channel>
</rss>
