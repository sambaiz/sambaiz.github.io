<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>docker on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/docker/</link>
    <description>Recent content in docker on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Sun, 22 Nov 2020 01:25:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GitHub ActionsのDocker container actionを作る</title>
      <link>https://www.sambaiz.net/article/310/</link>
      <pubDate>Sun, 22 Nov 2020 01:25:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/310/</guid>
      <description>GitHub ActionsのActionにはランナーで直接実行されるJavaScript actionと、Docker container action、複数のステップをまとめたComposite run steps actionがある。 Docker container actionは環境を固定できるが、イメージを取得する分JavaScript actionと比べて時間がかかり、DockerがインストールされているLinuxでしか使えない。 今回はこのDocker container actionを作って動かしてみる。
Dockerfileとメタデータのaction.ymlを用意する。 using: &#39;docker&#39; にするとDocker container actionになる。
::set-output name={name}::{value}の文字列をstdoutに出力するとoutputsのパラメータに値がセットされる。
$ cat Dockerfile FROM alpine:20200917 ENTRYPOINT [&amp;#34;echo&amp;#34;, &amp;#34;::set-output name=return-json::&amp;#34;] $ cat action.yml name: &amp;#39;sambaiz-test-docker-github-action&amp;#39; description: &amp;#39;Echo your word&amp;#39; inputs: i-say: description: &amp;#39;What do you say?&amp;#39; required: true default: &amp;#39;{}&amp;#39; outputs: return-word: description: &amp;#39;return word you said&amp;#39; runs: using: &amp;#39;docker&amp;#39; image: &amp;#39;Dockerfile&amp;#39; args: - ${{ inputs.i-say }} usesにはリポジトリではなくパスを渡すこともできるので、次のようなワークフローで試しに実行できる。</description>
    </item>
    
    <item>
      <title>VSCodeのdevcontainerにSAM CLIをインストールしlocal invokeする</title>
      <link>https://www.sambaiz.net/article/301/</link>
      <pubDate>Mon, 12 Oct 2020 09:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/301/</guid>
      <description>VSCodeのdevcontainerにAWS SAM CLIをインストールしてDockerを用いたlocal invokeもできるようにする。
VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する - sambaiz-net
HomebrewとSAM CLIのインストール 手順に従ってbrewでインストールする。
 Homebrewがsudoとgit、psを必要とするのでインストールする デフォルトのrootでHomebrewをインストールするとDon&amp;rsquo;t run this as root!になるので non-root userを作って そのユーザーで実行する brew install aws-sam-cliでsamはインストールできているのにexit 1して失敗するのを握り潰している 次にdockerが必要となるので入れている  FROMdebian:buster ARG USERNAME=vscode ARG USER_UID=1000 ARG USER_GID=$USER_UID # Create the user RUN groupadd --gid $USER_GID $USERNAME \  &amp;amp;&amp;amp; useradd --uid $USER_UID --gid $USER_GID -m $USERNAME \  # # [Optional] Add sudo support. Omit if you don&amp;#39;t need to install software after connecting.</description>
    </item>
    
    <item>
      <title>VSCodeのDocker開発コンテナでJupyter Notebookを開いてAthenaのクエリを実行し可視化する</title>
      <link>https://www.sambaiz.net/article/294/</link>
      <pubDate>Fri, 04 Sep 2020 19:34:04 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/294/</guid>
      <description>ローカルでJupyter Notebookを動かすために以前はjupyter/datascience-notebookのイメージを立ち上げていた。 Notebookはエディタとしての機能に乏しいため通常のコードを書くのが大変だったのだが、 VSCodeのPython extensionにはJupyter notebookサポートが入っていてそのまま開けて実行できるのを知ったので移行することにした。 今回はVSCodeのDocker開発コンテナからNotebookを開いてAthenaのクエリを実行し可視化する。
VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する - sambaiz-net
環境構築 ~/.awsをマウントする。Dockerfileは上記の記事のと同じ。
{ &amp;#34;name&amp;#34;: &amp;#34;Python&amp;#34;, &amp;#34;build&amp;#34;: { &amp;#34;dockerfile&amp;#34;: &amp;#34;Dockerfile&amp;#34; }, &amp;#34;settings&amp;#34;: { &amp;#34;terminal.integrated.shell.linux&amp;#34;: &amp;#34;/bin/bash&amp;#34;, &amp;#34;python.pythonPath&amp;#34;: &amp;#34;/usr/local/bin/python&amp;#34;, &amp;#34;python.linting.enabled&amp;#34;: true, &amp;#34;python.linting.pylintEnabled&amp;#34;: false, &amp;#34;python.linting.flake8Enabled&amp;#34;: true, &amp;#34;python.linting.flake8Path&amp;#34;: &amp;#34;/usr/local/bin/flake8&amp;#34;, &amp;#34;editor.formatOnSave&amp;#34;: true, &amp;#34;python.formatting.provider&amp;#34;: &amp;#34;yapf&amp;#34;, &amp;#34;python.formatting.yapfPath&amp;#34;: &amp;#34;/usr/local/bin/yapf&amp;#34;, &amp;#34;python.linting.mypyEnabled&amp;#34;: true, &amp;#34;python.linting.mypyPath&amp;#34;: &amp;#34;/usr/local/bin/mypy&amp;#34;, }, &amp;#34;extensions&amp;#34;: [ &amp;#34;ms-python.python&amp;#34; ], &amp;#34;mounts&amp;#34;: [ &amp;#34;source=${localEnv:HOME}/.aws,target=/root/.aws,type=bind,readonly&amp;#34; ] } アプリケーションで使うパッケージはPoetryでインストールすることにしている。
PoetryでPythonの依存パッケージを管理する - sambaiz-net
KernalをvenvのPythonに切り替えるためにipykernelをインストールする必要がある。
[tool.poetry.dependencies] python = &amp;#34;^3.8&amp;#34; PyAthena = &amp;#34;^1.11.1&amp;#34; pandas = &amp;#34;^1.1.1&amp;#34; ipykernel = &amp;#34;^5.</description>
    </item>
    
    <item>
      <title>VSCodeのRemote DevelopmentでSageMakerのコンテナ環境でモデルを開発する</title>
      <link>https://www.sambaiz.net/article/289/</link>
      <pubDate>Sun, 19 Jul 2020 19:34:04 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/289/</guid>
      <description>SageMakerで学習させるモデルを開発するにあたって、Notebooks上ではコードを書きづらいのでVS Codeで書いているのだが、ローカルに依存パッケージをインストールして実行しているため エディタ上では警告が出ていなくても、実際の環境にはパッケージがなかったりすることがある。
そんな場合に便利なのがVS CodeのRemote Development。 これはローカルのVS CodeからリモートのVS Code Serverに接続してその環境で開発することができるエクステンションで、 Dockerコンテナのほか、SSHでリモートマシンやVMに接続したり、WindowsならWSLにも接続して開発環境を揃えることができる。
SageMakerでPyTorchのモデルを学習させる - sambaiz-net
設定 .devcontainer/に次のファイルを置く。
PyTorch  Dockerfile  aws/deep-learning-containersの Deep Learning Containers Imagesから選び、ECRからpullするため認証情報を登録しておく。
$ aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin https://763104351884.dkr.ecr.us-east-1.amazonaws.com これをベースに、開発用ツールを入れてVSCodeのDevelopment Container Scriptsを実行する。
$ cat .devcontainer/Dockerfile FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.5.1-cpu-py36-ubuntu16.04 RUN conda install -y yapf flake8 mypy # VS Code Development Container Scripts # https://github.com/microsoft/vscode-dev-containers/tree/v0.128.0/script-library ARG INSTALL_ZSH=&amp;#34;true&amp;#34; ARG USERNAME=&amp;#34;vscode&amp;#34; ARG USER_UID=&amp;#34;1000&amp;#34; ARG USER_GID=&amp;#34;${USER_UID}&amp;#34; ARG UPGRADE_PACKAGES=&amp;#34;true&amp;#34; ARG COMMON_SCRIPT_SOURCE=&amp;#34;https://raw.</description>
    </item>
    
    <item>
      <title>ECSでアプリケーションを動かすBoilerplateを作った</title>
      <link>https://www.sambaiz.net/article/259/</link>
      <pubDate>Mon, 24 Feb 2020 16:17:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/259/</guid>
      <description>https://github.com/sambaiz/ecs-boilerplate
ECS上でアプリケーションを動かすBoilerplateを作った。CDKでデプロイする。以前Digdagを動かしたときのを汎用的にしたもの。
CDKでECS+Fargate上にDigdagを立ててCognito認証を挟む - sambaiz-net
new ECSStack(app, &amp;#39;ECSBoilerplateSampleStack&amp;#39;, { /* // If vpcAttributes is not specified, new VPC is created. vpcAttributes: { vpcId: &amp;#39;&amp;#39;, availabilityZones: [], publicSubnetIds: [], privateSubnetIds: [], }, // DNS record. Even if this is not specified, you can access with ELB domain (***.elb.amazonaws.com) route53: { zoneId: &amp;#39;&amp;#39;, zoneName: &amp;#39;example.com&amp;#39;, recordName: &amp;#39;foo&amp;#39;, }, // Certificate Manager ARN. Required if accessing with HTTPS acmArn: &amp;#39;arn:aws:acm:****&amp;#39; // default values containerPort: 8080, cpu: 256, memoryLimitMiB: 512, minCapacity: 1, maxCapacity: 5, scaleCPUPercent: 80 */ }); CDKがECRへのpushまでやってくれるのでcdk deployすれば動き始め、削除するときもStackを消せばよい。</description>
    </item>
    
    <item>
      <title>Buildkitとは</title>
      <link>https://www.sambaiz.net/article/258/</link>
      <pubDate>Tue, 11 Feb 2020 01:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/258/</guid>
      <description>Buildkitは高速でセキュアなコンテナイメージのビルドツール。 Docker本体にも18.09から統合され、 DOCKER_BUILDKIT=1 docker build するとBuildkitが使われるようになった。
LLB (low-level builder) BuildkitはDockerfileなどをLLBという中間言語にコンパイルする。 LLBは依存関係を表すDAG(Directed acyclic graph; 有向非巡回グラフ)で、 protobufで定義されている。 これによって処理を並列に実行したり、Dockerfileを変更してもそれ以降のステージのキャッシュを全て破棄する必要がなくなった。
--mount=type=cache Dockerfileを変更しても残せるキャッシュ。
# syntax = docker/dockerfile:1.1-experimental FROM golang RUN --mount=type=cache,target=/root/.cache/go-build \ go build ... --mount=type=secret 秘密鍵など一度ADDしてしまったファイルは最終的に消してもレイヤ上に残ってしまうが、--mount=type=secret でマウントすると残らない。
# syntax = docker/dockerfile:1.1-experimental FROM alpine:20200122 RUN apk add --no-cache git openssh RUN --mount=type=secret,id=ssh,dst=/root/.ssh/id_rsa \ ssh-keyscan -H github.com &amp;gt;&amp;gt; /root/.ssh/known_hosts &amp;amp;&amp;amp; \ git clone ... $ DOCKER_BUILDKIT=1 docker build --secret id=ssh,src=./id_rsa . 参考 Introducing BuildKit - Moby Blog</description>
    </item>
    
    <item>
      <title>ECS(EC2)のCloudFormation最小構成</title>
      <link>https://www.sambaiz.net/article/247/</link>
      <pubDate>Fri, 15 Nov 2019 20:12:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/247/</guid>
      <description>EC2でECSのServiceを動かすCFnテンプレートを書く。以前Fargateで動かしたものを一部再利用する。
ECS FargateでSidecarのFluentdでログをS3に送る構成をCloudFormationで構築する - sambaiz-net
EC2で動かす場合、自分でリソースが不足しないようにインスタンスのスケールを気遣うことになるが、VPC外での実行やprivilegedをtrueにするなどEC2でしかできないことがある。あと同リソースで比較すると安い。
まずはEC2インスタンス以外のリソースを書く。LaunchType以外はFargateのときとほぼ同じ。 LBなしでバッチのようなものを動かすことを想定した最小構成。
ECSCluster: Type: AWS::ECS::Cluster Properties: ClusterName: &amp;#39;test-cluster&amp;#39; LogGroup: Type: AWS::Logs::LogGroup Properties: LogGroupName: &amp;#39;test-task-log-group&amp;#39; RetentionInDays: 1 TaskDefinition: Type: AWS::ECS::TaskDefinition Properties: RequiresCompatibilities: - EC2 Cpu: &amp;#39;256&amp;#39; Memory: &amp;#39;512&amp;#39; ContainerDefinitions: - Name: &amp;#39;app&amp;#39; Image: &amp;#39;busybox&amp;#39; EntryPoint: - &amp;#39;sh&amp;#39; - &amp;#39;-c&amp;#39; Command: - &amp;#39;while true; do echo &amp;#34;{\&amp;#34;foo\&amp;#34;:1000,\&amp;#34;time\&amp;#34;:\&amp;#34;2019-05-09T20:00:00+09:00\&amp;#34;}&amp;#34;; sleep 10; done&amp;#39; Essential: &amp;#39;true&amp;#39; LogConfiguration: LogDriver: &amp;#39;awslogs&amp;#39; Options: awslogs-group: !Ref LogGroup awslogs-region: &amp;#39;ap-northeast-1&amp;#39; awslogs-stream-prefix: &amp;#39;app&amp;#39; Environment: - Name: &amp;#39;TZ&amp;#39; Value: &amp;#39;Asia/Tokyo&amp;#39; Volumes: - Name: &amp;#39;varlog&amp;#39; ECSService: Type: AWS::ECS::Service Properties: Cluster: !</description>
    </item>
    
    <item>
      <title>CDKでECS&#43;Fargate上にDigdagを立ててCognito認証を挟む</title>
      <link>https://www.sambaiz.net/article/234/</link>
      <pubDate>Wed, 31 Jul 2019 03:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/234/</guid>
      <description>AWSでワークフローエンジンDigdagを立てるにあたりスケールを見越してECS+Fargateで動かす。 全体のコードはGitHubにある。
FargateでECSを使う - sambaiz-net
リソースはCDKで作る。最近GAになったので高レベルのクラスを積極的に使っている。
AWS CDKでCloudFormationのテンプレートをTypeScriptから生成しデプロイする - sambaiz-net
$ npm run cdk -- --version 1.2.0 (build 6b763b7) VPC FargateなのでVPCが必要。 テンプレートを書くとSubnetやRouteTable、NATGatewayなど記述量が多くなるところだが、CDKだとこれだけで済む。
CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う - sambaiz-net
const vpc = new ec2.Vpc(this, &amp;#39;VPC&amp;#39;, { cidr: props.vpcCidr, natGateways: 1, maxAzs: 2, subnetConfiguration: [ { name: &amp;#39;digdag-public&amp;#39;, subnetType: ec2.SubnetType.PUBLIC, }, { name: &amp;#39;digdag-private&amp;#39;, subnetType: ec2.SubnetType.PRIVATE, }, { name: &amp;#39;digdag-db&amp;#39;, subnetType: ec2.SubnetType.ISOLATED, } ] }) DB DigdagはPostgreSQLを使う。
const db = new rds.DatabaseCluster(this, &amp;#39;DBCluster&amp;#39;, { engine: rds.</description>
    </item>
    
    <item>
      <title>ECS FargateでSidecarのFluentdでログをS3に送る構成をCloudFormationで構築する</title>
      <link>https://www.sambaiz.net/article/221/</link>
      <pubDate>Thu, 09 May 2019 23:54:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/221/</guid>
      <description>DAEMONを動かすことはできず、 fluentd logdriverもサポートされていないFargateで、 サイドカーとしてFluentdのコンテナを動かしてアプリケーションのログをS3に送る。 全体のコードはGitHubにある。
FargateでECSを使う - sambaiz-net
Kubernetesの1PodでAppとfluentdコンテナを動かしてBigQueryに送る - sambaiz-net
Fluentd 必要なプラグインと設定ファイルを入れたイメージを作る。
FROM fluent/fluentd:v1.4-1 USER root COPY ./fluent.conf /fluentd/etc/ # install plugin RUN apk add --update-cache --virtual .build-deps sudo build-base ruby-dev \ &amp;amp;&amp;amp; gem install fluent-plugin-s3 -v 1.0.0 --no-document \ &amp;amp;&amp;amp; gem install uuidtools \ &amp;amp;&amp;amp; gem sources --clear-all \ &amp;amp;&amp;amp; apk del .build-deps \ &amp;amp;&amp;amp; rm -rf /var/cache/apk/* \ /home/fluent/.gem/ruby/*/cache/*.gem # set timezone (Alpine) RUN apk --update-cache add tzdata &amp;amp;&amp;amp; \ cp /usr/share/zoneinfo/Asia/Tokyo /etc/localtime &amp;amp;&amp;amp; \ apk del tzdata &amp;amp;&amp;amp; \ rm -rf /var/cache/apk/* fluent.</description>
    </item>
    
    <item>
      <title>CircleCI 2.0でDocker imageをbuildしてタグを付けてContainer Registryに上げる</title>
      <link>https://www.sambaiz.net/article/183/</link>
      <pubDate>Wed, 22 Aug 2018 23:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/183/</guid>
      <description>(追記: 2019-04-13) 2.1からのOrbを使うと自分でjobを書かなくてもよくなる CircleCI 2.1からのOrbでdocker buildしてECRにpushし、Slackに通知させる - sambaiz-net
 masterにpushしたときと、リリースタグを切ったときにビルドされるようにする。
version: 2 jobs: build: docker: - image: google/cloud-sdk environment: GCP_PROJECT: &amp;lt;project_name&amp;gt; IMAGE_NAME: &amp;lt;image_name&amp;gt; steps: - checkout - setup_remote_docker: version: 18.05.0-ce - run: name: gcloud auth command: | echo $GCLOUD_SERVICE_KEY | base64 --decode &amp;gt; ${HOME}/gcloud-service-key.json gcloud auth activate-service-account --key-file ${HOME}/gcloud-service-key.json gcloud --quiet auth configure-docker - run: name: docker build &amp;amp; push command: | docker build -t asia.gcr.io/${GCP_PROJECT}/${IMAGE_NAME}:${CIRCLE_BUILD_NUM} . docker tag asia.</description>
    </item>
    
    <item>
      <title>MySQL InnoDBのロックの挙動</title>
      <link>https://www.sambaiz.net/article/158/</link>
      <pubDate>Sat, 03 Mar 2018 19:44:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/158/</guid>
      <description>https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html
トランザクション分離レベルはデフォルトのREPEATABLE-READ。
&amp;gt; SELECT @@GLOBAL.tx_isolation, @@tx_isolation; +-----------------------+-----------------+ | @@GLOBAL.tx_isolation | @@tx_isolation | +-----------------------+-----------------+ | REPEATABLE-READ | REPEATABLE-READ | +-----------------------+-----------------+ 準備 DBを立ち上げてテーブルとレコードを入れる。
$ cat schema_and_data.sql CREATE TABLE a ( id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY, name VARCHAR(128) NOT NULL ); CREATE TABLE b ( id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY, a_id BIGINT UNSIGNED NOT NULL, FOREIGN KEY (a_id) REFERENCES a (id) ); INSERT INTO a (id, name) VALUES (1, &amp;#39;1&amp;#39;); INSERT INTO a (id, name) VALUES (2, &amp;#39;2&amp;#39;); INSERT INTO a (id, name) VALUES (3, &amp;#39;3&amp;#39;); INSERT INTO a (id, name) VALUES (8, &amp;#39;8&amp;#39;); INSERT INTO a (id, name) VALUES (9, &amp;#39;9&amp;#39;); INSERT INTO a (id, name) VALUES (10, &amp;#39;10&amp;#39;); $ cat start.</description>
    </item>
    
    <item>
      <title>ElasticsearchをDockerで動かしてGrafanaで可視化する</title>
      <link>https://www.sambaiz.net/article/52/</link>
      <pubDate>Sun, 29 Jan 2017 17:08:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/52/</guid>
      <description>https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
vm.max_map_count (バーチャルメモリにマッピングできる最大ページ数) を262144以上にする。
$ sysctl vm.max_map_count $ grep vm.max_map_count /etc/sysctl.conf vm.max_map_count=262144 # sysctl -w vm.max_map_count=262144 30日トライアル後、有償ライセンスが必要になるxpackのsecurity(旧sheild)がデフォルトで有効になっているのでfalseにしている。
-cap-add=IPC_LOCKでLock memory(スワップアウトしないようにする)を 許可する。
https://www.elastic.co/guide/en/elasticsearch/reference/5.2/heap-size.html
ES_HEAP_SIZEでヒープ領域を設定する。デフォルトでは2GB。多いほどキャッシュに使用できる。 ただし、物理RAMの50%以下で、32GB近辺の境界を超えないようにする。
$ mkdir -p ~/do/elasticsearch/data $ docker run -itd -v elasticsearch:/usr/share/elasticsearch/data \ --name elasticsearch \ -p 9200:9200 \ -e xpack.security.enabled=false \ -e bootstrap.memory_lock=true -e cluster.name=hogehoge-cluster -e ES_JAVA_OPTS=&amp;#34;-Xms28g -Xmx28g&amp;#34; \ --cap-add=IPC_LOCK --ulimit memlock=-1:-1 --ulimit nofile=65536:65536 \ --restart=always \ docker.elastic.co/elasticsearch/elasticsearch:5.1.2 $ docker volume ls local elasticsearch 問題なく起動しているか確認する。
$ curl localhost:9200 | jq { &amp;#34;name&amp;#34;: &amp;#34;eqIkJ48&amp;#34;, &amp;#34;cluster_name&amp;#34;: &amp;#34;docker-cluster&amp;#34;, &amp;#34;cluster_uuid&amp;#34;: &amp;#34;Lsu_C7wORS6G-0m9PJ9sFQ&amp;#34;, &amp;#34;version&amp;#34;: { &amp;#34;number&amp;#34;: &amp;#34;5.</description>
    </item>
    
    <item>
      <title>ファイルディスクリプタの上限を増やす</title>
      <link>https://www.sambaiz.net/article/41/</link>
      <pubDate>Thu, 08 Dec 2016 21:36:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/41/</guid>
      <description>ファイルディスクリプタとは プロセスの外部とやりとりするための識別子。POSIXではint型で、0がstdin、1がstdout、2がstderrといったもの。 ファイルやデバイスに対するopen()や、 ネットワーク(INETドメインソケット)やホスト内(UNIXドメインソケット)で 通信するためのソケットを生成するsocket()などのシステムコールで生成される。
ファイルディスクリプタの上限 一つのプロセスがリソースを食いつぶさないように 使えるファイルディスクリプタの上限が決まっていて、ulimit -nで確認できる。デフォルトは大体1024。
$ ulimit -n 1024 各プロセスの上限と使っているファイルディスクリプタはこれで確認できる。
$ cat /proc/&amp;lt;プロセスID&amp;gt;/limits ... Max open files 1024 4096 files ... $ ls -l /proc/&amp;lt;プロセスID&amp;gt;/fd webサーバーのように同時にたくさん通信したりすると上限に達してしまい、Too many open filesになってしまうので増やす必要がある。
/etc/security/limits.confで変更する PAM認証時(ログインするときなど)に適用されるので、サーバーの起動時に立ち上がったデーモンには使えない。
$ cat /etc/pam.d/sshd ... session required pam_limits.so ... 全てのユーザー(*)のプロセスが使える ファイルディスクリプタ(nofile)のsoft(ユーザーが設定できる)とhard(rootが設定できる)上限を共に64000にする。
$ echo &amp;#34;* hard nofile 64000&amp;#34; &amp;gt;&amp;gt; /etc/security/limits.conf $ echo &amp;#34;* soft nofile 64000&amp;#34; &amp;gt;&amp;gt; /etc/security/limits.conf $ ulimit -n 64000 ulimit -nで変更する シェルと、起動したプロセスで有効。
$ ulimit -n 64000 dockerコンテナでは run時にulimitオプションで --ulimit nofile=11111のように指定することもできる。</description>
    </item>
    
    <item>
      <title>OpenVPNサーバーPritunlをDockerで動かす</title>
      <link>https://www.sambaiz.net/article/39/</link>
      <pubDate>Fri, 02 Dec 2016 21:05:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/39/</guid>
      <description>PritunlでVPNサーバーを立てる。
Dockerfileはこんな感じ。
https://hub.docker.com/r/sambaiz/pritunl/
FROM mongo:3.4 # https://docs.pritunl.com/docs/installation RUN echo &amp;#39;deb http://repo.pritunl.com/stable/apt jessie main&amp;#39; &amp;gt; /etc/apt/sources.list.d/pritunl.list &amp;amp;&amp;amp; \ apt-key adv --keyserver hkp://keyserver.ubuntu.com --recv 7568D9BB55FF9E5287D586017AE645C0CF8E292A &amp;amp;&amp;amp; \ apt-get --assume-yes update &amp;amp;&amp;amp; \ apt-get --assume-yes upgrade &amp;amp;&amp;amp; \ apt-get --assume-yes install pritunl iptables EXPOSE 80 443 12345/udp CMD mongod --fork --logpath /data/db/mongod.log &amp;amp;&amp;amp; echo &amp;#39;Setup Key:&amp;#39; `pritunl setup-key` &amp;amp;&amp;amp; pritunl start $ docker run -itd -p 80:80 -p 443:443 -p 12345:12345/udp --privileged sambaiz/pritunl $ docker logs &amp;lt;id&amp;gt; .</description>
    </item>
    
    <item>
      <title>GKEで複数コンテナのアプリケーションを動かす</title>
      <link>https://www.sambaiz.net/article/18/</link>
      <pubDate>Fri, 26 Aug 2016 21:57:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/18/</guid>
      <description>前回は単一コンテナのアプリケーションを動かしたが、今回はコンテナ間でやり取りが発生するものを動かす。 流れとしては、クライアントからのリクエストをGATEWAYサーバーで受け取り、SERVICEサーバーにリクエストし、その結果を返すまで。
プログラムは以下の通り、環境変数TYPEの値によって挙動を変えていて、同じイメージを使い回す。コードはここ。
var http = require(&amp;#39;http&amp;#39;); var handleRequest = function(request, response) { if(process.env.TYPE == &amp;#34;GATEWAY&amp;#34;){ console.log(&amp;#39;Passed.&amp;#39;); var options = { host: &amp;#39;service&amp;#39;, port: 8080, method: &amp;#39;GET&amp;#39; }; var req = http.request(options, function(res) { data = &amp;#34;&amp;#34; res.on(&amp;#39;data&amp;#39;, function (chunk) { data+=chunk; }); res.on(&amp;#39;end&amp;#39;, () =&amp;gt; { response.writeHead(200); response.end(data); }); }); req.on(&amp;#39;error&amp;#39;, function(e) { response.writeHead(500) response.end(e.message); }); req.end(); }else{ console.log(&amp;#39;Received.&amp;#39;); response.writeHead(200); response.end(&amp;#39;ok&amp;#39;); } }; var www = http.createServer(handleRequest); www.listen(8080); これをContainer RegistryにPushする。</description>
    </item>
    
    <item>
      <title>Google Container Engine(GKE)で単一コンテナのアプリケーションを動かす</title>
      <link>https://www.sambaiz.net/article/17/</link>
      <pubDate>Sun, 21 Aug 2016 23:37:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/17/</guid>
      <description>Kubernetes - Hello World Walkthrough
CloudSDKとkubectlのインストール Cloud SDKをインストールしてgloudコマンドを使えるようにする。
$ gcloud --version Google Cloud SDK 122.0.0 $ gcloud components install kubectl Google Container RegistryにPush $ export PROJECT_ID=&amp;#34;******&amp;#34; $ docker build -t gcr.io/$PROJECT_ID/test:v1 . $ gcloud docker push gcr.io/$PROJECT_ID/test:v1 プロジェクトの課金を有効にしていないとこんなエラーメッセージが出る。
denied: Unable to create the repository, please check that you have access to do so. Clusterの作成 $ gcloud config set core/project $PROJECT_ID $ gcloud config set compute/zone asia-east1-b $ gcloud container clusters create test-cluster $ gcloud config set container/cluster test-cluster Container Engine APIが有効になっていない場合はこうなる。 一度コンソールからContainer Engineを選ぶと、サービスの準備が始まって有効になる。</description>
    </item>
    
    <item>
      <title>Kubernetesのチュートリアルをたどる</title>
      <link>https://www.sambaiz.net/article/9/</link>
      <pubDate>Mon, 18 Jul 2016 22:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/9/</guid>
      <description>Kubernetesとは Kubernetes(発音はkoo-ber-nay&#39;-tace。 ギリシャ語で操舵手。)はGoogleによって開発が始められた、アプリケーションコンテナにおける自動デプロイ、スケーリング、操作を 自動化するOSS。K8sと略される。
Minikube K8sをローカルで試すために、MinikubeというVMの中で単一ノードのK8sクラスターを動かすツールを入れる。
v0.6.0
curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.6.0/minikube-darwin-amd64 &amp;amp;&amp;amp; chmod +x minikube &amp;amp;&amp;amp; sudo mv minikube /usr/local/bin/ $ minikube start Starting local Kubernetes cluster... ... $ kubectl version Client Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;2&amp;#34;, GitVersion:&amp;#34;v1.2.4&amp;#34;, GitCommit:&amp;#34;3eed1e3be6848b877ff80a93da3785d9034d0a4f&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} Server Version: version.Info{Major:&amp;#34;1&amp;#34;, Minor:&amp;#34;3&amp;#34;, GitVersion:&amp;#34;v1.3.0&amp;#34;, GitCommit:&amp;#34;283137936a498aed572ee22af6774b6fb6e9fd94&amp;#34;, GitTreeState:&amp;#34;clean&amp;#34;} Pods K8sではコンテナのグループをpodと呼ぶ。pod中のコンテナは共にデプロイされ、起動し、停止する。 また、グループとして複製される。
Podの定義は次のようにyamlで書かれる。
apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 Podの定義に望ましい状態を記述すると、Kubernatesはそれを見て現在の状態が一致しているかどうか確認する。 例えば、Podが作られたときに、コンテナがその中で動いている状態が望ましい状態だとすると、 コンテナが動かなくなったときに、Kubernatesは新しいものを再作成することで望ましい状態にする。</description>
    </item>
    
    <item>
      <title>Docker公式ドキュメント&#34;network コマンドを使う&#34;を読む</title>
      <link>https://www.sambaiz.net/article/7/</link>
      <pubDate>Fri, 15 Jul 2016 00:38:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/7/</guid>
      <description>Docker version 1.12.0-rc2 公式ドキュメントnetwork コマンドを使う の内容をまとめてみた。
dockerには3つのデフォルトネットワークが存在する。docker run時に--netオプションでネットワークを指定しない限り、 docker0として表示されるbridgeネットワークにコンテナを接続する。
$ docker network ls NETWORK ID NAME DRIVER SCOPE a3b712537566 bridge bridge local f6d86cb54edd host host local 33cb30b024d9 none null local ただし、後方互換性を維持するため、デフォルトのbridgeネットワークでは自動的に名前解決が行われない。
これらのネットワークとは別にユーザー定義のネットワークを作成することもできる。 単一ホストのbridgeネットワークと、複数ホストにまたがるoverlayネットワークから選択でき、 何も指定しなかったらbridgeになる。subnetを指定しなければ、 dockerデーモンがネットワークに対してサブネットを自動的に割り当てるが、 dockerが管理していないサブネットと重複するのを避けるために指定することが推奨されている。
$ docker network create -d bridge --subnet 172.25.0.0/16 isolated_nw $ docker network inspect isolated_nw $ docker network rm isolated_nw # 削除 docker network inspectで以下のようなネットワークの情報が得られる。
[ { &amp;#34;Name&amp;#34;: &amp;#34;isolated_nw&amp;#34;, &amp;#34;Id&amp;#34;: &amp;#34;c81547cf7ed897054ea645192c6c47dcf7a248e77bc8067609becab5330e417d&amp;#34;, &amp;#34;Scope&amp;#34;: &amp;#34;local&amp;#34;, &amp;#34;Driver&amp;#34;: &amp;#34;bridge&amp;#34;, &amp;#34;EnableIPv6&amp;#34;: false, &amp;#34;IPAM&amp;#34;: { &amp;#34;Driver&amp;#34;: &amp;#34;default&amp;#34;, &amp;#34;Options&amp;#34;: {}, &amp;#34;Config&amp;#34;: [ { &amp;#34;Subnet&amp;#34;: &amp;#34;172.</description>
    </item>
    
  </channel>
</rss>
