<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>database on sambaiz-net</title>
    <link>https://www.sambaiz.net/tags/database/</link>
    <description>Recent content in database on sambaiz-net</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>sambaiz-net</copyright>
    <lastBuildDate>Sun, 20 Feb 2022 01:49:00 +0900</lastBuildDate><atom:link href="https://www.sambaiz.net/tags/database/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>TPC-DSのクエリを用いたRedshift ServerlessとAthenaの性能比較</title>
      <link>https://www.sambaiz.net/article/397/</link>
      <pubDate>Sun, 20 Feb 2022 01:49:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/397/</guid>
      <description>Redshift Serverlessと他のサーバーレス集計サービス、Glue Data Catalogのテーブルへのクエリ実行 - sambaiz-net
現在PreviewのRedshift ServerlessとAthenaでデータベースのベンチマークであるTPC-DSのクエリを実行し性能を比較する。
GlueのTPC-DS Connectorで合計1TBのデータを生成しS3に保存した。 IAM Roleなどのリソースを作るCDKのコードはGitHubにある。
GlueのTPC-DS Connectorでデータを生成する - sambaiz-net
まずはjsonとparquetのデータに対して次のクエリを実行してみる。
select /* TPC-DS query96.tpl 0.1 */ count(*) from store_sales ,household_demographics ,time_dim, store where ss_sold_time_sk = time_dim.t_time_sk and ss_hdemo_sk = household_demographics.hd_demo_sk and ss_store_sk = s_store_sk and time_dim.t_hour = 8 and time_dim.t_minute &amp;gt;= 30 and household_demographics.hd_dep_count = 5 and store.s_store_name = &amp;#39;ese&amp;#39; order by count(*) limit 100; 結果は次の通り。
   service format run time (sec) cost (ap-northeast-1)     Athena json 738.</description>
    </item>
    
    <item>
      <title>GlueのTPC-DS Connectorでデータを生成する</title>
      <link>https://www.sambaiz.net/article/393/</link>
      <pubDate>Tue, 18 Jan 2022 21:26:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/393/</guid>
      <description>AthenaのTPC-DS Connectorで250GBのデータを生成してS3に出力しようとしたところ最大までLambdaのリソースを上げてもタイムアウトしてしまったのでGlueで行う。
AthenaのFederated QueryでTPC-DS Connectorを用いてデータを生成する - sambaiz-net
GlueのTPC-DS connectorをSubscribeしてActivateする。
 スクリプトはこんな感じで必要なパラメータを外から渡せるようにした。scaleはAthenaのものと同じくGB単位。 アップロード時にカタログにテーブルが追加されるようにしている。
from socket import create_connection import sys from awsglue.transforms import * from awsglue.utils import getResolvedOptions from pyspark.context import SparkContext from awsglue.context import GlueContext from awsglue.job import Job args = getResolvedOptions(sys.argv, [&amp;#34;JOB_NAME&amp;#34;, &amp;#34;BUCKET_NAME&amp;#34;, &amp;#34;DATABASE_NAME&amp;#34;, &amp;#34;SCALE&amp;#34;, &amp;#34;NUM_PARTITIONS&amp;#34;, &amp;#34;FORMAT&amp;#34;, &amp;#34;CONNECTION_NAME&amp;#34;]) sc = SparkContext() glueContext = GlueContext(sc) spark = glueContext.spark_session job = Job(glueContext) job.init(args[&amp;#34;JOB_NAME&amp;#34;], args) bucketName = args[&amp;#34;BUCKET_NAME&amp;#34;] tables = [ &amp;#34;call_center&amp;#34;, &amp;#34;catalog_page&amp;#34;, &amp;#34;catalog_returns&amp;#34;, &amp;#34;catalog_sales&amp;#34;, &amp;#34;customer&amp;#34;, &amp;#34;customer_address&amp;#34;, &amp;#34;customer_demographics&amp;#34;, &amp;#34;date_dim&amp;#34;, &amp;#34;dbgen_version&amp;#34;, &amp;#34;household_demographics&amp;#34;, &amp;#34;income_band&amp;#34;, &amp;#34;inventory&amp;#34;, &amp;#34;item&amp;#34;, &amp;#34;promotion&amp;#34;, &amp;#34;reason&amp;#34;, &amp;#34;ship_mode&amp;#34;, &amp;#34;store&amp;#34;, &amp;#34;store_returns&amp;#34;, &amp;#34;store_sales&amp;#34;, &amp;#34;time_dim&amp;#34;, &amp;#34;warehouse&amp;#34;, &amp;#34;web_page&amp;#34;, &amp;#34;web_returns&amp;#34;, &amp;#34;web_sales&amp;#34;, &amp;#34;web_site&amp;#34; ] for table in tables: df = glueContext.</description>
    </item>
    
    <item>
      <title>AthenaのFederated QueryでTPC-DS Connectorを用いてデータを生成する</title>
      <link>https://www.sambaiz.net/article/391/</link>
      <pubDate>Sat, 25 Dec 2021 23:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/391/</guid>
      <description>AthenaのFederated Queryは データソースコネクタとなるLambdaを通してDynamoDBやRDSといったS3以外のデータソースにクエリを実行できる機能。
 今回はAWS公式のリポジトリにあるTPC-DS Connectorを用いて、意思決定支援(Decision Support)におけるデータベースのベンチマークであるTPC-DSのデータを生成する。
公式のリポジトリにあるとはいえカスタム扱いなので自分でビルドする必要がある。基本的にREADME通り進めれば良いが、jdk16ではビルドに失敗したのでjdk8を入れて実行した。
$ brew tap homebrew/cask-versions $ brew install --cask corretto8 export JAVA_HOME=`/usr/libexec/java_home -v 1.8` export PATH=${JAVA_HOME}/bin:${PATH} $ java -version openjdk version &amp;#34;1.8.0_312&amp;#34; OpenJDK Runtime Environment Corretto-8.312.07.1 (build 1.8.0_312-b07) OpenJDK 64-Bit Server VM Corretto-8.312.07.1 (build 25.312-b07, mixed mode) publish.shの引数でもリージョンを取るが、これとは別に.aws/configなどで指定されていないとAWS SDKの方で読めずに失敗する。
$ git clone https://github.com/awslabs/aws-athena-query-federation.git -b v2021.51.1 --depth 1 $ cd aws-athena-query-federation/ $ mvn clean install -DskipTests=true $ cd athena-tpcds/ $ mvn clean install -DskipTests=true # export AWS_REGION=us-west-2 $ .</description>
    </item>
    
    <item>
      <title>MySQLのslow queryを出してEXPLAINしてインデックスを張る</title>
      <link>https://www.sambaiz.net/article/240/</link>
      <pubDate>Sat, 21 Sep 2019 22:50:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/240/</guid>
      <description>MySQLのslow queryを出してEXPLAINしてインデックスを張るのを ISUCON9の予選問題でやってみる。
$ mysql --version mysql Ver 8.0.17 for osx10.14 on x86_64 (Homebrew) slow_query_logをONにすると実行時間がlong_query_timeを超えていてmin_examined_row_limit以上の行を返すクエリがslow query logに出力される。 log_queries_not_using_indexesが有効ならインデックスを使用してないクエリもログに出て、 log_throttle_queries_not_using_indexesでその分あたりの数を制限できる。 MySQL 8.0.14から追加されたlog_slow_extraをONにするとフィールドが追加される。
mysql&amp;gt; show variables like &amp;#39;%slow%&amp;#39;; +---------------------------+-----------------------------------+ | Variable_name | Value | +---------------------------+-----------------------------------+ | log_slow_admin_statements | OFF | | log_slow_extra | OFF | | log_slow_slave_statements | OFF | | slow_launch_time | 2 | | slow_query_log | OFF | | slow_query_log_file | /usr/local/var/mysql/mbp-slow.log | +---------------------------+-----------------------------------+ 6 rows in set (0.00 sec) mysql&amp;gt; show variables like &amp;#39;%long_query%&amp;#39;; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.</description>
    </item>
    
    <item>
      <title>CloudFormationでVPCを作成してLambdaをデプロイしAurora Serverlessを使う</title>
      <link>https://www.sambaiz.net/article/206/</link>
      <pubDate>Sun, 03 Feb 2019 17:31:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/206/</guid>
      <description>Aurora ServerlessはオートスケールするAuroraで、 使ったAurora Capacity Unit (ACU)によって料金が発生するため、 使用頻度が少なかったり変動するアプリケーションにおいて安くRDBを使うことができる。 インスタンスを立てると最低でも月3000円くらいかかるが、Serverlessだとほとんどストレージ分から運用することができて趣味でも使いやすい。 ただしLambdaと同様に常に同等のリソースを使っている状態だとインスタンスと比べて割高になる。
今回はLambdaで使う。 Serverlessと名前には付いているが用途としてはLambdaに限らず、 むしろコンテナの数が容易に増え得るLambdaは同時接続数が問題になるRDBと一般に相性が良くない。 現在Betaの、コネクションを張らずにHTTPSでクエリを投げられるData APIはこの問題を解消すると思われるが、トランザクションが張れなかったり、レスポンスサイズに制限があるようだ。今回はコンソール上から初期クエリを流すためにData APIを有効にしている。
他の選択肢として、DynamoDBは現状最有力で最近トランザクションもサポートされたがSQLのように複雑なクエリは投げられない。 Athenaはクエリは投げられるがそこそこ時間がかかるし、INSERT/UPDATEはできずクエリごとに料金が発生する。
Serverless Frameworkを使ってリソースを作成しデプロイする。リポジトリはここ。
Serverless FrameworkでLambdaをデプロイする - sambaiz-net
VPCの作成 Aurora Serverlessの制限の一つとしてVPC内からしか接続しかできないというものがある。ということでVPCから作成していく。以前Terraformで作ったのと同じリソースをCloudFormationで作る。
TerraformでVPCを管理するmoduleを作る - sambaiz-net
LambdaをVPC内で動かすとコンテナ起動時にENIも作成するため立ち上がりの際時間がかかる。必要なら定期的に呼び出して削除されないようにする。 また、今回はテストのため/24でVPCを切っているが、小さいとENIのIPアドレスが枯渇する可能性がある。
 VPC  TestVPC: Type: AWS::EC2::VPC Properties: CidrBlock: 172.32.0.0/24 Tags: - Key: Name Value: test-vpc  Subnet  Aurora Serverlessのために少なくとも2つのサブネットが必要。
TestPublicSubnet: Type: AWS::EC2::Subnet Properties: VpcId: !Ref TestVPC CidrBlock: 172.32.0.0/25 AvailabilityZone: us-east-1d Tags: - Key: Name Value: test-public-subnet1 TestPrivateSubnet1: Type: AWS::EC2::Subnet Properties: VpcId: !</description>
    </item>
    
    <item>
      <title>MySQLのtime_zoneとgo-sql-driver/mysqlの設定</title>
      <link>https://www.sambaiz.net/article/189/</link>
      <pubDate>Tue, 02 Oct 2018 22:22:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/189/</guid>
      <description>MySQLのtime_zoneとgo-sql-driver/mysqlの設定による挙動を確認する。
&amp;gt; select version(); +-----------+  | version() | +-----------+  | 5.7.21 | +-----------+ タイムゾーンがロードされていない場合はロードする。
&amp;gt; select count(*) from mysql.time_zone \\G; *************************** 1. row *************************** count(*): 0 $ mysql_tzinfo_to_sql /usr/share/zoneinfo/ | mysql -u root mysql time_zoneはデフォルト値のSYSTEM。つまりJSTで、my.cnfのdefault-time-zoneで変更できる。 NOW()もJSTの時間を返している。
&amp;gt; show variables like &amp;#39;%time_zone%&amp;#39;; +------------------+--------+ | Variable_name | Value | +------------------+--------+ | system_time_zone | JST | | time_zone | SYSTEM | +------------------+--------+  &amp;gt; SELECT NOW(); mysql&amp;gt; SELECT NOW() \G; *************************** 1.</description>
    </item>
    
    <item>
      <title>MySQL InnoDBのロックの挙動</title>
      <link>https://www.sambaiz.net/article/158/</link>
      <pubDate>Sat, 03 Mar 2018 19:44:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/158/</guid>
      <description>https://dev.mysql.com/doc/refman/5.7/en/innodb-locking.html
トランザクション分離レベルはデフォルトのREPEATABLE-READ。
&amp;gt; SELECT @@GLOBAL.tx_isolation, @@tx_isolation; +-----------------------+-----------------+ | @@GLOBAL.tx_isolation | @@tx_isolation | +-----------------------+-----------------+ | REPEATABLE-READ | REPEATABLE-READ | +-----------------------+-----------------+ 準備 DBを立ち上げてテーブルとレコードを入れる。
$ cat schema_and_data.sql CREATE TABLE a ( id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY, name VARCHAR(128) NOT NULL ); CREATE TABLE b ( id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY, a_id BIGINT UNSIGNED NOT NULL, FOREIGN KEY (a_id) REFERENCES a (id) ); INSERT INTO a (id, name) VALUES (1, &amp;#39;1&amp;#39;); INSERT INTO a (id, name) VALUES (2, &amp;#39;2&amp;#39;); INSERT INTO a (id, name) VALUES (3, &amp;#39;3&amp;#39;); INSERT INTO a (id, name) VALUES (8, &amp;#39;8&amp;#39;); INSERT INTO a (id, name) VALUES (9, &amp;#39;9&amp;#39;); INSERT INTO a (id, name) VALUES (10, &amp;#39;10&amp;#39;); $ cat start.</description>
    </item>
    
    <item>
      <title>xorm reverseでDBスキーマからGoのstructを生成する</title>
      <link>https://www.sambaiz.net/article/153/</link>
      <pubDate>Sat, 10 Feb 2018 15:55:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/153/</guid>
      <description>GoのORMのxormにはxorm reverseというDBのスキーマから以下のようなテンプレートに沿ったGoのstructなどを生成するツールがある。
package {{.Model}} import ( {{range .Imports}}&amp;#34;{{.}}&amp;#34;{{end}} ) {{range .Tables}} type {{Mapper .Name}} struct { {{$table := .}} {{range .Columns}}	{{Mapper .Name}}	{{Type .}} {{end}} } {{end}} リポジトリにあるテンプレートにxorm用テンプレートとgo用のテンプレートが用意されているように、単体で使うこともできる。 また、テンプレートを書く言語としてもGo以外にC++もサポートしている。
xormのcmdとドライバをインストール。
$ go get github.com/go-xorm/cmd/xorm $ go get github.com/go-sql-driver/mysql $ xorm Version: 0.2.0524 様々な型のカラムを含むテーブルで試す。
$ cat schema.sql CREATE TABLE table1 ( n_tinyint TINYINT, n_int INT, n_int_unsigned INT UNSIGNED NOT NULL DEFAULT 1, n_bigint BIGINT, n_float FLOAT, n_double DOUBLE, d_date DATE, d_datetime DATETIME, s_char CHAR(64), s_varchar VARCHAR(64), s_text TEXT, s_json JSON, b_binary BLOB, e_enum ENUM(&amp;#39;aaa&amp;#39;, &amp;#39;bbb&amp;#39;, &amp;#39;ccc&amp;#39;) ) $ cat setup.</description>
    </item>
    
    <item>
      <title>Gooseリポジトリのmerge時にバージョンを上げmigrationボタンをSlackに出す</title>
      <link>https://www.sambaiz.net/article/149/</link>
      <pubDate>Fri, 19 Jan 2018 09:30:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/149/</guid>
      <description>GooseはGo製のDB Migrationツール。
コード
こんなリポジトリを作成し、各自ブランチを切ってGoose形式のup/downのSQLを書き、終わったらPullRequestを出す。
goose/ .keep .circleci/config.yml create_test_table.sql $ cat create_test_table.sql -- +goose Up -- SQL in this section is executed when the migration is applied. CREATE TABLE testtable ( id BIGINT UNSIGNED PRIMARY KEY AUTO_INCREMENT, n INT NOT NULL, c VARCHAR (20) NOT NULL UNIQUE ); -- +goose Down -- SQL in this section is executed when the migration is rolled back. DROP TABLE testtable; 無事Approveされ、mergeされるとCircleCIが走り、 SQLをgooseディレクトリの中にバージョンを付けて移し、 SlackにpostMessageするエンドポイントにリクエストを飛ばす。
ここでバージョンを作成することによって、並列で作業し、レビューなどの関係で適用順が前後しても修正する必要をなくしている。ただ、pushされる前に複数のブランチを連続でmergeする場合うまく動かないのでそれはなんとかする必要がある。</description>
    </item>
    
    <item>
      <title>MySQLのALTER TABLEのメモ</title>
      <link>https://www.sambaiz.net/article/93/</link>
      <pubDate>Sat, 15 Apr 2017 19:45:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/93/</guid>
      <description>しばらく書かないとどういう構文だったか忘れてしまう。
MySQL :: MySQL 5.6 リファレンスマニュアル :: 13.1.7 ALTER TABLE 構文
CREATE TABLE t0 ( id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY, c1 VARCHAR(30), c2 VARCHAR(30) ); CREATE TABLE t2 ( id BIGINT UNSIGNED AUTO_INCREMENT PRIMARY KEY ); ALTER TABLE t0 RENAME t1; ALTER TABLE t1 ADD COLUMN t2_id BIGINT UNSIGNED AFTER id, ADD COLUMN c3 INTEGER NOT NULL AFTER t2_id, MODIFY COLUMN c1 VARCHAR(30) NOT NULL, DROP COLUMN c2, ADD INDEX (c3), ADD FOREIGN KEY (t2_id) REFERENCES t2(id) ON UPDATE RESTRICT ON DELETE RESTRICT ; mysql&amp;gt; SHOW CREATE TABLE t1 \G; *************************** 1.</description>
    </item>
    
    <item>
      <title>GoogleのkvsライブラリLevelDBを使う</title>
      <link>https://www.sambaiz.net/article/45/</link>
      <pubDate>Sat, 24 Dec 2016 21:18:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/45/</guid>
      <description>LevelDBとは https://github.com/google/leveldb
Googleが作った高速なkey-valueストレージライブラリ。
ChromeのIndexedDBやprometheusなどで使われている。
特徴  Keyと任意のバイト配列のValue データはKeyでソートされる。ソートのための比較関数はオーバーライドできる。 基本的な操作はPut, Get, Delete。 複数の変更を一つのatomicなバッチで行える 一環したデータのビューを取得するために、一時的なスナップショットを作成できる データを前にも後ろにもイテレーションできる データはSnappy compression libraryで自動で圧縮される。 ファイルシステムの操作など外部のアクティビティを仮想的なインタフェースを通して行うので、OSとのやりとりをカスタマイズできる。  制限  SQLデータベースではない。リレーショナルなデータモデルは持てないし、SQLやインデックスにも対応していない。 一度に一つのプロセスしかDBにアクセスできない。  キャッシュ  DBはファイルシステムのディレクトリに対応する名前を持ち、内容はそのディレクトリに保存される。 各ファイルには圧縮したブロックが保存され、良く使うものについては非圧縮のブロックがキャッシュされる。 ソートして隣接するキーは通常、同じブロックに配置される。ディスク転送とキャッシュはブロック単位。  フィルタ  Getの際、不要なデータを読まなくていいようにフィルタ(Bloom Filter)を用いることができる。 独自の比較関数(末尾のスペースを無視するなど)を使う場合、Bloom Filterを使うことができないことがあるので、その場合は独自のフィルタが必要。  レベル  最近の更新はログファイルに保存される。これが決められたサイズ(デフォルトでは約4MB)に達すると、sorted table(sst)に変換され、新しいログファイルが生成される。 現在のログファイルのコピーがメモリ(memtable)にも乗って読み取りで参照される。 sstはキーによってソートされたエントリーを保存する。エントリーはキーの値か、削除マーカー。 sstはレベルによってまとめられる。ログファイルから変換されると、特別なyoungレベル(level-0とも呼ばれる)に配置される。 youngファイルの数があるしきい値(現在4つ)を超えると全てのyoungファイルを全てのlevel-1ファイルとマージし、新しいlevel-1ファイルを生成する(2MBごとに1ファイル)。 youngレベルのファイルにはキーが重複していることがある。しかし、他のレベルでは重複しないキーの範囲がある。 level-L(L&amp;gt;=1)のファイルの合計サイズが10^L MBを超えたとき、level-Lのファイルと、level-(L+1)の全てのファイルをマージし、新しいlevel-(L+1)ファイルを生成する。 これによって、バルク読み込み/書き込みのみを使い、コストが高いシークを最小限にして、youngレベルから大きいレベルに更新を徐々にマイグレーションすることができる。  動かしてみる LevelDBのgo実装。
syndtr/goleveldb
$ go get github.com/syndtr/goleveldb/leveldb まずDBを開く。
// open db, err := leveldb.OpenFile(&amp;#34;/Users/sambaiz/leveldb&amp;#34;, nil) defer db.Close() if err != nil { panic(err) } 普通に5個(key0~4)、バッチで5個(key5~9)のデータを入れて、そのうち一つを消す。</description>
    </item>
    
    <item>
      <title>MySQLで大文字小文字を区別しないのを直す</title>
      <link>https://www.sambaiz.net/article/11/</link>
      <pubDate>Sun, 24 Jul 2016 22:12:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/11/</guid>
      <description>Collationの話。
MySQL 5.6 CREATE TABLE sample ( id SERIAL, name VARCHAR(30) ) ENGINE=InnoDB CHARACTER SET utf8mb4; INSERT INTO sample (name) VALUES (&amp;#39;tom&amp;#39;),(&amp;#39;Tom&amp;#39;),(&amp;#39;TOM&amp;#39;); このテーブルを&amp;quot;tom&amp;quot;で絞り込むとこうなる。大文字小文字を区別していない。
mysql&amp;gt; SELECT * FROM sample2 WHERE name = &amp;#39;tom&amp;#39;; +----+------+ | id | name | +----+------+ | 1 | tom | | 2 | Tom | | 3 | TOM | +----+------+ 3 rows in set (0.01 sec) MySQL :: MySQL 5.6 リファレンスマニュアル :: B.5.5.1 文字列検索での大文字/小文字の区別
 単純な比較操作 (&amp;gt;=、&amp;gt;、=、&amp;lt;、&amp;lt;=、ソート、およびグループ化) は、各文字の「ソート値」に基づきます。 同じソート値を持つ文字は同じ文字として扱われます。たとえば、「e」 と 「é」 が対象の照合順序で同じソート値を持つ場合は、等しいと判断されます。</description>
    </item>
    
    <item>
      <title>MySQLのUNIX_TIMESTAMPにある程度未来の日付を渡すと0になる</title>
      <link>https://www.sambaiz.net/article/4/</link>
      <pubDate>Mon, 04 Jul 2016 19:49:00 +0900</pubDate>
      
      <guid>https://www.sambaiz.net/article/4/</guid>
      <description>以下、MySQL5.6で遭遇した。
MySQLのUNIX_TIMESTAMPは DATETIME文字列などを引数にとり、UNIXタイムスタンプ(1970-01-01 00:00:00 UTCを起点とした経過秒数)を返す関数だ。
mysql&amp;gt; SET SESSION time_zone = &amp;#39;UTC&amp;#39;; mysql&amp;gt; select UNIX_TIMESTAMP(&amp;#39;1970-01-01 00:00:00&amp;#39;); +---------------------------------------+ | UNIX_TIMESTAMP(&amp;#39;1970-01-01 00:00:00&amp;#39;) | +---------------------------------------+ | 0 | +---------------------------------------+ 1 row in set (0.00 sec) ただし、2038年1月19日3時14分7秒(UTC)以降を渡すと0になってしまう。 これはドキュメントにも書いてある通り範囲外だから。
mysql&amp;gt; select UNIX_TIMESTAMP(&amp;#39;2038-01-19-03-14-07&amp;#39;); +---------------------------------------+ | UNIX_TIMESTAMP(&amp;#39;2038-01-19-03-14-07&amp;#39;) | +---------------------------------------+ | 2147483647 | +---------------------------------------+ 1 row in set (0.04 sec) mysql&amp;gt; select UNIX_TIMESTAMP(&amp;#39;2038-01-19-03-14-08&amp;#39;); +---------------------------------------+ | UNIX_TIMESTAMP(&amp;#39;2038-01-19-03-14-08&amp;#39;) | +---------------------------------------+ | 0 | +---------------------------------------+ 1 row in set (0.00 sec) では、この境目は何かというと、32ビットで表せる符号付数値の最大値だ。</description>
    </item>
    
  </channel>
</rss>
